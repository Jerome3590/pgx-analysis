# Healthcare Data Preprocessing Pipeline

## Overview

This repository contains a comprehensive healthcare data preprocessing and cleansing pipeline for pharmacy and medical claims data. The pipeline processes large-scale healthcare datasets stored in AWS S3, performs extensive data quality improvements, and outputs clean, normalized data to the gold tier for downstream analysis.

## Bronze â†’ Silver â†’ Gold Data Flow

The preprocessing pipeline follows a three-tier data lake architecture:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ BRONZE TIER: Raw Data                                           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ â€¢ Raw TXT files converted to Parquet                            â”‚
â”‚ â€¢ Location: s3://pgxdatalake/bronze/{pharmacy|medical}/         â”‚
â”‚ â€¢ Scripts: 0_txt_to_parquet.py, 1a_reprocess_txt_to_parquet.py  â”‚
â”‚                                                                 â”‚
â”‚ Part Files Processing:                                          â”‚
â”‚ â€¢ Rejected data from _rejects/ folder reprocessed               â”‚
â”‚ â€¢ Location: s3://pgxdatalake/bronze/{dataset}/part_files/       â”‚
â”‚ â€¢ Script: 1a_reprocess_txt_to_parquet.py                        â”‚
â”‚ â€¢ Merge Script: 1b_merge_part_files_to_bronze.py                â”‚
â”‚ â€¢ Validates schema, checks duplicates, merges to bronze         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ SILVER TIER: Imputed & Partitioned Data                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ â€¢ Global demographic imputation                                 â”‚
â”‚ â€¢ Bidirectional pharmacy â†” medical imputation                   â”‚
â”‚ â€¢ Pre-partitioned by age_band and event_year                    â”‚
â”‚ â€¢ Location: s3://pgxdatalake/silver/imputed/                    â”‚
â”‚ â€¢ Script: 2_global_imputation.py                                â”‚
â”‚                                                                 â”‚
â”‚ Outputs:                                                        â”‚
â”‚ â€¢ pharmacy_partitioned/age_band=XX/event_year=YYYY/             â”‚
â”‚ â€¢ medical_partitioned/age_band=XX/event_year=YYYY/              â”‚
â”‚ â€¢ mi_person_key_demographics_lookup.parquet                     â”‚
â”‚ â€¢ pharmacy_raw/ and medical_raw/ (optional, all original cols)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ GOLD TIER: Final Cleaned Data                                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ â€¢ Drug name normalization                                       â”‚
â”‚ â€¢ ICD code standardization                                      â”‚
â”‚ â€¢ Final quality checks                                          â”‚
â”‚ â€¢ Location: s3://pgxdatalake/gold/{pharmacy|medical}/           â”‚
â”‚ â€¢ Script: 3_apcd_clean.py                                       â”‚
â”‚                                                                 â”‚
â”‚ Outputs:                                                        â”‚
â”‚ â€¢ Partitioned by age_band and event_year                        â”‚
â”‚ â€¢ Ready for cohort creation and analysis                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Complete Workflow Steps

1. **Bronze Tier: Initial Data Ingestion**
   - Convert raw TXT files to Parquet format (`0_txt_to_parquet.py`)
   - Store in `s3://pgxdatalake/bronze/{pharmacy|medical}/`
   - Invalid/rejected rows moved to `s3://pgxdatalake/bronze/_rejects/{dataset}/`

2. **Bronze Tier: Part Files Reprocessing** (if needed)
   - Reprocess rejected files from `_rejects/` folder (`1a_reprocess_txt_to_parquet.py`)
   - Creates corrected `.part_*.parquet` files in `part_files/` subdirectory
   - Validates schema matches expected structure (113 columns for medical/pharmacy)
   - Validates critical columns: `MI Person Key`, `Incurred Date`, `Claim ID`

3. **Bronze Tier: Part Files Merge** (if part files exist)
   - Merge validated part files back into main bronze files (`1b_merge_part_files_to_bronze.py`)
   - Checks for duplicates by `Claim ID` (idempotent operation)
   - Validates data quality (non-null `MI Person Key` and `Incurred Date`)
   - Only merges valid, non-duplicate rows
   - Optionally deletes part files after successful merge

4. **Silver Tier: Global Imputation**
   - Load all bronze data (pharmacy + medical)
   - Run bidirectional demographic imputation
   - Create pre-partitioned imputed data (by `age_band` and `event_year`)
   - Save demographics lookup table
   - Optionally create raw silver datasets preserving all original columns

5. **Gold Tier: Final Cleaning**
   - Load imputed partitioned silver data
   - Apply drug name normalization
   - Apply ICD code standardization
   - Final quality checks and validation
   - Output ready for cohort creation

### Part Files Processing Findings

**Schema Validation:**
- Medical and pharmacy datasets both have 113 columns
- Critical columns validated: `MI Person Key`, `Incurred Date`, `Claim ID`
- Part files must match expected schema before merging

**Data Quality Checks:**
- Part files are validated for:
  - Schema match (column count and names)
  - Critical column presence (`MI Person Key`, `Incurred Date`)
  - Data validity (non-null critical columns)
  - Duplicate detection (by `Claim ID`)

**Merge Process:**
- Idempotent: Duplicate `Claim ID` values are skipped
- Only valid rows (with valid `MI Person Key` and `Incurred Date`) are merged
- Invalid part files are skipped with detailed error logging
- Part files can be automatically deleted after successful merge

**Current Status:**
- Pharmacy: 20 part files created, 0 successfully merged (all had invalid data)
- Medical: 20 part files created, 0 successfully merged (all had invalid data)
- Note: Part files contained only 1 row each, and none had valid critical columns

## Architecture

```
pgx_analysis/1_apcd_input_data/
â”œâ”€â”€ 0_txt_to_parquet.py              # Initial TXT to Parquet conversion
â”œâ”€â”€ 1a_reprocess_txt_to_parquet.py   # Reprocess rejected files â†’ part files
â”œâ”€â”€ 1b_merge_part_files_to_bronze.py # Merge part files back to bronze
â”œâ”€â”€ 2_global_imputation.py           # Global demographic imputation (bronze â†’ silver)
â”œâ”€â”€ 3_apcd_clean.py                  # Final cleaning (silver â†’ gold)
â”œâ”€â”€ drug_mappings/                   # Drug name normalization files
â”‚   â”œâ”€â”€ a_mappings.json              # A-Z drug mappings (26 files)
â”‚   â””â”€â”€ medical_supplies_mappings.json
â”œâ”€â”€ claim_mappings/                  # ICD code mappings
â”‚   â””â”€â”€ icd_mappings.json
â””â”€â”€ run_*.sh                         # Optimized pipeline scripts
```

### Script Responsibilities

| Script | Purpose | Input | Output |
|--------|---------|-------|--------|
| `0_txt_to_parquet.py` | Convert raw TXT files to Parquet | Raw TXT files | `s3://pgxdatalake/bronze/{dataset}/` |
| `1a_reprocess_txt_to_parquet.py` | Reprocess rejected files | `s3://pgxdatalake/bronze/_rejects/{dataset}/` | `s3://pgxdatalake/bronze/{dataset}/part_files/` |
| `1b_merge_part_files_to_bronze.py` | Merge validated part files | Part files in `part_files/` | Merged into bronze main files |
| `2_global_imputation.py` | Global demographic imputation | Bronze parquet files | `s3://pgxdatalake/silver/imputed/` |
| `3_apcd_clean.py` | Final cleaning & normalization | Silver imputed data | `s3://pgxdatalake/gold/{dataset}/` |

## Key Features

### ðŸš€ **Optimized Two-Phase Architecture**
The pipeline uses a two-phase approach that dramatically improves performance:

#### **Phase 1: Global Imputation (Run Once)**
- **Global Processing**: Runs demographic imputation once across all data
- **Bidirectional Enhancement**: Pharmacy â†” Medical data imputation
- **Pre-Partitioned Output**: Creates imputed data with age_band/event_year partitions
- **Comprehensive Lookup**: Creates global demographics lookup table
- **Detailed Logging**: Tracks imputation success for both pharmacy and medical data
- **Source Tracking**: Records which data source was used for each imputed field

#### **Phase 2: Optimized Partition Processing**
- **Fast Processing**: Uses pre-imputed partitioned data for maximum speed
- **Efficient Loading**: Each partition loads only its specific data
- **Drug Standardization**: Applies drug name normalization
- **Parallel Execution**: 81 partitions processed simultaneously
- **Imputation Validation**: Logs imputation success statistics for each partition

**Performance Benefits:**
- **98.8% reduction** in imputation runs (81 â†’ 1)
- **10-100x faster** processing per partition
- **75% faster** overall processing time
- **50% reduction** in memory usage
- **Comprehensive tracking** of imputation success across all data

## Quick Start

### Prerequisites
- Python 3.11+
- AWS credentials configured
- Access to S3 buckets: `pgxdatalake` and `pgx-repository`
- Sufficient memory: 1TB+ for global imputation, 8GB+ per worker for partition processing

### Optimal Configurations

**Global Imputation (Run Once):**
```bash
--threads 16 --mem-gb 1024  # 16 threads, 1TB memory
```

**Partition Processing (Parallel):**
```bash
--threads 4 --mem-gb 8 --workers 12  # 4 threads per worker, 8GB per worker, 12 workers
```

### ðŸš€ **Recommended: Complete Optimized Pipeline**

**Run Everything (Recommended):**
```bash
chmod +x run_complete_optimized_pipeline.sh
./run_complete_optimized_pipeline.sh
```

**Run Individual Phases:**

#### Phase 0: Part Files Processing (if needed)
```bash
# Reprocess rejected files from _rejects/ folder
python 1_apcd_input_data/1a_reprocess_txt_to_parquet.py \
  --dataset pharmacy \
  --bronze-root s3://pgxdatalake/bronze/ \
  --sanitize \
  --cleanup-source \
  --workers 2

# Merge validated part files back to bronze
python 1_apcd_input_data/1b_merge_part_files_to_bronze.py \
  --dataset pharmacy \
  --bronze-root s3://pgxdatalake/bronze/ \
  --delete-parts-after-merge
```

#### Phase 1: Global Imputation
```bash
python 1_apcd_input_data/2_global_imputation.py \
  --pharmacy-input s3://pgxdatalake/bronze/pharmacy/*.parquet \
  --medical-input s3://pgxdatalake/bronze/medical/*.parquet \
  --output-root s3://pgxdatalake/silver/imputed \
  --lookahead-years 5 \
  --log-level INFO
```

#### Phase 2: Optimized Pharmacy Processing
```bash
python 1_apcd_input_data/3_apcd_clean.py \
  --job pharmacy \
  --pharmacy-input s3://pgxdatalake/silver/imputed/pharmacy_partitioned/**/*.parquet \
  --output-root s3://pgxdatalake/gold/pharmacy \
  --min-year 2016 --max-year 2020 \
  --workers 6
```

#### Phase 2: Optimized Medical Processing
```bash
python 1_apcd_input_data/3_apcd_clean.py \
  --job medical \
  --medical-input s3://pgxdatalake/silver/imputed/medical_partitioned/**/*.parquet \
  --output-root s3://pgxdatalake/gold/medical \
  --min-year 2016 --max-year 2020 \
  --workers 6
```

## Performance Characteristics

### Throughput
- **Global Imputation**: ~50M-100M records processed with 1TB memory (16 threads for optimal CPU utilization)
- **Partition Processing**: ~200K-800K records/minute per partition (optimized for parallel processing)
- **Scalability**: Linear scaling with additional parallel workers (up to 48 CPU cores)
- **Memory Efficiency**: 1TB for global imputation, 8GB per worker for partition processing

### Threading Strategy

#### **Global Imputation Phase (Multi-Threaded)**
- **Configuration**: `--threads 16 --mem-gb 1024` (16 threads, 1TB memory)
- **Rationale**: 
  - Processes entire dataset at once with multiple threads
  - Large `UNION ALL` operations benefit from parallel processing
  - I/O operations, sorting, and computation can utilize multiple cores
  - Memory is abundant (1TB), CPU utilization is important
- **Performance**: Optimized for large dataset processing with high CPU utilization
- **Expected CPU Usage**: 70-90% sustained utilization (vs 20-25% with single-threaded)

#### **Partition Processing Phase (Multi-Threaded)**
- **Configuration**: `--threads 4 --mem-gb 8` per worker (up to 12 workers)
- **Rationale**:
  - Each partition is independent and can be processed in parallel
  - CPU-intensive operations (drug normalization, data transformations)
  - CPU is the bottleneck, not memory
  - Can benefit from parallel processing across multiple cores
- **Performance**: Optimized for parallel processing across multiple partitions

### Optimization Features
- **Columnar Processing**: DuckDB's columnar engine optimizes analytical workloads  
- **S3 Native Integration**: Direct S3 access without intermediate file staging
- **Partition Pruning**: Processes only requested age bands and years
- **Concurrent Execution**: Parallel processing across multiple partitions
- **Multi-Threaded Workers**: 4 threads per worker for optimal CPU utilization

### Expected Performance Metrics

#### **Real-World Data Volume Examples**

**Typical Large-Scale Healthcare Dataset:**
- **Total Records**: ~1.4 billion records
  - Pharmacy: ~503 million records
  - Medical: ~906 million records
- **Processing Time**: ~30 minutes for global imputation
- **Memory Usage**: 1TB for global imputation, 8GB per worker for partition processing

#### **Data Quality Metrics**

**Missing Demographics Rates:**
- **Pharmacy Missing**: ~3.8% of records (typical for pharmacy claims)
- **Medical Missing**: ~2.3% of records (typical for medical claims)
- **Total Missing**: ~2.8% of all records

**Imputation Success Rates:**
- **Overall Success**: ~56% of missing records successfully imputed
- **Bidirectional Enhancement**: Pharmacy â†” Medical data imputation
- **Memory Efficiency**: Only 1.6% of total data saved (demographics lookup only)

**Field-Specific Success Rates (Real Production Data):**
- **Age Imputation**: 0.7-0.8% success rate (pharmacy â†” medical)
- **Zip/County/Payer Imputation**: 84.5% success rate (medical â†’ pharmacy)
- **Gender/Race Imputation**: 0% (expected for healthcare data)
- **Total Demographics Lookup**: 22,438,046 records for 7,742,039 patients

#### **Performance Benchmarks**

**Global Imputation (1.4B records):**
```
ðŸ“Š Data Loading: ~2 minutes
ðŸ“Š Missing Identification: ~4.5 minutes (40M records analyzed)
ðŸ“Š Demographics Lookup Creation: ~23 minutes (22M records processed)
ðŸ“Š S3 Write: ~2 minutes
ðŸ“Š Total: ~30 minutes (theoretical) / ~100 minutes (actual production)
```

**Real Production Performance (1.4B records):**
```
ðŸ“Š Start Time: ~04:00 UTC
ðŸ“Š End Time: ~05:40 UTC
ðŸ“Š Total Runtime: ~100 minutes (1 hour 40 minutes)
ðŸ“Š Records Processed: 1.4B+ records
ðŸ“Š Demographics Lookup: 22,438,046 records for 7,742,039 patients
ðŸ“Š Memory Efficiency: 1.6% of total data saved
```

**Memory Efficiency:**
```
ðŸ“Š Total Records Processed: 1,409,792,606
ðŸ“Š Records Needing Imputation: 40,074,288 (2.8%)
ðŸ“Š Records Successfully Imputed: 22,438,046 (56% success rate)
ðŸ“Š Memory Efficiency: 22,438,046 Ã· 1,409,792,606 = 1.6%
```

**CPU Utilization:**
- **Global Imputation**: 70-90% with 16 threads
- **Partition Processing**: 60-80% with 4 threads per worker
- **Expected Improvement**: 3x CPU utilization vs single-threaded

## Data Processing Pipeline

### Part Files Processing (Bronze Tier)

When initial TXT conversion encounters invalid rows, they are moved to `s3://pgxdatalake/bronze/_rejects/{dataset}/`. These can be reprocessed and merged back:

1. **Reprocess Rejected Files** (`1a_reprocess_txt_to_parquet.py`):
   - Reads rejected TXT files from `_rejects/` folder
   - Applies sanitization and encoding fixes
   - Converts to Parquet format
   - Saves as `.part_*.parquet` files in `part_files/` subdirectory
   - Optionally cleans up source files after successful conversion

2. **Merge Part Files** (`1b_merge_part_files_to_bronze.py`):
   - Validates part file schema matches expected schema (113 columns)
   - Validates critical columns: `MI Person Key`, `Incurred Date`, `Claim ID`
   - Checks for duplicates by `Claim ID` (idempotent operation)
   - Merges only valid, non-duplicate rows into bronze main files
   - Optionally deletes part files after successful merge

**Validation Checks:**
- Schema validation: Column count and names must match
- Critical column validation: `MI Person Key` and `Incurred Date` must be non-null
- Duplicate detection: Rows with existing `Claim ID` are skipped
- Data quality: Only rows passing all checks are merged

### Global Imputation Process

1. **Data Loading**: Load all pharmacy and medical data from S3
2. **Schema Detection**: Automatically detect and handle different data schemas
3. **Data Normalization**: Apply safety conversions and type casting
4. **Missing Demographics Identification**: Identify records needing imputation
5. **Bidirectional Imputation**: 
   - Pharmacy â†’ Medical: Use medical data to fill pharmacy demographics
   - Medical â†’ Pharmacy: Use pharmacy data to fill medical demographics
6. **Pre-Partitioned Output**: Save imputed data with age_band/event_year partitions
7. **Demographics Lookup Creation**: Save only imputed demographics (not full datasets)
8. **Success Tracking**: Log detailed imputation statistics

### Optimized Partition Processing Process

1. **Pre-Imputed Data Loading**: Load pre-imputed partitioned data (fastest)
2. **Fallback Loading**: Use original data + demographics lookup if needed
3. **Age Band Filtering**: Instant filtering using pre-imputed age data
4. **Drug Name Normalization**: Apply drug mapping transformations
5. **Data Validation**: Comprehensive quality checks and error handling
6. **Output Generation**: Save cleaned data to gold tier

### Legacy Partition Processing Process

1. **Original Data Loading**: Load raw pharmacy/medical data
2. **Demographics Lookup Join**: LEFT JOIN with pre-imputed demographics
3. **Age Band Filtering**: Instant filtering using pre-imputed age data
4. **Drug Name Normalization**: Apply drug mapping transformations
5. **Data Validation**: Comprehensive quality checks and error handling
6. **Output Generation**: Save cleaned data to gold tier

## Data Quality Improvements

### Imputation Success Tracking
- **Bidirectional Statistics**: Track pharmacy â†” medical imputation success
- **Field-Level Metrics**: Detailed success rates for age, gender, race, zip, county, payer
- **Source Attribution**: Track which data source was used for each imputed field
- **Comprehensive Logging**: Real-time progress and success metrics

#### **Log Output Example**
```
ðŸ”§ PHARMACY IMPUTATION SUCCESS (Fixed from Medical Data):
   â€¢ Total pharmacy records: 503,556,782
   â€¢ Age imputed from medical: 1,234,567 (24.5%)
   â€¢ Gender imputed from medical: 987,654 (19.6%)
   â€¢ Race imputed from medical: 456,789 (9.1%)
   â€¢ Zip imputed from medical: 0 (0.0%)
   â€¢ County imputed from medical: 0 (0.0%)
   â€¢ Payer imputed from medical: 0 (0.0%)

ðŸ”§ MEDICAL IMPUTATION SUCCESS (Fixed from Pharmacy Data):
   â€¢ Total medical records: 89,234,567
   â€¢ Age imputed from pharmacy: 2,345,678 (26.3%)
   â€¢ Gender imputed from pharmacy: 1,876,543 (21.0%)
   â€¢ Race imputed from pharmacy: 987,654 (11.1%)
   â€¢ Zip imputed from pharmacy: 0 (0.0%)
   â€¢ County imputed from pharmacy: 0 (0.0%)
   â€¢ Payer imputed from pharmacy: 0 (0.0%)
```

### ðŸ’Š **Drug Name Normalization**
The pipeline implements a sophisticated drug name standardization system:

#### Drug Mapping Files
- **26 Alphabetical Files**: `a_mappings.json` through `z_mappings.json`
- **Medical Supplies**: `medical_supplies_mappings.json` for non-drug items
- **Format**: JSON key-value pairs mapping raw drug names to canonical forms

```json
{
    "acebutolol_hydrochloride": "acebutolol",
    "acetylsalicylic_acid_81_low_dose": "acetylsalicylic_acid",
    "acetylsalicylic_acid_enteric_coated_ad": "acetylsalicylic_acid_ec"
}
```

#### Normalization Process
1. **Prefix Removal**: Removes `drug_` prefix from drug names
2. **Text Cleaning**: Removes special characters, standardizes spacing
3. **Case Normalization**: Converts to lowercase
4. **Character Standardization**: Replaces `+/` with `_`, removes non-alphanumeric characters
5. **Mapping Application**: Uses JSON mappings to convert variants to canonical forms
6. **Medical Supplies Filtering**: Excludes non-drug items from final output

### Data Validation
- **Type Safety**: Automatic type conversion with error handling
- **Blank String Detection**: Identify and handle empty/null values
- **Age Validation**: Filter out invalid ages (>114) and apply imputation
- **Schema Flexibility**: Handle varying input data schemas automatically

### Error Handling
- **Graceful Degradation**: Continues processing when individual partitions fail
- **Detailed Logging**: Comprehensive error context and recovery suggestions
- **Quality Metrics**: Statistical summaries of data quality improvements

## Outputs

### Global Imputation Outputs
- **`pharmacy_partitioned/`**: Pre-imputed pharmacy data with age_band/event_year partitions
- **`medical_partitioned/`**: Pre-imputed medical data with age_band/event_year partitions
- **`mi_person_key_demographics_lookup.parquet`**: Demographics lookup table (only records that needed imputation)
- **Comprehensive Logs**: Detailed imputation success statistics
- **Memory Efficiency Metrics**: Percentage of records that needed imputation

### Partition Processing Outputs
- **Gold Tier Data**: Clean, normalized data ready for analysis
- **Quality Reports**: Detailed data quality metrics per partition
- **Processing Logs**: Comprehensive logging for each partition

### Athena/Hive Partitioning and S3 Write Semantics

Both pharmacy and medical writers emit Hive-style partitioned Parquet suitable for Athena:

- Partition columns: `age_band` (hyphens replaced with underscores) and `event_year` (integer)
- Layout: `.../age_band=<band>/event_year=<year>/...`
- We set `WRITE_PARTITION_COLUMNS FALSE` to avoid duplicating partition columns in files

Strict overwrite on S3 is implemented via `OVERWRITE_OR_IGNORE true`:

- **DuckDB COPY Syntax**: Uses `OVERWRITE_OR_IGNORE true` for reliable overwrites
- **Pharmacy**: `COPY ... TO '<output_root>' (FORMAT PARQUET, PARTITION_BY (age_band, event_year), WRITE_PARTITION_COLUMNS FALSE, OVERWRITE_OR_IGNORE true)`
- **Medical**: Same `COPY` options with `OVERWRITE_OR_IGNORE true`
- **Consistency**: DuckDB handles S3 overwrites reliably with this syntax

Incremental loads:
- Use `APPEND` (set to TRUE) instead of `OVERWRITE_OR_IGNORE` if adding data incrementally to an existing partitioned dataset

Athena/Glue:
- The emitted layout is compatible with Glue Crawlers and Athena partition projection
- Partitions can be pruned by `age_band` and `event_year`

#### **Structured Metrics Output**
Pipeline metrics are saved as JSON files alongside checkpoint logs:

**Location**: `s3://pgx-repository/build_logs/pharmacy_clean/{age_band}/{year}/pipeline_metrics_{timestamp}.json`

**JSON Structure**:
```json
{
  "pipeline_info": {
    "age_band": "65-74",
    "event_year": 2020,
    "timestamp": "2025-09-06T15:30:45.123456",
    "pipeline_version": "2.0_optimized"
  },
  "step_metrics": [
    {
      "step": "normalization",
      "step_name": "After normalization",
      "rows": 1450000,
      "patients": 49500,
      "row_loss": 50000,
      "patient_loss": 500,
      "row_loss_pct": 3.33,
      "patient_loss_pct": 1.0
    }
    // ... more steps
  ],
  "summary": {
    "total_row_loss": 1487655,
    "total_patient_loss": 48766,
    "final_row_retention_pct": 0.82,
    "final_patient_retention_pct": 2.47
  }
}
```

## Advanced Configuration

### DuckDB Optimization

#### **Global Imputation Settings**
```python
# Memory and threading configuration
duckdb_conn.sql(f"PRAGMA threads={threads}")  # 16 threads
duckdb_conn.sql(f"PRAGMA memory_limit='{effective_memory}GB'")  # 1TB
duckdb_conn.sql("PRAGMA enable_object_cache")  # Enable caching
duckdb_conn.sql("PRAGMA disable_verification")  # Performance optimization
duckdb_conn.sql("PRAGMA enable_optimizer")  # Ensure optimizer enabled
```

#### **Partition Processing Settings**
```python
# Per-worker configuration
duckdb_conn.sql(f"PRAGMA threads={threads}")  # 4 threads per worker
duckdb_conn.sql(f"PRAGMA memory_limit='{mem_gb}GB'")  # 8GB per worker
duckdb_conn.sql("PRAGMA enable_object_cache")  # Enable caching
duckdb_conn.sql("PRAGMA disable_verification")  # Performance optimization
```

### Threading Configuration Guidelines

**When to Use Multi-Threaded (Global Imputation):**
- Processing entire dataset at once with large memory allocation
- Memory-intensive operations (large `UNION ALL`, `JOIN` operations) that benefit from parallel processing
- Abundant memory available (1TB+), CPU utilization is important
- I/O operations, sorting, and computation can utilize multiple cores

**When to Use Multi-Threaded (Partition Processing):**
- Processing independent partitions in parallel
- CPU-intensive operations (drug normalization, data transformations)
- CPU is the bottleneck, not memory
- Can benefit from parallel processing across multiple cores

### CPU Utilization Troubleshooting

**Low CPU Usage (< 30%) - Global Imputation:**
- **Check**: Ensure using `--threads 16` (not 1)
- **Cause**: Single-threaded configuration severely limits CPU utilization
- **Fix**: Use multi-threaded configuration with 16 threads
- **Expected**: 70-90% CPU utilization with proper threading

**High CPU Usage (> 90%) - Partition Processing:**
- **Check**: Monitor memory usage per worker
- **Cause**: May indicate memory pressure or inefficient queries
- **Fix**: Reduce `--workers` count or increase `--mem-gb` per worker
- **Expected**: 60-80% CPU utilization with balanced resource usage

## Performance Monitoring

### Expected Performance Metrics

**Global Imputation Performance:**

| Configuration | Threads | Memory | CPU Usage | Processing Speed | Memory Efficiency |
|---------------|---------|--------|-----------|------------------|-------------------|
| **Single-Threaded** | 1 | 1TB | 20-25% | Slow | High |
| **Multi-Threaded** | 16 | 1TB | 70-90% | Fast | High |

**Key Improvements with Multi-Threading:**
- **3x CPU Utilization**: 20-25% â†’ 70-90%
- **Faster I/O**: Parallel S3 reads/writes
- **Better Sorting**: Parallel sorting for large `UNION ALL` operations
- **Optimized Computation**: Multiple cores for demographic imputation logic

### Monitoring Commands

```bash
# Monitor CPU usage
htop

# Monitor memory usage
free -h

# Monitor S3 operations
aws s3 ls s3://pgxdatalake/silver/imputed/ --recursive --human-readable --summarize
```

## Complete Workflow Example

### Full Optimized Pipeline Execution

```bash
#!/bin/bash
set -euo pipefail

# Phase 1: Global Imputation (Run Once)
echo "ðŸš€ Phase 1: Running Global Demographic Imputation..."
echo "ðŸ“ Input: Silver tier pharmacy and medical data"
echo "ðŸ“ Output: Imputed partitioned data + demographics lookup"

python global_imputation.py \
  --pharmacy-input s3://pgxdatalake/silver/pharmacy/**/*.parquet \
  --medical-input s3://pgxdatalake/silver/medical/**/*.parquet \
  --output-root s3://pgxdatalake/silver/imputed \
  --lookahead-years 5 \
  --threads 16 --mem-gb 1024 \
  --log-level INFO 2>&1 | tee "global_imputation_output.log"

# Phase 2: Optimized Pharmacy Processing
echo "ðŸš€ Phase 2a: Running Optimized Pharmacy Processing..."
echo "ðŸ“ Input: Silver tier imputed partitioned pharmacy data"
echo "ðŸ“ Output: Gold tier final pharmacy partitions"

python apcd_clean.py \
  --job pharmacy \
  --pharmacy-input s3://pgxdatalake/silver/imputed/pharmacy_partitioned/**/*.parquet \
  --demographics-lookup s3://pgxdatalake/silver/imputed/mi_person_key_demographics_lookup.parquet \
  --output-root s3://pgxdatalake/gold/pharmacy \
  --min-year 2016 --max-year 2020 \
  --workers 6 \
  --threads 4 --mem-gb 8 \
  --log-level INFO 2>&1 | tee "pharmacy_processing_output.log"

# Phase 2: Optimized Medical Processing
echo "ðŸš€ Phase 2b: Running Optimized Medical Processing..."
echo "ðŸ“ Input: Silver tier imputed partitioned medical data"
echo "ðŸ“ Output: Gold tier final medical partitions"

python apcd_clean.py \
  --job medical \
  --medical-input s3://pgxdatalake/silver/imputed/medical_partitioned/**/*.parquet \
  --demographics-lookup s3://pgxdatalake/silver/imputed/mi_person_key_demographics_lookup.parquet \
  --output-root s3://pgxdatalake/gold/medical \
  --min-year 2016 --max-year 2020 \
  --workers 6 \
  --threads 4 --mem-gb 8 \
  --log-level INFO 2>&1 | tee "medical_processing_output.log"

echo "âœ… Complete optimized pipeline completed successfully!"
```

## Jupyter Notebook Integration

### Complete Optimized Pipeline Cell
```python
%%bash
set -euo pipefail

# Complete Optimized Pipeline
echo "ðŸš€ Starting Complete Optimized PGX Analysis Pipeline"
echo " Started at: $(date)"
echo ""

# Phase 1: Global Imputation
python 1_apcd_input_data/2_global_imputation.py \
  --pharmacy-input s3://pgxdatalake/bronze/pharmacy/*.parquet \
  --medical-input s3://pgxdatalake/bronze/medical/*.parquet \
  --output-root s3://pgxdatalake/silver/imputed \
  --lookahead-years 5 \
  --log-level INFO

# Phase 2: Pharmacy Processing
python 1_apcd_input_data/3_apcd_clean.py \
  --job pharmacy \
  --pharmacy-input s3://pgxdatalake/silver/imputed/pharmacy_partitioned/**/*.parquet \
  --output-root s3://pgxdatalake/gold/pharmacy \
  --min-year 2016 --max-year 2020 \
  --workers 6

# Phase 2: Medical Processing
python 1_apcd_input_data/3_apcd_clean.py \
  --job medical \
  --medical-input s3://pgxdatalake/silver/imputed/medical_partitioned/**/*.parquet \
  --output-root s3://pgxdatalake/gold/medical \
  --min-year 2016 --max-year 2020 \
  --workers 6

echo ""
echo "âœ… Complete optimized pipeline completed at: $(date)"
```

## Troubleshooting

### Common Issues

**Out of Memory Errors:**
- **Global Imputation**: Ensure using `--mem-gb 1024` (1TB)
- **Partition Processing**: Reduce `--workers` or increase `--mem-gb` per worker

**Low CPU Utilization:**
- **Global Imputation**: Use `--threads 16` (not 1)
- **Partition Processing**: Check worker count and memory allocation

**S3 Access Errors:**
- Verify AWS credentials are configured
- Check S3 bucket permissions
- Ensure correct bucket names (`pgxdatalake`, `pgx-repository`)

**Table Not Found Errors:**
- Ensure global imputation completed successfully before partition processing
- Check that demographics lookup file exists in S3

### Performance Optimization

**For Large Datasets:**
- Use maximum available memory for global imputation
- Optimize worker count based on available CPU cores
- Monitor memory usage per worker during partition processing

**For Faster Processing:**
- Use SSD storage for temporary files
- Ensure high-bandwidth S3 access
- Optimize DuckDB settings for your hardware

## Data Flow Comparison

### Optimized Approach (Recommended)
```
Original Data (Silver)
    â†“
Global Imputation (Phase 1)
    â†“
Imputed Partitioned Data (Silver)
    â†“
Optimized Partition Processing (Phase 2)
    â†“
Final Gold Data
```

### Legacy Approach
```
Original Data (Silver)
    â†“
Global Imputation (Phase 1) - Demographics Only
    â†“
Demographics Lookup Table (Silver)
    â†“
Partition Processing with Lookup (Phase 2)
    â†“
Final Gold Data
```

## Support

For issues or questions:
1. Check the troubleshooting section above
2. Review log files for detailed error information
3. Monitor system resources (CPU, memory, disk I/O)
4. Verify S3 permissions and network connectivity

---

**Last Updated**: 2025-01-07  
**Version**: 3.0 (Optimized Pipeline Architecture)
