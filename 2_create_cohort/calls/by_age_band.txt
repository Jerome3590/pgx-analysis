import subprocess
import concurrent.futures
import boto3
import traceback
import sys
import os
import functools

from helpers_1997_13.cohort_utils import check_existing_cohorts as cu_check_existing_cohorts, run_cohort as cu_run_cohort

# Script configuration - use repository-relative entrypoint for portability
script_path = os.path.abspath(os.path.join(os.path.dirname(__file__), '..', '0_create_cohort.py'))
python_bin = sys.executable

age_band = "45-54"


def check_existing_cohorts(age_band):
    # Delegate to shared helper which accepts optional age_bands/event_years
    return cu_check_existing_cohorts(age_bands=[age_band])


def run_cohort(job):
    """Wrap the centralized runner and bind script_path/python_bin/target_icd."""
    target_icd = os.environ.get('PGX_TARGET_ICD_CODES', 'F1120')
    cb = functools.partial(cu_run_cohort, script_path=script_path, python_bin=python_bin, target_icd=target_icd)
    return cb(job)

# Check S3 for cohorts and locks
jobs_to_process = check_existing_cohorts(age_band)

if not jobs_to_process:
    print(f"\nAll cohorts for age band {age_band} already exist or are locked. No jobs to run.")
    exit(0)

# Calculate a reasonable number of workers based on job count
# Too many concurrent workers cause lock contention - stick to just 2
MAX_WORKERS = min(2, len(jobs_to_process))
print(f"\nStarting {len(jobs_to_process)} cohort processing jobs for age band {age_band} with {MAX_WORKERS} parallel workers...", flush=True)
print(f"{'='*80}", flush=True)

# Process a subset of jobs at a time to avoid output flooding
def process_job_batch(job_batch, batch_num, total_batches):
    results = []
    job_statuses = {}
    
    print(f"\nProcessing batch {batch_num}/{total_batches} with {len(job_batch)} jobs...", flush=True)
    
    with concurrent.futures.ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
        # Submit all jobs
        future_to_job = {executor.submit(run_cohort, job): job for job in job_batch}
        total_jobs = len(future_to_job)
        completed = 0
        
        # Process results as they complete
        for future in concurrent.futures.as_completed(future_to_job):
            job = future_to_job[future]
            job_id = f"{job['age_band']}/{job['event_year']}"
            completed += 1
            
            try:
                result = future.result()
                results.append(result)
                job_statuses[job_id] = result
                print(f"\n[{completed}/{total_jobs}] Job status: {result}", flush=True)
            except Exception as e:
                error_msg = f"Job execution error for {job_id}: {str(e)}"
                print(f"\n[{completed}/{total_jobs}] {error_msg}", flush=True)
                print(traceback.format_exc(), flush=True)
                results.append(f"ERROR: {job_id} - {str(e)}")
                job_statuses[job_id] = f"ERROR: {str(e)}"
            
            print(f"\nProgress: {completed}/{total_jobs} jobs completed ({100*completed/total_jobs:.1f}%)", flush=True)
            print(f"{'='*80}", flush=True)
    
    return results, job_statuses

# Split the jobs into smaller batches for better management
BATCH_SIZE = 2  # Process just 2 at a time to avoid output buffering issues
all_job_batches = [jobs_to_process[i:i+BATCH_SIZE] for i in range(0, len(jobs_to_process), BATCH_SIZE)]
all_results = []
all_job_statuses = {}

# Process each batch
for batch_num, job_batch in enumerate(all_job_batches, 1):
    batch_results, batch_statuses = process_job_batch(job_batch, batch_num, len(all_job_batches))
    all_results.extend(batch_results)
    all_job_statuses.update(batch_statuses)
    
    # Print a short batch summary
    print(f"\nBatch {batch_num}/{len(all_job_batches)} complete.", flush=True)
    
    # If there are more batches, pause briefly to let previous output complete
    if batch_num < len(all_job_batches):
        print(f"Pausing briefly before starting next batch...", flush=True)
        import time
        time.sleep(5)

# Summarize all results
print("\n" + "="*80)
print(f"FINAL SUMMARY OF RESULTS FOR AGE BAND {age_band}:")
print("="*80)
success_count = sum(1 for r in all_results if r.startswith("SUCCESS"))
locked_count = sum(1 for r in all_results if r.startswith("SKIPPED_LOCKED"))
failed_count = sum(1 for r in all_results if r.startswith("FAILED") or r.startswith("ERROR"))

print(f"✓ Successful: {success_count}")
print(f"⚠ Skipped (locked): {locked_count}")
print(f"✗ Failed: {failed_count}")

if all_job_statuses:
    print("\nDetailed status by job:")
    for job_id, status in sorted(all_job_statuses.items()):
        if status.startswith("SUCCESS"):
            status_icon = "✓"
        elif status.startswith("SKIPPED"):
            status_icon = "⚠"
        else:
            status_icon = "✗"
        print(f"{status_icon} {job_id}: {status}")

print("="*80) 