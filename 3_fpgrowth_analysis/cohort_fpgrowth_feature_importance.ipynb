{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cohort-Specific FPGrowth Feature Importance Analysis\n",
    "\n",
    "**Purpose:** Cohort-level frequent pattern mining for process mining and comparative analysis  \n",
    "**Updated:** November 23, 2025  \n",
    "**Hardware:** Optimized for EC2 (32 cores, 1TB RAM)  \n",
    "**Output:** `s3://pgxdatalake/gold/fpgrowth/cohort/{item_type}/cohort_name={cohort}/...`\n",
    "\n",
    "## Key Features\n",
    "\n",
    "‚úÖ **Three Item Types** - Drugs, ICD codes, CPT codes  \n",
    "‚úÖ **Cohort-Specific Patterns** - Discovers patterns unique to each cohort  \n",
    "‚úÖ **Parallel Processing** - Processes multiple cohorts simultaneously  \n",
    "‚úÖ **BupaR Integration** - Outputs ready for process mining workflows  \n",
    "‚úÖ **Comparative Analysis** - Compare patterns between OPIOID_ED and ED_NON_OPIOID cohorts\n",
    "\n",
    "## Methodology\n",
    "\n",
    "For each combination of (cohort, age_band, event_year, item_type):\n",
    "1. Extract items from cohort-specific data\n",
    "2. Create patient-level transactions\n",
    "3. Encode transactions into binary matrix\n",
    "4. Run FP-Growth to find frequent itemsets\n",
    "5. Generate association rules\n",
    "6. Save results to S3 in organized structure\n",
    "\n",
    "## Key Differences from Global Analysis\n",
    "\n",
    "| Aspect | Global FPGrowth | Cohort FPGrowth |\n",
    "|--------|-----------------|-----------------|\n",
    "| **Scope** | All patients (~5.7M) | Individual cohorts (~10K-100K) |\n",
    "| **Purpose** | Universal ML features | Process mining patterns |\n",
    "| **Support Threshold** | 0.01 (1%) | 0.05 (5%) |\n",
    "| **Output** | `global/{item_type}/` | `cohort/{item_type}/cohort_name={c}/...` |\n",
    "| **Use Case** | CatBoost consistency | BupaR pathway analysis |\n",
    "| **Parallelization** | Sequential by item type | Parallel by cohort |\n",
    "\n",
    "## Expected Runtime (EC2: 32 cores, 1TB RAM)\n",
    "\n",
    "- **Cohorts**: 2 (opioid_ed, ed_non_opioid)\n",
    "- **Age bands √ó Years**: ~100 combinations per cohort\n",
    "- **Item types**: 3 (drug_name, icd_code, cpt_code)\n",
    "- **Total jobs**: ~600 combinations\n",
    "- **Avg time per job**: ~1-2 minutes\n",
    "- **Total runtime**: ~2-4 hours (with MAX_WORKERS=4)\n",
    "\n",
    "## S3 Output Structure\n",
    "\n",
    "```\n",
    "s3://pgxdatalake/gold/fpgrowth/cohort/\n",
    "‚îú‚îÄ‚îÄ drug_name/\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ cohort_name=opioid_ed/\n",
    "‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ age_band=65-74/event_year=2020/\n",
    "‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ itemsets.json\n",
    "‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ rules.json\n",
    "‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ summary.json\n",
    "‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ ...\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ cohort_name=ed_non_opioid/...\n",
    "‚îú‚îÄ‚îÄ icd_code/\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ (same structure)\n",
    "‚îî‚îÄ‚îÄ cpt_code/\n",
    "    ‚îî‚îÄ‚îÄ (same structure)\n",
    "```\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import logging\n",
    "from pathlib import Path\n",
    "import time\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "\n",
    "# MLxtend for FP-Growth\n",
    "from mlxtend.frequent_patterns import fpgrowth, association_rules\n",
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "\n",
    "# Project root\n",
    "project_root = Path.cwd().parent if Path.cwd().name == '3_fpgrowth_analysis' else Path.cwd()\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "# Project utilities\n",
    "from helpers_1997_13.common_imports import s3_client, S3_BUCKET\n",
    "from helpers_1997_13.duckdb_utils import get_duckdb_connection\n",
    "from helpers_1997_13.s3_utils import save_to_s3_json, save_to_s3_parquet, get_cohort_parquet_path\n",
    "from helpers_1997_13.fpgrowth_utils import run_fpgrowth_drug_token_with_fallback, convert_frozensets\n",
    "from helpers_1997_13.visualization_utils import create_network_visualization\n",
    "from helpers_1997_13.constants import AGE_BANDS, EVENT_YEARS\n",
    "\n",
    "print(f\"‚úì Project root: {project_root}\")\n",
    "print(f\"‚úì All imports successful\")\n",
    "print(f\"‚úì Timestamp: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# EC2 CONFIGURATION (32 cores, 1TB RAM)\n",
    "# =============================================================================\n",
    "\n",
    "# FP-Growth parameters (higher threshold for cohort-specific patterns)\n",
    "MIN_SUPPORT = 0.05       # 5% support (items must appear in 5% of patients within cohort)\n",
    "MIN_CONFIDENCE = 0.3     # 30% confidence for association rules\n",
    "\n",
    "# Item types to process\n",
    "ITEM_TYPES = ['drug_name', 'icd_code', 'cpt_code']\n",
    "\n",
    "# Processing parameters\n",
    "MAX_WORKERS = 4  # Parallel workers (adjust based on available cores)\n",
    "COHORTS_TO_PROCESS = ['opioid_ed', 'ed_non_opioid']  # Specify cohorts to process\n",
    "\n",
    "# Paths\n",
    "S3_OUTPUT_BASE = f\"s3://{S3_BUCKET}/gold/fpgrowth/cohort\"\n",
    "LOCAL_DATA_PATH = project_root / \"data\" / \"gold\" / \"cohorts_F1120\"\n",
    "\n",
    "# Setup logger with file output (prevents Jupyter rate limit issues)\n",
    "logger = logging.getLogger('cohort_fpgrowth')\n",
    "logger.setLevel(logging.INFO)\n",
    "logger.handlers.clear()  # Clear any existing handlers\n",
    "\n",
    "# File handler - full logs to file\n",
    "log_file = project_root / \"3_fpgrowth_analysis\" / \"cohort_fpgrowth_execution.log\"\n",
    "file_handler = logging.FileHandler(log_file)\n",
    "file_handler.setLevel(logging.INFO)\n",
    "file_handler.setFormatter(logging.Formatter('%(asctime)s - %(levelname)s - %(message)s'))\n",
    "logger.addHandler(file_handler)\n",
    "\n",
    "# Console handler - only major milestones (prevents Jupyter rate limit)\n",
    "console_handler = logging.StreamHandler()\n",
    "console_handler.setLevel(logging.WARNING)  # Only warnings/errors to console\n",
    "console_handler.setFormatter(logging.Formatter('%(asctime)s - %(levelname)s - %(message)s'))\n",
    "logger.addHandler(console_handler)\n",
    "\n",
    "print(f\"‚úì Min Support: {MIN_SUPPORT}\")\n",
    "print(f\"‚úì Min Confidence: {MIN_CONFIDENCE}\")\n",
    "print(f\"‚úì Item Types: {ITEM_TYPES}\")\n",
    "print(f\"‚úì Max Workers: {MAX_WORKERS}\")\n",
    "print(f\"‚úì Cohorts: {COHORTS_TO_PROCESS}\")\n",
    "print(f\"‚úì S3 Output: {S3_OUTPUT_BASE}\")\n",
    "print(f\"‚úì Local Data: {LOCAL_DATA_PATH}\")\n",
    "print(f\"‚úì Local Data Exists: {LOCAL_DATA_PATH.exists()}\")\n",
    "print(f\"‚úì Detailed logs ‚Üí {log_file}\")\n",
    "print(f\"‚úì Console output: WARNING level only (check log file for progress)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Discover Available Cohorts\n",
    "\n",
    "Scan local data to find all available cohort combinations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discover_cohorts(local_data_path, cohort_filter=None):\n",
    "    \"\"\"\n",
    "    Discover all available cohort combinations from local data.\n",
    "    \"\"\"\n",
    "    cohort_jobs = []\n",
    "    \n",
    "    for cohort_dir in local_data_path.glob(\"cohort_name=*\"):\n",
    "        cohort_name = cohort_dir.name.replace(\"cohort_name=\", \"\")\n",
    "        \n",
    "        # Filter if specified\n",
    "        if cohort_filter and cohort_name not in cohort_filter:\n",
    "            continue\n",
    "        \n",
    "        for year_dir in cohort_dir.glob(\"event_year=*\"):\n",
    "            event_year = year_dir.name.replace(\"event_year=\", \"\")\n",
    "            \n",
    "            for age_dir in year_dir.glob(\"age_band=*\"):\n",
    "                age_band = age_dir.name.replace(\"age_band=\", \"\")\n",
    "                \n",
    "                # Check if cohort file exists\n",
    "                cohort_file = age_dir / \"cohort.parquet\"\n",
    "                if cohort_file.exists():\n",
    "                    cohort_jobs.append({\n",
    "                        'cohort': cohort_name,\n",
    "                        'age_band': age_band,\n",
    "                        'event_year': event_year,\n",
    "                        'local_path': str(cohort_file)\n",
    "                    })\n",
    "    \n",
    "    return cohort_jobs\n",
    "\n",
    "# Discover available cohorts\n",
    "cohort_jobs = discover_cohorts(LOCAL_DATA_PATH, cohort_filter=COHORTS_TO_PROCESS)\n",
    "\n",
    "print(f\"\\nüìä Discovered Cohorts:\")\n",
    "print(f\"  Total combinations: {len(cohort_jobs)}\")\n",
    "\n",
    "# Group by cohort\n",
    "cohort_counts = {}\n",
    "for job in cohort_jobs:\n",
    "    cohort_counts[job['cohort']] = cohort_counts.get(job['cohort'], 0) + 1\n",
    "\n",
    "for cohort, count in cohort_counts.items():\n",
    "    print(f\"  {cohort}: {count} combinations\")\n",
    "\n",
    "print(f\"\\n  Sample jobs:\")\n",
    "for job in cohort_jobs[:5]:\n",
    "    print(f\"    {job['cohort']}/{job['age_band']}/{job['event_year']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Define Cohort Processing Function\n",
    "\n",
    "Create a function to process a single cohort for a specific item type with FP-Growth.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_single_cohort(job, item_type):\n",
    "    \"\"\"Process a single cohort for a specific item type with FP-Growth analysis.\"\"\"\n",
    "    cohort = job['cohort']\n",
    "    age_band = job['age_band']\n",
    "    event_year = job['event_year']\n",
    "    local_path = job['local_path']\n",
    "    \n",
    "    cohort_logger = logging.getLogger(f\"{cohort}_{age_band}_{event_year}_{item_type}\")\n",
    "    cohort_logger.setLevel(logging.INFO)\n",
    "    \n",
    "    try:\n",
    "        start_time = time.time()\n",
    "        cohort_logger.info(f\"Processing {cohort}/{age_band}/{event_year} - {item_type}\")\n",
    "        \n",
    "        # Extract items based on type\n",
    "        con = get_duckdb_connection(logger=cohort_logger)\n",
    "        \n",
    "        if item_type == 'drug_name':\n",
    "            query = f\"\"\"\n",
    "            SELECT mi_person_key, drug_name as item\n",
    "            FROM read_parquet('{local_path}')\n",
    "            WHERE drug_name IS NOT NULL AND drug_name != '' AND event_type = 'pharmacy'\n",
    "            \"\"\"\n",
    "        elif item_type == 'icd_code':\n",
    "            query = f\"\"\"\n",
    "            WITH all_icds AS (\n",
    "                SELECT mi_person_key, primary_icd_diagnosis_code as icd FROM read_parquet('{local_path}') \n",
    "                WHERE primary_icd_diagnosis_code IS NOT NULL AND event_type = 'medical'\n",
    "                UNION ALL\n",
    "                SELECT mi_person_key, two_icd_diagnosis_code as icd FROM read_parquet('{local_path}') \n",
    "                WHERE two_icd_diagnosis_code IS NOT NULL AND event_type = 'medical'\n",
    "                UNION ALL\n",
    "                SELECT mi_person_key, three_icd_diagnosis_code as icd FROM read_parquet('{local_path}') \n",
    "                WHERE three_icd_diagnosis_code IS NOT NULL AND event_type = 'medical'\n",
    "                UNION ALL\n",
    "                SELECT mi_person_key, four_icd_diagnosis_code as icd FROM read_parquet('{local_path}') \n",
    "                WHERE four_icd_diagnosis_code IS NOT NULL AND event_type = 'medical'\n",
    "                UNION ALL\n",
    "                SELECT mi_person_key, five_icd_diagnosis_code as icd FROM read_parquet('{local_path}') \n",
    "                WHERE five_icd_diagnosis_code IS NOT NULL AND event_type = 'medical'\n",
    "            )\n",
    "            SELECT mi_person_key, icd as item FROM all_icds WHERE icd != ''\n",
    "            \"\"\"\n",
    "        elif item_type == 'cpt_code':\n",
    "            query = f\"\"\"\n",
    "            SELECT mi_person_key, procedure_code as item\n",
    "            FROM read_parquet('{local_path}')\n",
    "            WHERE procedure_code IS NOT NULL AND procedure_code != '' AND event_type = 'medical'\n",
    "            \"\"\"\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown item_type: {item_type}\")\n",
    "        \n",
    "        df = con.execute(query).df()\n",
    "        con.close()\n",
    "        \n",
    "        if df.empty:\n",
    "            cohort_logger.warning(f\"No {item_type} data for {cohort}/{age_band}/{event_year}\")\n",
    "            return (cohort, age_band, event_year, item_type, False, \"No data\")\n",
    "        \n",
    "        # Create transactions\n",
    "        cohort_logger.info(f\"Building {len(df)} transactions...\")\n",
    "        transactions = (\n",
    "            df.groupby('mi_person_key')['item']\n",
    "            .apply(lambda x: sorted(set(x.tolist())))\n",
    "            .tolist()\n",
    "        )\n",
    "        \n",
    "        if not transactions:\n",
    "            cohort_logger.warning(f\"No valid transactions for {cohort}/{age_band}/{event_year}\")\n",
    "            return (cohort, age_band, event_year, item_type, False, \"No transactions\")\n",
    "        \n",
    "        # Encode transactions\n",
    "        cohort_logger.info(f\"Encoding {len(transactions)} transactions...\")\n",
    "        te = TransactionEncoder()\n",
    "        te_ary = te.fit(transactions).transform(transactions)\n",
    "        df_encoded = pd.DataFrame(te_ary, columns=te.columns_)\n",
    "        \n",
    "        # Run FP-Growth\n",
    "        cohort_logger.info(f\"Running FP-Growth (min_support={MIN_SUPPORT})...\")\n",
    "        itemsets = fpgrowth(df_encoded, min_support=MIN_SUPPORT, use_colnames=True)\n",
    "        itemsets = itemsets.sort_values('support', ascending=False).reset_index(drop=True)\n",
    "        \n",
    "        if itemsets.empty:\n",
    "            cohort_logger.warning(f\"No itemsets found for {cohort}/{age_band}/{event_year}\")\n",
    "            return (cohort, age_band, event_year, item_type, False, \"No itemsets\")\n",
    "        \n",
    "        # Generate rules\n",
    "        cohort_logger.info(f\"Generating association rules...\")\n",
    "        rules = association_rules(itemsets, metric=\"confidence\", min_threshold=MIN_CONFIDENCE)\n",
    "        rules = rules.sort_values('lift', ascending=False).reset_index(drop=True)\n",
    "        \n",
    "        # Convert frozensets for JSON\n",
    "        itemsets_json = itemsets.copy()\n",
    "        itemsets_json['itemsets'] = itemsets_json['itemsets'].apply(list)\n",
    "        \n",
    "        rules_json = rules.copy() if not rules.empty else pd.DataFrame()\n",
    "        if not rules_json.empty:\n",
    "            rules_json['antecedents'] = rules_json['antecedents'].apply(list)\n",
    "            rules_json['consequents'] = rules_json['consequents'].apply(list)\n",
    "        \n",
    "        # Save to S3\n",
    "        s3_base = f\"{S3_OUTPUT_BASE}/{item_type}/cohort_name={cohort}/age_band={age_band}/event_year={event_year}\"\n",
    "        \n",
    "        itemsets_path = f\"{s3_base}/itemsets.json\"\n",
    "        save_to_s3_json(itemsets_json.to_dict(orient='records'), itemsets_path)\n",
    "        \n",
    "        if not rules_json.empty:\n",
    "            rules_path = f\"{s3_base}/rules.json\"\n",
    "            save_to_s3_json(rules_json.to_dict(orient='records'), rules_path)\n",
    "        \n",
    "        summary = {\n",
    "            'timestamp': datetime.now().isoformat(),\n",
    "            'cohort': cohort, 'age_band': age_band, 'event_year': event_year,\n",
    "            'item_type': item_type,\n",
    "            'total_patients': len(transactions),\n",
    "            'total_itemsets': len(itemsets),\n",
    "            'total_rules': len(rules),\n",
    "            'min_support': MIN_SUPPORT,\n",
    "            'min_confidence': MIN_CONFIDENCE\n",
    "        }\n",
    "        summary_path = f\"{s3_base}/summary.json\"\n",
    "        save_to_s3_json(summary, summary_path)\n",
    "        \n",
    "        elapsed = time.time() - start_time\n",
    "        cohort_logger.info(f\"‚úì Completed in {elapsed:.1f}s\")\n",
    "        \n",
    "        return (cohort, age_band, event_year, item_type, True, f\"{len(itemsets)} itemsets, {len(rules)} rules\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        cohort_logger.error(f\"Error: {e}\")\n",
    "        return (cohort, age_band, event_year, item_type, False, str(e))\n",
    "\n",
    "print(\"‚úì Cohort processing function defined\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Process Cohorts in Parallel\n",
    "\n",
    "Run FP-Growth for all cohort combinations using parallel processing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"COHORT FPGROWTH ANALYSIS - START\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Cohorts: {len(cohort_jobs)} combinations\")\n",
    "print(f\"Item types: {ITEM_TYPES}\")\n",
    "print(f\"Total jobs: {len(cohort_jobs) * len(ITEM_TYPES)}\")\n",
    "print(f\"Max workers: {MAX_WORKERS}\")\n",
    "print(f\"Detailed progress ‚Üí Check log file\")\n",
    "print()\n",
    "\n",
    "logger.info(f\"\\n{'='*80}\")\n",
    "logger.info(f\"COHORT FPGROWTH ANALYSIS - START\")\n",
    "logger.info(f\"{'='*80}\")\n",
    "logger.info(f\"Cohorts: {len(cohort_jobs)} combinations\")\n",
    "logger.info(f\"Item types: {ITEM_TYPES}\")\n",
    "logger.info(f\"Total jobs: {len(cohort_jobs) * len(ITEM_TYPES)}\")\n",
    "\n",
    "start_time = time.time()\n",
    "results = []\n",
    "completed = 0\n",
    "failed = 0\n",
    "\n",
    "# Create all combinations of cohorts and item types\n",
    "all_jobs = [(job, item_type) for job in cohort_jobs for item_type in ITEM_TYPES]\n",
    "total_jobs = len(all_jobs)\n",
    "\n",
    "with ProcessPoolExecutor(max_workers=MAX_WORKERS) as executor:\n",
    "    # Submit all jobs\n",
    "    future_to_params = {executor.submit(process_single_cohort, job, item_type): (job, item_type) \n",
    "                        for job, item_type in all_jobs}\n",
    "    \n",
    "    # Process results as they complete\n",
    "    for future in as_completed(future_to_params):\n",
    "        job, item_type = future_to_params[future]\n",
    "        try:\n",
    "            cohort, age_band, event_year, item_type, success, message = future.result()\n",
    "            results.append({\n",
    "                'cohort': cohort,\n",
    "                'age_band': age_band,\n",
    "                'event_year': event_year,\n",
    "                'item_type': item_type,\n",
    "                'success': success,\n",
    "                'message': message\n",
    "            })\n",
    "            \n",
    "            if success:\n",
    "                completed += 1\n",
    "                logger.info(f\"[{completed + failed}/{total_jobs}] ‚úì {cohort}/{age_band}/{event_year}/{item_type}: {message}\")\n",
    "                # Print every 10 successes or milestones\n",
    "                if completed % 10 == 0 or completed == total_jobs:\n",
    "                    print(f\"Progress: {completed}/{total_jobs} completed ({completed/total_jobs*100:.1f}%), {failed} failed\")\n",
    "            else:\n",
    "                failed += 1\n",
    "                logger.warning(f\"[{completed + failed}/{total_jobs}] ‚úó {cohort}/{age_band}/{event_year}/{item_type}: {message}\")\n",
    "                print(f\"‚ö† Failed: {cohort}/{age_band}/{event_year}/{item_type}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            failed += 1\n",
    "            logger.error(f\"[{completed + failed}/{total_jobs}] ‚úó {job['cohort']}/{job['age_band']}/{job['event_year']}/{item_type}: {e}\")\n",
    "            print(f\"‚ö† Error: {job['cohort']}/{job['age_band']}/{job['event_year']}/{item_type}\")\n",
    "            results.append({\n",
    "                'cohort': job['cohort'],\n",
    "                'age_band': job['age_band'],\n",
    "                'event_year': job['event_year'],\n",
    "                'item_type': item_type,\n",
    "                'success': False,\n",
    "                'message': str(e)\n",
    "            })\n",
    "\n",
    "elapsed = time.time() - start_time\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"COHORT FPGROWTH ANALYSIS - COMPLETE\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"  Total jobs: {total_jobs}\")\n",
    "print(f\"  Successful: {completed}\")\n",
    "print(f\"  Failed: {failed}\")\n",
    "print(f\"  Success rate: {completed/total_jobs*100:.1f}%\")\n",
    "print(f\"  Total time: {elapsed:.1f}s ({elapsed/60:.1f}min)\")\n",
    "print(f\"  Avg time per job: {elapsed/total_jobs:.1f}s\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Analyze Results\n",
    "\n",
    "Review processing results and identify any issues.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert results to DataFrame for analysis\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "print(\"\\nüìä Results by Cohort and Item Type:\")\n",
    "summary = results_df.groupby(['cohort', 'item_type'])['success'].agg([\n",
    "    ('total', 'count'), \n",
    "    ('successful', 'sum'),\n",
    "    ('success_rate', lambda x: f\"{x.mean()*100:.1f}%\")\n",
    "])\n",
    "print(summary)\n",
    "\n",
    "print(\"\\nüìä Results by Item Type:\")\n",
    "item_summary = results_df.groupby('item_type')['success'].agg([\n",
    "    ('total', 'count'), \n",
    "    ('successful', 'sum'),\n",
    "    ('success_rate', lambda x: f\"{x.mean()*100:.1f}%\")\n",
    "])\n",
    "print(item_summary)\n",
    "\n",
    "print(\"\\n‚ùå Failed Jobs:\")\n",
    "failed_df = results_df[~results_df['success']]\n",
    "if not failed_df.empty:\n",
    "    print(failed_df[['cohort', 'age_band', 'event_year', 'item_type', 'message']].to_string())\n",
    "else:\n",
    "    print(\"  None! All jobs completed successfully.\")\n",
    "\n",
    "print(\"\\n‚úì Successful Jobs Sample:\")\n",
    "success_df = results_df[results_df['success']]\n",
    "if not success_df.empty:\n",
    "    print(success_df[['cohort', 'age_band', 'event_year', 'item_type', 'message']].head(15))\n",
    "else:\n",
    "    print(\"  No successful jobs.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Next Steps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"COHORT FPGROWTH ANALYSIS - SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nüìä Processing Statistics:\")\n",
    "print(f\"  Cohort combinations: {len(cohort_jobs)}\")\n",
    "print(f\"  Item types: {len(ITEM_TYPES)} (drug_name, icd_code, cpt_code)\")\n",
    "print(f\"  Total jobs: {total_jobs}\")\n",
    "print(f\"  Successfully processed: {completed}\")\n",
    "print(f\"  Failed: {failed}\")\n",
    "print(f\"  Success rate: {completed/total_jobs*100:.1f}%\")\n",
    "print(f\"  Processing time: {elapsed:.1f}s ({elapsed/60:.1f}min)\")\n",
    "\n",
    "print(f\"\\nüîç FP-Growth Configuration:\")\n",
    "print(f\"  Min support: {MIN_SUPPORT} ({MIN_SUPPORT*100:.1f}%)\")\n",
    "print(f\"  Min confidence: {MIN_CONFIDENCE} ({MIN_CONFIDENCE*100:.1f}%)\")\n",
    "print(f\"  Parallel workers: {MAX_WORKERS}\")\n",
    "\n",
    "print(f\"\\nüíæ Output Location:\")\n",
    "print(f\"  S3 Base: {S3_OUTPUT_BASE}\")\n",
    "print(f\"  Structure: <item_type>/cohort_name=<name>/age_band=<band>/event_year=<year>/\")\n",
    "print(f\"  Item types:\")\n",
    "print(f\"    - drug_name/ (pharmacy events)\")\n",
    "print(f\"    - icd_code/ (diagnosis codes)\")\n",
    "print(f\"    - cpt_code/ (procedure codes)\")\n",
    "print(f\"  Files per cohort:\")\n",
    "print(f\"    - itemsets.json (frequent itemsets)\")\n",
    "print(f\"    - rules.json (association rules)\")\n",
    "print(f\"    - summary.json (metadata)\")\n",
    "\n",
    "print(f\"\\nüéØ Next Steps:\")\n",
    "print(f\"  1. Load cohort-specific itemsets for BupaR process mining\")\n",
    "print(f\"  2. Compare patterns between OPIOID_ED and ED_NON_OPIOID cohorts across item types\")\n",
    "print(f\"  3. Use association rules for pathway analysis\")\n",
    "print(f\"  4. Filter features for cohort-specific CatBoost models\")\n",
    "print(f\"  5. Create network visualizations for cohort-specific patterns\")\n",
    "\n",
    "print(f\"\\nüìù Example Usage:\")\n",
    "print(f\"  # Load drug patterns for a specific cohort\")\n",
    "print(f\"  from helpers_1997_13.s3_utils import load_from_s3_json\")\n",
    "print(f\"  itemsets = load_from_s3_json('{S3_OUTPUT_BASE}/drug_name/cohort_name=opioid_ed/age_band=65-74/event_year=2020/itemsets.json')\")\n",
    "print(f\"  # Load ICD patterns for same cohort\")\n",
    "print(f\"  icd_itemsets = load_from_s3_json('{S3_OUTPUT_BASE}/icd_code/cohort_name=opioid_ed/age_band=65-74/event_year=2020/itemsets.json')\")\n",
    "\n",
    "print(f\"\\n‚úì Analysis complete: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(\"=\"*80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
