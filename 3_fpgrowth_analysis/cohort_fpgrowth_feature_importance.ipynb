{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cohort-Specific FPGrowth Feature Importance Analysis\n",
    "\n",
    "**Purpose:** Cohort-level frequent pattern mining for process mining and comparative analysis  \n",
    "**Updated:** November 23, 2025  \n",
    "**Hardware:** Optimized for EC2 (32 cores, 1TB RAM)  \n",
    "**Output:** `s3://pgxdatalake/gold/fpgrowth/cohort/{item_type}/cohort_name={cohort}/...`\n",
    "\n",
    "Key Features\n",
    "\n",
    "‚úÖ **Three Item Types** - Drugs, ICD codes, CPT codes  \n",
    "‚úÖ **Cohort-Specific Patterns** - Discovers patterns unique to each cohort  \n",
    "‚úÖ **Parallel Processing** - Processes multiple cohorts simultaneously  \n",
    "‚úÖ **BupaR Integration** - Outputs ready for process mining workflows  \n",
    "‚úÖ **Comparative Analysis** - Compare patterns between OPIOID_ED and ED_NON_OPIOID cohorts\n",
    "\n",
    "Methodology\n",
    "\n",
    "For each combination of (cohort, age_band, event_year, item_type):\n",
    "1. Extract items from cohort-specific data\n",
    "2. Create patient-level transactions\n",
    "3. Encode transactions into binary matrix\n",
    "4. Run FP-Growth to find frequent itemsets\n",
    "5. Generate association rules\n",
    "6. Save results to S3 in organized structure\n",
    "\n",
    "Key Differences from Global Analysis\n",
    "\n",
    "| Aspect | Global FPGrowth | Cohort FPGrowth |\n",
    "|--------|-----------------|-----------------|\n",
    "| **Scope** | All patients (~5.7M) | Individual cohorts (~10K-100K) |\n",
    "| **Purpose** | Universal ML features | Process mining patterns |\n",
    "| **Support Threshold** | 0.01 (1%) | 0.05 (5%) |\n",
    "| **Output** | `global/{item_type}/` | `cohort/{item_type}/cohort_name={c}/...` |\n",
    "| **Use Case** | CatBoost consistency | BupaR pathway analysis |\n",
    "| **Parallelization** | Sequential by item type | Parallel by cohort |\n",
    "\n",
    "Expected Runtime (EC2: 32 cores, 1TB RAM)\n",
    "\n",
    "- **Cohorts**: 2 (opioid_ed, ed_non_opioid)\n",
    "- **Age bands √ó Years**: ~100 combinations per cohort\n",
    "- **Item types**: 3 (drug_name, icd_code, cpt_code)\n",
    "- **Total jobs**: ~600 combinations\n",
    "- **Avg time per job**: ~1-2 minutes\n",
    "- **Total runtime**: ~2-4 hours (with MAX_WORKERS=4)\n",
    "\n",
    "S3 Output Structure\n",
    "\n",
    "```\n",
    "s3://pgxdatalake/gold/fpgrowth/cohort/\n",
    "‚îú‚îÄ‚îÄ drug_name/\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ cohort_name=opioid_ed/\n",
    "‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ age_band=65-74/event_year=2020/\n",
    "‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ itemsets.json\n",
    "‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ rules.json\n",
    "‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ summary.json\n",
    "‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ ...\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ cohort_name=ed_non_opioid/...\n",
    "‚îú‚îÄ‚îÄ icd_code/\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ (same structure)\n",
    "‚îî‚îÄ‚îÄ cpt_code/\n",
    "    ‚îî‚îÄ‚îÄ (same structure)\n",
    "```\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Setup and Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Project root: /home/pgx3874/pgx-analysis\n",
      "‚úì All imports successful\n",
      "‚úì Timestamp: 2025-11-24 00:28:54\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import logging\n",
    "from pathlib import Path\n",
    "import psutil\n",
    "import duckdb\n",
    "import time\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "\n",
    "# MLxtend for FP-Growth\n",
    "from mlxtend.frequent_patterns import fpgrowth, association_rules\n",
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "\n",
    "# Project root\n",
    "project_root = Path.cwd().parent if Path.cwd().name == '3_fpgrowth_analysis' else Path.cwd()\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "# Project utilities\n",
    "from helpers_1997_13.common_imports import s3_client, S3_BUCKET\n",
    "from helpers_1997_13.duckdb_utils import get_duckdb_connection\n",
    "from helpers_1997_13.s3_utils import save_to_s3_json, save_to_s3_parquet, get_cohort_parquet_path\n",
    "from helpers_1997_13.fpgrowth_utils import run_fpgrowth_drug_token_with_fallback, convert_frozensets\n",
    "from helpers_1997_13.visualization_utils import create_network_visualization\n",
    "from helpers_1997_13.constants import AGE_BANDS, EVENT_YEARS\n",
    "\n",
    "print(f\"‚úì Project root: {project_root}\")\n",
    "print(f\"‚úì All imports successful\")\n",
    "print(f\"‚úì Timestamp: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EC2 Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Min Support: 0.05\n",
      "‚úì Min Confidence: 0.3\n",
      "‚úì Item Types: ['drug_name', 'icd_code', 'cpt_code']\n",
      "‚úì Max Workers: 4\n",
      "‚úì Cohorts: ['opioid_ed', 'ed_non_opioid']\n",
      "‚úì S3 Output: s3://pgxdatalake/gold/fpgrowth/cohort\n",
      "‚úì Local Data: /home/pgx3874/pgx-analysis/data\n",
      "‚úì Local Data Exists: True\n",
      "‚úì Detailed logs ‚Üí /home/pgx3874/pgx-analysis/3_fpgrowth_analysis/cohort_fpgrowth_execution.log\n",
      "‚úì Console output: WARNING level only (check log file for progress)\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# EC2 CONFIGURATION (32 cores, 1TB RAM)\n",
    "# =============================================================================\n",
    "\n",
    "# FP-Growth parameters (higher threshold for cohort-specific patterns)\n",
    "MIN_SUPPORT = 0.05       # 5% support (items must appear in 5% of patients within cohort)\n",
    "MIN_CONFIDENCE = 0.5     # 50% confidence - only strong associations\n",
    "\n",
    "# CPT-specific parameters (prevent memory exhaustion from millions of rules)\n",
    "MIN_SUPPORT_CPT = 0.15   # 15% support for CPT codes (focuses on common patterns)\n",
    "MIN_CONFIDENCE_CPT = 0.6 # 60% confidence for CPT (very strong associations only)\n",
    "\n",
    "# Rule limits (focus on most important rules)\n",
    "MAX_RULES_PER_COHORT = 1000  # Keep top 1000 rules by lift (practical limit)\n",
    "\n",
    "# Target-focused rule mining (NEW!)\n",
    "TARGET_FOCUSED = True  # Only generate rules that predict target outcomes\n",
    "TARGET_ICD_CODES = ['F11.20', 'F11.21', 'F11.22', 'F11.23', 'F11.24', 'F11.25', 'F11.29']  # Opioid dependence codes\n",
    "TARGET_HCG_LINES = [\n",
    "    \"P51 - ER Visits and Observation Care\",\n",
    "    \"O11 - Emergency Room\",\n",
    "    \"P33 - Urgent Care Visits\"\n",
    "]  # ED visits (HCG Line codes - matches phase2_event_processing.py)\n",
    "TARGET_PREFIXES = ['TARGET_ICD:', 'TARGET_ED:']  # Prefixes for target items in transactions\n",
    "\n",
    "# Item types to process\n",
    "ITEM_TYPES = ['drug_name', 'icd_code', 'cpt_code']\n",
    "\n",
    "# Processing parameters\n",
    "MAX_WORKERS = 1  # Sequential processing to prevent memory issues\n",
    "\n",
    "# DRY RUN MODE (test with limited cohorts first)\n",
    "DRY_RUN = True  # Set to False to process all cohorts\n",
    "DRY_RUN_LIMIT = 5  # Number of cohort combinations to process in dry run\n",
    "COHORTS_TO_PROCESS = ['opioid_ed', 'ed_non_opioid']  # Specify cohorts to process\n",
    "\n",
    "# Paths\n",
    "S3_OUTPUT_BASE = f\"s3://{S3_BUCKET}/gold/fpgrowth/cohort\"\n",
    "LOCAL_DATA_PATH = Path(\"/mnt/nvme/cohorts\")  # Instance storage (NVMe SSD for fast I/O)\n",
    "\n",
    "# Setup logger with file output (prevents Jupyter rate limit issues)\n",
    "logger = logging.getLogger('cohort_fpgrowth')\n",
    "logger.setLevel(logging.INFO)\n",
    "logger.handlers.clear()  # Clear any existing handlers\n",
    "\n",
    "# File handler - full logs to file\n",
    "log_file = project_root / \"3_fpgrowth_analysis\" / \"cohort_fpgrowth_execution.log\"\n",
    "file_handler = logging.FileHandler(log_file)\n",
    "file_handler.setLevel(logging.INFO)\n",
    "file_handler.setFormatter(logging.Formatter('%(asctime)s - %(levelname)s - %(message)s'))\n",
    "logger.addHandler(file_handler)\n",
    "\n",
    "# Console handler - only major milestones (prevents Jupyter rate limit)\n",
    "console_handler = logging.StreamHandler()\n",
    "console_handler.setLevel(logging.WARNING)  # Only warnings/errors to console\n",
    "console_handler.setFormatter(logging.Formatter('%(asctime)s - %(levelname)s - %(message)s'))\n",
    "logger.addHandler(console_handler)\n",
    "\n",
    "print(f\"‚úì Min Support (drug/ICD): {MIN_SUPPORT} (5%)\")\n",
    "print(f\"‚úì Min Support (CPT): {MIN_SUPPORT_CPT} (15% - focuses on common patterns)\")\n",
    "print(f\"‚úì Min Confidence (drug/ICD): {MIN_CONFIDENCE} (50% - strong associations)\")\n",
    "print(f\"‚úì Min Confidence (CPT): {MIN_CONFIDENCE_CPT} (60% - very strong associations)\")\n",
    "print(f\"‚úì Max Rules per Cohort: {MAX_RULES_PER_COHORT:,} (top rules by lift)\")\n",
    "print(f\"‚úì Item Types: {ITEM_TYPES}\")\n",
    "print(f\"‚úì Max Workers: {MAX_WORKERS} (sequential - prevents OOM)\")\n",
    "if DRY_RUN:\n",
    "    print(f\"‚úì DRY RUN MODE: Processing only {DRY_RUN_LIMIT} cohort combinations\")\n",
    "    print(f\"  ‚Üí Set DRY_RUN = False to process all cohorts\")\n",
    "else:\n",
    "    print(f\"‚úì FULL RUN MODE: Processing all cohorts\")\n",
    "print(f\"‚úì Cohorts: {COHORTS_TO_PROCESS}\")\n",
    "print(f\"‚úì S3 Output: {S3_OUTPUT_BASE}\")\n",
    "print(f\"‚úì S3 Retry: 3 attempts with exponential backoff\")\n",
    "print(f\"‚úì Local Data: {LOCAL_DATA_PATH}\")\n",
    "print(f\"‚úì Local Data Exists: {LOCAL_DATA_PATH.exists()}\")\n",
    "print(f\"‚úì Detailed logs ‚Üí {log_file}\")\n",
    "print(f\"‚úì Console output: WARNING level only (check log file for progress)\")\n",
    "print(\"\\nüéØ Quality Over Quantity Approach:\")\n",
    "print(\"  - High confidence thresholds (50-60%) = meaningful patterns only\")\n",
    "print(\"  - CPT uses 15% support (vs 5%) = focuses on common procedures\")\n",
    "print(\"  - Top 1,000 rules by lift = actionable insights, not exhaustive lists\")\n",
    "print(\"  - 2 parallel workers = stable memory usage\")\n",
    "print(f\"\\nüéØ TARGET-FOCUSED RULE MINING: {'ENABLED' if TARGET_FOCUSED else 'DISABLED'}\")\n",
    "if TARGET_FOCUSED:\n",
    "    print(f\"  - Target ICD codes: {TARGET_ICD_CODES}\")\n",
    "    print(f\"  - Target HCG lines (ED visits): {TARGET_HCG_LINES}\")\n",
    "    print(\"  - Only generates rules that PREDICT target outcomes\")\n",
    "    print(\"  - Example: {Metoprolol, Gabapentin} ‚Üí {TARGET_ICD:OPIOID_DEPENDENCE}\")\n",
    "    print(\"  - Example: {99213: Office Visit, J0670: Morphine} ‚Üí {TARGET_ED:EMERGENCY_DEPT}\")\n",
    "    print(\"  ‚úÖ Drastically reduces rule count (only predictive patterns)\")\n",
    "    print(\"  ‚úÖ More actionable for BupaR (pathways to target)\")\n",
    "    print(\"  ‚úÖ Better for CatBoost (features that predict outcome)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mem_logging_header",
   "metadata": {},
   "source": [
    "## Memory Monitoring\n",
    "\n",
    "Helper function to log memory usage at critical points.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mem_logging_func",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_memory(logger, stage=\"\"):\n",
    "    \"\"\"Log current memory usage.\"\"\"\n",
    "    try:\n",
    "        mem = psutil.virtual_memory()\n",
    "        mem_used_gb = mem.used / (1024**3)\n",
    "        mem_total_gb = mem.total / (1024**3)\n",
    "        mem_percent = mem.percent\n",
    "        mem_avail_gb = mem.available / (1024**3)\n",
    "        \n",
    "        logger.info(f\"[MEMORY {stage}] Used: {mem_used_gb:.1f} GB / {mem_total_gb:.1f} GB ({mem_percent:.1f}%) | Available: {mem_avail_gb:.1f} GB\")\n",
    "        \n",
    "        # Warning if memory usage is high\n",
    "        if mem_percent > 85:\n",
    "            logger.warning(f\"‚ö†Ô∏è  HIGH MEMORY USAGE: {mem_percent:.1f}% - May cause OOM!\")\n",
    "        \n",
    "        return mem_percent\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error getting memory info: {e}\")\n",
    "        return 0.0\n",
    "\n",
    "print(\"‚úì Memory logging function defined\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Discover Available Cohorts\n",
    "\n",
    "Scan local data to find all available cohort combinations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Discovered Cohorts:\n",
      "  Total combinations: 45\n",
      "  opioid_ed: 45 combinations\n",
      "\n",
      "  Sample jobs:\n",
      "    opioid_ed/13-24/2016\n",
      "    opioid_ed/25-44/2016\n",
      "    opioid_ed/45-54/2016\n",
      "    opioid_ed/55-64/2016\n",
      "    opioid_ed/65-74/2016\n"
     ]
    }
   ],
   "source": [
    "def discover_cohorts(local_data_path, cohort_filter=None):\n",
    "    \"\"\"\n",
    "    Discover all available cohort combinations from local data.\n",
    "    \"\"\"\n",
    "    cohort_jobs = []\n",
    "    \n",
    "    for cohort_dir in local_data_path.glob(\"cohort_name=*\"):\n",
    "        cohort_name = cohort_dir.name.replace(\"cohort_name=\", \"\")\n",
    "        \n",
    "        # Filter if specified\n",
    "        if cohort_filter and cohort_name not in cohort_filter:\n",
    "            continue\n",
    "        \n",
    "        for year_dir in cohort_dir.glob(\"event_year=*\"):\n",
    "            event_year = year_dir.name.replace(\"event_year=\", \"\")\n",
    "            \n",
    "            for age_dir in year_dir.glob(\"age_band=*\"):\n",
    "                age_band = age_dir.name.replace(\"age_band=\", \"\")\n",
    "                \n",
    "                # Check if cohort file exists\n",
    "                cohort_file = age_dir / \"cohort.parquet\"\n",
    "                if cohort_file.exists():\n",
    "                    cohort_jobs.append({\n",
    "                        'cohort': cohort_name,\n",
    "                        'age_band': age_band,\n",
    "                        'event_year': event_year,\n",
    "                        'local_path': str(cohort_file)\n",
    "                    })\n",
    "    \n",
    "    return cohort_jobs\n",
    "\n",
    "# Discover available cohorts\n",
    "cohort_jobs = discover_cohorts(LOCAL_DATA_PATH, cohort_filter=COHORTS_TO_PROCESS)\n",
    "\n",
    "# Apply DRY_RUN limit if enabled\n",
    "if DRY_RUN and len(cohort_jobs) > DRY_RUN_LIMIT:\n",
    "    print(f\"\\n‚ö†Ô∏è  DRY RUN: Limiting from {len(cohort_jobs)} to {DRY_RUN_LIMIT} cohort combinations\")\n",
    "    cohort_jobs = cohort_jobs[:DRY_RUN_LIMIT]\n",
    "\n",
    "print(f\"\\nüìä Discovered Cohorts:\")\n",
    "print(f\"  Total combinations: {len(cohort_jobs)}\")\n",
    "\n",
    "# Group by cohort\n",
    "cohort_counts = {}\n",
    "for job in cohort_jobs:\n",
    "    cohort_counts[job['cohort']] = cohort_counts.get(job['cohort'], 0) + 1\n",
    "\n",
    "for cohort, count in cohort_counts.items():\n",
    "    print(f\"  {cohort}: {count} combinations\")\n",
    "\n",
    "print(f\"\\n  Sample jobs:\")\n",
    "for job in cohort_jobs[:5]:\n",
    "    print(f\"    {job['cohort']}/{job['age_band']}/{job['event_year']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Define Cohort Processing Function\n",
    "\n",
    "Create a function to process a single cohort for a specific item type with FP-Growth.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Cohort processing function defined\n"
     ]
    }
   ],
   "source": [
    "def process_single_cohort(job, item_type):\n",
    "    \"\"\"Process a single cohort for a specific item type with FP-Growth analysis.\"\"\"\n",
    "    cohort = job['cohort']\n",
    "    age_band = job['age_band']\n",
    "    event_year = job['event_year']\n",
    "    local_path = job['local_path']\n",
    "    \n",
    "    cohort_logger = logging.getLogger(f\"{cohort}_{age_band}_{event_year}_{item_type}\")\n",
    "    cohort_logger.setLevel(logging.INFO)\n",
    "    \n",
    "    try:\n",
    "        start_time = time.time()\n",
    "        cohort_logger.info(f\"Processing {cohort}/{age_band}/{event_year} - {item_type}\")\n",
    "        log_memory(cohort_logger, \"START\")\n",
    "        \n",
    "        # Extract items based on type + TARGET MARKERS (for target-focused rules)\n",
    "        # Simple in-memory connection (no AWS needed for local parquet reads)\n",
    "        con = duckdb.connect(':memory:')\n",
    "        con.sql(\"SET threads = 1\")\n",
    "        \n",
    "        if item_type == 'drug_name':\n",
    "            query = f\"\"\"\n",
    "            SELECT mi_person_key, drug_name as item\n",
    "            FROM read_parquet('{local_path}')\n",
    "            WHERE drug_name IS NOT NULL AND drug_name != '' AND event_type = 'pharmacy'\n",
    "            \"\"\"\n",
    "        elif item_type == 'icd_code':\n",
    "            # For ICD codes: extract all diagnosis codes + mark target opioid codes\n",
    "            query = f\"\"\"\n",
    "            WITH all_icds AS (\n",
    "                SELECT mi_person_key, primary_icd_diagnosis_code as icd FROM read_parquet('{local_path}') \n",
    "                WHERE primary_icd_diagnosis_code IS NOT NULL AND event_type = 'medical'\n",
    "                UNION ALL\n",
    "                SELECT mi_person_key, two_icd_diagnosis_code as icd FROM read_parquet('{local_path}') \n",
    "                WHERE two_icd_diagnosis_code IS NOT NULL AND event_type = 'medical'\n",
    "                UNION ALL\n",
    "                SELECT mi_person_key, three_icd_diagnosis_code as icd FROM read_parquet('{local_path}') \n",
    "                WHERE three_icd_diagnosis_code IS NOT NULL AND event_type = 'medical'\n",
    "                UNION ALL\n",
    "                SELECT mi_person_key, four_icd_diagnosis_code as icd FROM read_parquet('{local_path}') \n",
    "                WHERE four_icd_diagnosis_code IS NOT NULL AND event_type = 'medical'\n",
    "                UNION ALL\n",
    "                SELECT mi_person_key, five_icd_diagnosis_code as icd FROM read_parquet('{local_path}') \n",
    "                WHERE five_icd_diagnosis_code IS NOT NULL AND event_type = 'medical'\n",
    "            )\n",
    "            SELECT mi_person_key, icd as item FROM all_icds WHERE icd != ''\n",
    "            \"\"\"\n",
    "        elif item_type == 'cpt_code':\n",
    "            query = f\"\"\"\n",
    "            SELECT mi_person_key, procedure_code as item\n",
    "            FROM read_parquet('{local_path}')\n",
    "            WHERE procedure_code IS NOT NULL AND procedure_code != '' AND event_type = 'medical'\n",
    "            \"\"\"\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown item_type: {item_type}\")\n",
    "        \n",
    "        df = con.execute(query).df()\n",
    "        log_memory(cohort_logger, \"After data extraction\")\n",
    "        \n",
    "        # Add target markers if TARGET_FOCUSED mode is enabled\n",
    "        if TARGET_FOCUSED:\n",
    "            cohort_logger.info(\"Adding target markers...\")\n",
    "            \n",
    "            # Get target information for each patient\n",
    "            target_query = f\"\"\"\n",
    "            SELECT DISTINCT \n",
    "                mi_person_key,\n",
    "                primary_icd_diagnosis_code,\n",
    "                hcg_line\n",
    "            FROM read_parquet('{local_path}')\n",
    "            WHERE mi_person_key IS NOT NULL\n",
    "            \"\"\"\n",
    "            df_targets = con.execute(target_query).df()\n",
    "            \n",
    "            # Create target items for each patient\n",
    "            target_items = []\n",
    "            for _, row in df_targets.iterrows():\n",
    "                patient_id = row['mi_person_key']\n",
    "                # Check for opioid ICD codes\n",
    "                if pd.notna(row['primary_icd_diagnosis_code']) and any(\n",
    "                    row['primary_icd_diagnosis_code'].startswith(code.replace('.', '')) \n",
    "                    for code in TARGET_ICD_CODES\n",
    "                ):\n",
    "                    target_items.append({'mi_person_key': patient_id, 'item': 'TARGET_ICD:OPIOID_DEPENDENCE'})\n",
    "                \n",
    "                # Check for ED visits (HCG Line - correct field!)\n",
    "                if pd.notna(row['hcg_line']) and row['hcg_line'] in TARGET_HCG_LINES:\n",
    "                    target_items.append({'mi_person_key': patient_id, 'item': 'TARGET_ED:EMERGENCY_DEPT'})\n",
    "            \n",
    "            if target_items:\n",
    "                df_targets_items = pd.DataFrame(target_items)\n",
    "                df = pd.concat([df, df_targets_items], ignore_index=True)\n",
    "                cohort_logger.info(f\"Added {len(target_items)} target markers\")\n",
    "                log_memory(cohort_logger, \"After target markers\")\n",
    "        \n",
    "        con.close()\n",
    "        \n",
    "        if df.empty:\n",
    "            cohort_logger.warning(f\"No {item_type} data for {cohort}/{age_band}/{event_year}\")\n",
    "            return (cohort, age_band, event_year, item_type, False, \"No data\")\n",
    "        \n",
    "        # Create transactions (group items by patient)\n",
    "        cohort_logger.info(f\"Building transactions from {len(df)} rows...\")\n",
    "        transactions = (\n",
    "            df.groupby('mi_person_key')['item']\n",
    "            .apply(lambda x: sorted(set(x.tolist())))\n",
    "            .tolist()\n",
    "        )\n",
    "        \n",
    "        if not transactions:\n",
    "            cohort_logger.warning(f\"No valid transactions for {cohort}/{age_band}/{event_year}\")\n",
    "            return (cohort, age_band, event_year, item_type, False, \"No transactions\")\n",
    "        \n",
    "        # Encode transactions\n",
    "        cohort_logger.info(f\"Encoding {len(transactions)} transactions...\")\n",
    "        te = TransactionEncoder()\n",
    "        te_ary = te.fit(transactions).transform(transactions)\n",
    "        df_encoded = pd.DataFrame(te_ary, columns=te.columns_)\n",
    "        log_memory(cohort_logger, \"After encoding\")\n",
    "        \n",
    "        # Run FP-Growth (use higher support for CPT to prevent rule explosion)\n",
    "        min_sup = MIN_SUPPORT_CPT if item_type == 'cpt_code' else MIN_SUPPORT\n",
    "        cohort_logger.info(f\"Running FP-Growth (min_support={min_sup})...\")\n",
    "        itemsets = fpgrowth(df_encoded, min_support=min_sup, use_colnames=True)\n",
    "        itemsets = itemsets.sort_values('support', ascending=False).reset_index(drop=True)\n",
    "        \n",
    "        if itemsets.empty:\n",
    "            cohort_logger.warning(f\"No itemsets found for {cohort}/{age_band}/{event_year}\")\n",
    "            return (cohort, age_band, event_year, item_type, False, \"No itemsets\")\n",
    "        \n",
    "        cohort_logger.info(f\"Found {len(itemsets)} itemsets\")\n",
    "        log_memory(cohort_logger, \"After FP-Growth\")\n",
    "        \n",
    "        # Generate rules (with appropriate thresholds and limits)\n",
    "        min_conf = MIN_CONFIDENCE_CPT if item_type == 'cpt_code' else MIN_CONFIDENCE\n",
    "        cohort_logger.info(f\"Generating rules (min_confidence={min_conf})...\")\n",
    "        \n",
    "        try:\n",
    "            all_rules = association_rules(itemsets, metric=\"confidence\", min_threshold=min_conf)\n",
    "            \n",
    "            if len(all_rules) > 0:\n",
    "                # Split rules: target-predicting vs control (non-target)\n",
    "                if TARGET_FOCUSED:\n",
    "                    # Target rules: consequent contains target marker\n",
    "                    target_mask = all_rules['consequents'].apply(\n",
    "                        lambda x: any(item.startswith(tuple(TARGET_PREFIXES)) for item in x)\n",
    "                    )\n",
    "                    rules_target = all_rules[target_mask].copy()\n",
    "                    rules_control = all_rules[~target_mask].copy()\n",
    "                    \n",
    "                    cohort_logger.info(f\"Split: {len(rules_target)} target rules, {len(rules_control)} control rules\")\n",
    "                    \n",
    "                    # Limit both sets to top N by lift\n",
    "                    if len(rules_target) > 0:\n",
    "                        rules_target = rules_target.sort_values('lift', ascending=False)\n",
    "                        if len(rules_target) > MAX_RULES_PER_COHORT:\n",
    "                            cohort_logger.info(f\"Keeping top {MAX_RULES_PER_COHORT} target rules (from {len(rules_target)})\")\n",
    "                            rules_target = rules_target.head(MAX_RULES_PER_COHORT)\n",
    "                        rules_target = rules_target.reset_index(drop=True)\n",
    "                    \n",
    "                    if len(rules_control) > 0:\n",
    "                        rules_control = rules_control.sort_values('lift', ascending=False)\n",
    "                        if len(rules_control) > MAX_RULES_PER_COHORT:\n",
    "                            cohort_logger.info(f\"Keeping top {MAX_RULES_PER_COHORT} control rules (from {len(rules_control)})\")\n",
    "                            rules_control = rules_control.head(MAX_RULES_PER_COHORT)\n",
    "                        rules_control = rules_control.reset_index(drop=True)\n",
    "                    \n",
    "                    # Keep target rules as main 'rules' for backward compatibility\n",
    "                    rules = rules_target\n",
    "                else:\n",
    "                    # Not target-focused: all rules are kept\n",
    "                    rules = all_rules.sort_values('lift', ascending=False).head(MAX_RULES_PER_COHORT).reset_index(drop=True)\n",
    "                    rules_control = pd.DataFrame()\n",
    "                \n",
    "                cohort_logger.info(f\"Final: {len(rules)} target rules, {len(rules_control)} control rules\")\n",
    "                log_memory(cohort_logger, \"After rule generation\")\n",
    "            else:\n",
    "                cohort_logger.info(f\"No rules met confidence threshold of {min_conf}\")\n",
    "                rules = pd.DataFrame()\n",
    "                rules_control = pd.DataFrame()\n",
    "                \n",
    "        except MemoryError as e:\n",
    "            cohort_logger.error(f\"MemoryError during rule generation - skipping rules\")\n",
    "            rules = pd.DataFrame()\n",
    "            rules_control = pd.DataFrame()\n",
    "        except Exception as e:\n",
    "            cohort_logger.error(f\"Error generating rules: {e}\")\n",
    "            rules = pd.DataFrame()\n",
    "            rules_control = pd.DataFrame()\n",
    "        \n",
    "        # Convert frozensets for JSON\n",
    "        itemsets_json = itemsets.copy()\n",
    "        itemsets_json['itemsets'] = itemsets_json['itemsets'].apply(list)\n",
    "        \n",
    "        # Prepare rules for saving (split target rules by type, plus control)\n",
    "        rules_by_target = {}\n",
    "        \n",
    "        # Process target rules (split by ICD vs ED)\n",
    "        if not rules.empty:\n",
    "            rules_json = rules.copy()\n",
    "            rules_json['antecedents'] = rules_json['antecedents'].apply(list)\n",
    "            rules_json['consequents'] = rules_json['consequents'].apply(list)\n",
    "            \n",
    "            # Split target rules by outcome type\n",
    "            rules_by_target['TARGET_ICD'] = rules_json[\n",
    "                rules_json['consequents'].apply(lambda x: any('TARGET_ICD:' in str(item) for item in x))\n",
    "            ]\n",
    "            rules_by_target['TARGET_ED'] = rules_json[\n",
    "                rules_json['consequents'].apply(lambda x: any('TARGET_ED:' in str(item) for item in x))\n",
    "            ]\n",
    "        \n",
    "        # Process control rules (non-target patterns)\n",
    "        if not rules_control.empty:\n",
    "            rules_control_json = rules_control.copy()\n",
    "            rules_control_json['antecedents'] = rules_control_json['antecedents'].apply(list)\n",
    "            rules_control_json['consequents'] = rules_control_json['consequents'].apply(list)\n",
    "            rules_by_target['CONTROL'] = rules_control_json\n",
    "        \n",
    "        if rules_by_target:\n",
    "            cohort_logger.info(f\"Prepared for S3: {len(rules_by_target.get('TARGET_ICD', pd.DataFrame()))} ICD, \"\n",
    "                             f\"{len(rules_by_target.get('TARGET_ED', pd.DataFrame()))} ED, \"\n",
    "                             f\"{len(rules_by_target.get('CONTROL', pd.DataFrame()))} control\")\n",
    "        \n",
    "        # Save to S3 (with retry logic for reliability)\n",
    "        s3_base = f\"{S3_OUTPUT_BASE}/{item_type}/cohort_name={cohort}/age_band={age_band}/event_year={event_year}\"\n",
    "        \n",
    "        cohort_logger.info(f\"Saving results to S3...\")\n",
    "        max_retries = 3\n",
    "        for attempt in range(max_retries):\n",
    "            try:\n",
    "                itemsets_path = f\"{s3_base}/itemsets.json\"\n",
    "                save_to_s3_json(itemsets_json.to_dict(orient='records'), itemsets_path)\n",
    "                \n",
    "                # Save rules by target type (separate files)\n",
    "                if rules_by_target:\n",
    "                    for target_type, target_rules in rules_by_target.items():\n",
    "                        if not target_rules.empty:\n",
    "                            rules_path = f\"{s3_base}/rules_{target_type}.json\"\n",
    "                            save_to_s3_json(target_rules.to_dict(orient='records'), rules_path)\n",
    "                            cohort_logger.info(f\"Saved {len(target_rules)} {target_type} rules\")\n",
    "                \n",
    "                summary = {\n",
    "                    'timestamp': datetime.now().isoformat(),\n",
    "                    'cohort': cohort, 'age_band': age_band, 'event_year': event_year,\n",
    "                    'item_type': item_type,\n",
    "                    'total_patients': len(transactions),\n",
    "                    'total_itemsets': len(itemsets),\n",
    "                    'total_rules': len(rules),\n",
    "                    'rules_by_target': {\n",
    "                        'TARGET_ICD': len(rules_by_target.get('TARGET_ICD', pd.DataFrame())),\n",
    "                        'TARGET_ED': len(rules_by_target.get('TARGET_ED', pd.DataFrame())),\n",
    "                        'CONTROL': len(rules_by_target.get('CONTROL', pd.DataFrame()))\n",
    "                    } if rules_by_target else {'TARGET_ICD': 0, 'TARGET_ED': 0, 'CONTROL': 0},\n",
    "                    'min_support': min_sup,\n",
    "                    'min_confidence': min_conf,\n",
    "                    'max_rules_limit': MAX_RULES_PER_COHORT,\n",
    "                    'rules_truncated': len(rules) == MAX_RULES_PER_COHORT,\n",
    "                    'target_focused': TARGET_FOCUSED,\n",
    "                    'target_icd_codes': TARGET_ICD_CODES if TARGET_FOCUSED else None,\n",
    "                    'target_hcg_lines': TARGET_HCG_LINES if TARGET_FOCUSED else None\n",
    "                }\n",
    "                summary_path = f\"{s3_base}/summary.json\"\n",
    "                save_to_s3_json(summary, summary_path)\n",
    "                \n",
    "                cohort_logger.info(f\"‚úì Saved to S3 successfully\")\n",
    "                break  # Success - exit retry loop\n",
    "                \n",
    "            except Exception as s3_error:\n",
    "                if attempt < max_retries - 1:\n",
    "                    cohort_logger.warning(f\"S3 upload attempt {attempt+1} failed: {s3_error}, retrying...\")\n",
    "                    time.sleep(2 ** attempt)  # Exponential backoff\n",
    "                else:\n",
    "                    cohort_logger.error(f\"S3 upload failed after {max_retries} attempts: {s3_error}\")\n",
    "                    raise\n",
    "        \n",
    "        elapsed = time.time() - start_time\n",
    "        log_memory(cohort_logger, \"END\")\n",
    "        cohort_logger.info(f\"‚úì Completed in {elapsed:.1f}s\")\n",
    "        \n",
    "        return (cohort, age_band, event_year, item_type, True, f\"{len(itemsets)} itemsets, {len(rules)} rules\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        cohort_logger.error(f\"Error: {e}\")\n",
    "        return (cohort, age_band, event_year, item_type, False, str(e))\n",
    "\n",
    "print(\"‚úì Cohort processing function defined\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Process Cohorts in Parallel\n",
    "\n",
    "Run FP-Growth for all cohort combinations using parallel processing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "COHORT FPGROWTH ANALYSIS - START\n",
      "================================================================================\n",
      "Cohorts: 45 combinations\n",
      "Item types: ['drug_name', 'icd_code', 'cpt_code']\n",
      "Total jobs: 135\n",
      "Max workers: 4\n",
      "Detailed progress ‚Üí Check log file\n",
      "\n",
      "Progress: 10/135 completed (7.4%), 0 failed\n",
      "Progress: 20/135 completed (14.8%), 0 failed\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"COHORT FPGROWTH ANALYSIS - START\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Cohorts: {len(cohort_jobs)} combinations\")\n",
    "print(f\"Item types: {ITEM_TYPES}\")\n",
    "print(f\"Total jobs: {len(cohort_jobs) * len(ITEM_TYPES)}\")\n",
    "print(f\"Max workers: {MAX_WORKERS}\")\n",
    "print(f\"Detailed progress ‚Üí Check log file\")\n",
    "print()\n",
    "\n",
    "logger.info(f\"\\n{'='*80}\")\n",
    "logger.info(f\"COHORT FPGROWTH ANALYSIS - START\")\n",
    "logger.info(f\"{'='*80}\")\n",
    "logger.info(f\"Cohorts: {len(cohort_jobs)} combinations\")\n",
    "logger.info(f\"Item types: {ITEM_TYPES}\")\n",
    "logger.info(f\"Total jobs: {len(cohort_jobs) * len(ITEM_TYPES)}\")\n",
    "\n",
    "# Helper function to check if cohort results exist in S3\n",
    "def check_cohort_exists(item_type: str, cohort: str, age_band: str, event_year: str) -> bool:\n",
    "    \"\"\"Check if cohort results already exist in S3 (by checking for summary.json).\"\"\"\n",
    "    s3 = boto3.client('s3')\n",
    "    key = f\"gold/fpgrowth/cohort/{item_type}/cohort_name={cohort}/age_band={age_band}/event_year={event_year}/summary.json\"\n",
    "    try:\n",
    "        s3.head_object(Bucket='pgxdatalake', Key=key)\n",
    "        return True\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "start_time = time.time()\n",
    "results = []\n",
    "completed = 0\n",
    "failed = 0\n",
    "skipped = 0\n",
    "\n",
    "# Create all combinations of cohorts and item types\n",
    "all_jobs_initial = [(job, item_type) for job in cohort_jobs for item_type in ITEM_TYPES]\n",
    "\n",
    "# Filter out already-completed jobs\n",
    "print(\"\\nChecking for existing results in S3...\")\n",
    "all_jobs = []\n",
    "for job, item_type in all_jobs_initial:\n",
    "    if check_cohort_exists(item_type, job['cohort'], job['age_band'], job['event_year']):\n",
    "        logger.info(f\"Skipping {job['cohort']}/{job['age_band']}/{job['event_year']}/{item_type} - already exists\")\n",
    "        skipped += 1\n",
    "        results.append({\n",
    "            'cohort': job['cohort'],\n",
    "            'age_band': job['age_band'],\n",
    "            'event_year': job['event_year'],\n",
    "            'item_type': item_type,\n",
    "            'success': True,\n",
    "            'message': 'Already exists in S3 (skipped)'\n",
    "        })\n",
    "    else:\n",
    "        all_jobs.append((job, item_type))\n",
    "\n",
    "total_jobs = len(all_jobs_initial)\n",
    "print(f\"Total jobs: {total_jobs}\")\n",
    "print(f\"Already completed: {skipped}\")\n",
    "print(f\"To process: {len(all_jobs)}\")\n",
    "print()\n",
    "\n",
    "with ProcessPoolExecutor(max_workers=MAX_WORKERS) as executor:\n",
    "    # Submit all jobs\n",
    "    future_to_params = {executor.submit(process_single_cohort, job, item_type): (job, item_type) \n",
    "                        for job, item_type in all_jobs}\n",
    "    \n",
    "    # Process results as they complete\n",
    "    for future in as_completed(future_to_params):\n",
    "        job, item_type = future_to_params[future]\n",
    "        try:\n",
    "            cohort, age_band, event_year, item_type, success, message = future.result()\n",
    "            results.append({\n",
    "                'cohort': cohort,\n",
    "                'age_band': age_band,\n",
    "                'event_year': event_year,\n",
    "                'item_type': item_type,\n",
    "                'success': success,\n",
    "                'message': message\n",
    "            })\n",
    "            \n",
    "            if success:\n",
    "                completed += 1\n",
    "                logger.info(f\"[{completed + failed}/{len(all_jobs)}] ‚úì {cohort}/{age_band}/{event_year}/{item_type}: {message}\")\n",
    "                # Print every 10 successes or milestones\n",
    "                if completed % 10 == 0 or (completed + failed) == len(all_jobs):\n",
    "                    print(f\"Progress: {completed}/{len(all_jobs)} completed ({completed/len(all_jobs)*100:.1f}%), {failed} failed, {skipped} skipped\")\n",
    "            else:\n",
    "                failed += 1\n",
    "                logger.warning(f\"[{completed + failed}/{total_jobs}] ‚úó {cohort}/{age_band}/{event_year}/{item_type}: {message}\")\n",
    "                print(f\"‚ö† Failed: {cohort}/{age_band}/{event_year}/{item_type}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            failed += 1\n",
    "            logger.error(f\"[{completed + failed}/{total_jobs}] ‚úó {job['cohort']}/{job['age_band']}/{job['event_year']}/{item_type}: {e}\")\n",
    "            print(f\"‚ö† Error: {job['cohort']}/{job['age_band']}/{job['event_year']}/{item_type}\")\n",
    "            results.append({\n",
    "                'cohort': job['cohort'],\n",
    "                'age_band': job['age_band'],\n",
    "                'event_year': job['event_year'],\n",
    "                'item_type': item_type,\n",
    "                'success': False,\n",
    "                'message': str(e)\n",
    "            })\n",
    "\n",
    "elapsed = time.time() - start_time\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"COHORT FPGROWTH ANALYSIS - COMPLETE\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"  Total jobs: {total_jobs}\")\n",
    "print(f\"  Already in S3 (skipped): {skipped}\")\n",
    "print(f\"  Newly processed: {completed}\")\n",
    "print(f\"  Failed: {failed}\")\n",
    "print(f\"  Overall success rate: {(skipped + completed)/total_jobs*100:.1f}%\")\n",
    "print(f\"  Total time: {elapsed:.1f}s ({elapsed/60:.1f}min)\")\n",
    "if len(all_jobs) > 0:\n",
    "    print(f\"  Avg time per new job: {elapsed/len(all_jobs):.1f}s\")\n",
    "else:\n",
    "    print(f\"  (No new jobs processed - all results already in S3)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Analyze Results\n",
    "\n",
    "Review processing results and identify any issues.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert results to DataFrame for analysis\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "print(\"\\nüìä Results by Cohort and Item Type:\")\n",
    "summary = results_df.groupby(['cohort', 'item_type'])['success'].agg([\n",
    "    ('total', 'count'), \n",
    "    ('successful', 'sum'),\n",
    "    ('success_rate', lambda x: f\"{x.mean()*100:.1f}%\")\n",
    "])\n",
    "print(summary)\n",
    "\n",
    "print(\"\\nüìä Results by Item Type:\")\n",
    "item_summary = results_df.groupby('item_type')['success'].agg([\n",
    "    ('total', 'count'), \n",
    "    ('successful', 'sum'),\n",
    "    ('success_rate', lambda x: f\"{x.mean()*100:.1f}%\")\n",
    "])\n",
    "print(item_summary)\n",
    "\n",
    "print(\"\\n‚ùå Failed Jobs:\")\n",
    "failed_df = results_df[~results_df['success']]\n",
    "if not failed_df.empty:\n",
    "    print(failed_df[['cohort', 'age_band', 'event_year', 'item_type', 'message']].to_string())\n",
    "else:\n",
    "    print(\"  None! All jobs completed successfully.\")\n",
    "\n",
    "print(\"\\n‚úì Successful Jobs Sample:\")\n",
    "success_df = results_df[results_df['success']]\n",
    "if not success_df.empty:\n",
    "    print(success_df[['cohort', 'age_band', 'event_year', 'item_type', 'message']].head(15))\n",
    "else:\n",
    "    print(\"  No successful jobs.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Next Steps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"COHORT FPGROWTH ANALYSIS - SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nüìä Processing Statistics:\")\n",
    "print(f\"  Cohort combinations: {len(cohort_jobs)}\")\n",
    "print(f\"  Item types: {len(ITEM_TYPES)} (drug_name, icd_code, cpt_code)\")\n",
    "print(f\"  Total jobs: {total_jobs}\")\n",
    "print(f\"  Successfully processed: {completed}\")\n",
    "print(f\"  Failed: {failed}\")\n",
    "print(f\"  Success rate: {completed/total_jobs*100:.1f}%\")\n",
    "print(f\"  Processing time: {elapsed:.1f}s ({elapsed/60:.1f}min)\")\n",
    "\n",
    "print(f\"\\nüîç FP-Growth Configuration:\")\n",
    "print(f\"  Min support: {MIN_SUPPORT} ({MIN_SUPPORT*100:.1f}%)\")\n",
    "print(f\"  Min confidence: {MIN_CONFIDENCE} ({MIN_CONFIDENCE*100:.1f}%)\")\n",
    "print(f\"  Parallel workers: {MAX_WORKERS}\")\n",
    "\n",
    "print(f\"\\nüíæ Output Location:\")\n",
    "print(f\"  S3 Base: {S3_OUTPUT_BASE}\")\n",
    "print(f\"  Structure: <item_type>/cohort_name=<name>/age_band=<band>/event_year=<year>/\")\n",
    "print(f\"  Item types:\")\n",
    "print(f\"    - drug_name/ (pharmacy events)\")\n",
    "print(f\"    - icd_code/ (diagnosis codes)\")\n",
    "print(f\"    - cpt_code/ (procedure codes)\")\n",
    "print(f\"  Files per cohort:\")\n",
    "print(f\"    - itemsets.json (frequent itemsets)\")\n",
    "print(f\"    - rules.json (association rules)\")\n",
    "print(f\"    - summary.json (metadata)\")\n",
    "\n",
    "print(f\"\\nüéØ Next Steps:\")\n",
    "print(f\"  1. Load cohort-specific itemsets for BupaR process mining\")\n",
    "print(f\"  2. Compare patterns between OPIOID_ED and ED_NON_OPIOID cohorts across item types\")\n",
    "print(f\"  3. Use association rules for pathway analysis\")\n",
    "print(f\"  4. Filter features for cohort-specific CatBoost models\")\n",
    "print(f\"  5. Create network visualizations for cohort-specific patterns\")\n",
    "\n",
    "print(f\"\\nüìù Example Usage:\")\n",
    "print(f\"  # Load drug patterns for a specific cohort\")\n",
    "print(f\"  from helpers_1997_13.s3_utils import load_from_s3_json\")\n",
    "print(f\"  itemsets = load_from_s3_json('{S3_OUTPUT_BASE}/drug_name/cohort_name=opioid_ed/age_band=65-74/event_year=2020/itemsets.json')\")\n",
    "print(f\"  # Load ICD patterns for same cohort\")\n",
    "print(f\"  icd_itemsets = load_from_s3_json('{S3_OUTPUT_BASE}/icd_code/cohort_name=opioid_ed/age_band=65-74/event_year=2020/itemsets.json')\")\n",
    "\n",
    "print(f\"\\n‚úì Analysis complete: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(\"=\"*80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Auto-Shutdown EC2 Instance (Optional)\n",
    "\n",
    "Set `SHUTDOWN_EC2 = True` to automatically stop the EC2 instance after analysis completes.\n",
    "\n",
    "**Note:** This is a **STOP** (not terminate), so you can restart the instance later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# EC2 AUTO-SHUTDOWN (OPTIONAL)\n",
    "# =============================================================================\n",
    "# Set SHUTDOWN_EC2 = True to enable, False to disable\n",
    "SHUTDOWN_EC2 = False  # Change to True to enable auto-shutdown\n",
    "\n",
    "if SHUTDOWN_EC2:\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"Shutting down EC2 instance...\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    import subprocess\n",
    "    import requests\n",
    "    import shutil\n",
    "    \n",
    "    # Get instance ID from EC2 metadata service\n",
    "    try:\n",
    "        response = requests.get(\n",
    "            \"http://169.254.169.254/latest/meta-data/instance-id\",\n",
    "            timeout=2\n",
    "        )\n",
    "        if response.status_code == 200:\n",
    "            instance_id = response.text.strip()\n",
    "            print(f\"Instance ID: {instance_id}\")\n",
    "            \n",
    "            # Find AWS CLI\n",
    "            aws_cmd = shutil.which(\"aws\")\n",
    "            if not aws_cmd:\n",
    "                # Try common paths\n",
    "                for path in [\"/usr/local/bin/aws\", \"/usr/bin/aws\", \n",
    "                           \"/home/ec2-user/.local/bin/aws\", \n",
    "                           \"/home/ubuntu/.local/bin/aws\",\n",
    "                           \"/home/pgx3874/.local/bin/aws\"]:\n",
    "                    if Path(path).exists():\n",
    "                        aws_cmd = path\n",
    "                        break\n",
    "            \n",
    "            if aws_cmd:\n",
    "                # Stop the instance (use terminate-instances for permanent deletion)\n",
    "                shutdown_cmd = [aws_cmd, \"ec2\", \"stop-instances\", \"--instance-ids\", instance_id]\n",
    "                \n",
    "                print(f\"Running: {' '.join(shutdown_cmd)}\")\n",
    "                result = subprocess.run(shutdown_cmd, capture_output=True, text=True)\n",
    "                \n",
    "                if result.returncode == 0:\n",
    "                    print(\"‚úì EC2 instance stop command sent successfully\")\n",
    "                    print(\"Instance will stop in a few moments.\")\n",
    "                    print(\"Note: This is a STOP (not terminate), so you can restart it later.\")\n",
    "                    if result.stdout:\n",
    "                        print(f\"\\nAWS Response:\\n{result.stdout}\")\n",
    "                else:\n",
    "                    print(f\"‚úó EC2 stop command failed with exit code {result.returncode}\")\n",
    "                    if result.stderr:\n",
    "                        print(f\"Error: {result.stderr}\")\n",
    "                    print(\"Check AWS credentials and IAM permissions.\")\n",
    "            else:\n",
    "                print(\"‚úó AWS CLI not found. Cannot shutdown instance.\")\n",
    "                print(\"Install AWS CLI or ensure it's in your PATH.\")\n",
    "                print(\"Manual shutdown: aws ec2 stop-instances --instance-ids \" + instance_id)\n",
    "        else:\n",
    "            print(f\"‚úó Metadata service returned status code {response.status_code}\")\n",
    "            print(\"Could not retrieve instance ID.\")\n",
    "    \n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(\"‚úó Could not retrieve instance ID from metadata service.\")\n",
    "        print(f\"Error: {e}\")\n",
    "        print(\"If running on EC2, check that metadata service is accessible.\")\n",
    "        print(\"\\nManual shutdown command:\")\n",
    "        print(\"  aws ec2 stop-instances --instance-ids <your-instance-id>\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"‚úó Unexpected error during shutdown: {e}\")\n",
    "\n",
    "else:\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"EC2 Auto-Shutdown: DISABLED\")\n",
    "    print(\"=\"*80)\n",
    "    print(\"To enable auto-shutdown, set SHUTDOWN_EC2 = True in this cell.\")\n",
    "    print(\"Instance will continue running.\")\n",
    "    print(\"\\nTo manually stop this instance later:\")\n",
    "    print(\"  aws ec2 stop-instances --instance-ids $(ec2-metadata --instance-id | cut -d ' ' -f 2)\")\n",
    "    print(\"Or use AWS Console: EC2 > Instances > Select instance > Instance State > Stop\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}