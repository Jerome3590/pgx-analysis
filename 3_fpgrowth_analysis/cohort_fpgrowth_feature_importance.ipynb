{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cohort-Specific FPGrowth Feature Importance Analysis\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook performs **cohort-specific FPGrowth analysis** to discover drug patterns unique to each cohort. The results are used for:\n",
    "\n",
    "1. **BupaR Process Mining**: Cohort-specific treatment pathways and sequences\n",
    "2. **Comparative Analysis**: Understand differences between OPIOID_ED and ED_NON_OPIOID cohorts\n",
    "3. **Feature Filtering**: Pre-filter features for cohort-specific CatBoost models\n",
    "\n",
    "## Key Differences from Global Analysis\n",
    "\n",
    "| Aspect | Global FPGrowth | Cohort FPGrowth |\n",
    "|--------|-----------------|-----------------|\n",
    "| **Scope** | All patients | Individual cohorts |\n",
    "| **Purpose** | Universal ML features | Process mining patterns |\n",
    "| **Support Threshold** | 0.005 (lower) | 0.05 (higher) |\n",
    "| **Output** | Single encoding map | Multiple cohort-specific results |\n",
    "| **Use Case** | CatBoost consistency | BupaR pathway analysis |\n",
    "\n",
    "## Key Outputs\n",
    "\n",
    "- **Cohort Drug Patterns**: Drug combinations specific to each cohort\n",
    "- **Association Rules**: Cohort-specific prescribing patterns\n",
    "- **Network Visualizations**: Visual representation of drug associations per cohort\n",
    "- **Feature Manifests**: Metadata about processing status\n",
    "\n",
    "## S3 Output Structure\n",
    "\n",
    "```\n",
    "s3://pgxdatalake/gold/fpgrowth/cohort/\n",
    "‚îú‚îÄ‚îÄ cohort_name=opioid_ed/\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ age_band=65-74/\n",
    "‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ event_year=2020/\n",
    "‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ itemsets.json\n",
    "‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ rules.json\n",
    "‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ drug_network.html\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ ...\n",
    "‚îî‚îÄ‚îÄ cohort_name=ed_non_opioid/\n",
    "    ‚îî‚îÄ‚îÄ (same structure)\n",
    "```\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import logging\n",
    "from pathlib import Path\n",
    "import time\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "\n",
    "# MLxtend for FP-Growth\n",
    "from mlxtend.frequent_patterns import fpgrowth, association_rules\n",
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "\n",
    "# Project root\n",
    "project_root = Path.cwd().parent if Path.cwd().name == '3_fpgrowth_analysis' else Path.cwd()\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "# Project utilities\n",
    "from helpers_1997_13.common_imports import s3_client, S3_BUCKET\n",
    "from helpers_1997_13.duckdb_utils import get_duckdb_connection\n",
    "from helpers_1997_13.s3_utils import save_to_s3_json, save_to_s3_parquet, get_cohort_parquet_path\n",
    "from helpers_1997_13.fpgrowth_utils import run_fpgrowth_drug_token_with_fallback, convert_frozensets\n",
    "from helpers_1997_13.visualization_utils import create_network_visualization\n",
    "from helpers_1997_13.constants import AGE_BANDS, EVENT_YEARS\n",
    "\n",
    "print(f\"‚úì Project root: {project_root}\")\n",
    "print(f\"‚úì All imports successful\")\n",
    "print(f\"‚úì Timestamp: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FP-Growth parameters (higher threshold for cohort-specific patterns)\n",
    "MIN_SUPPORT = 0.05  # 5% support threshold\n",
    "MIN_CONFIDENCE = 0.3  # 30% confidence threshold\n",
    "TOP_K = 30  # Top K itemsets to extract\n",
    "TIMEOUT_SECONDS = 300\n",
    "\n",
    "# Processing parameters\n",
    "MAX_WORKERS = 4  # Parallel workers for processing multiple cohorts\n",
    "COHORTS_TO_PROCESS = ['opioid_ed', 'ed_non_opioid']  # Can specify specific cohorts\n",
    "\n",
    "# S3 output path\n",
    "S3_OUTPUT_BASE = f\"s3://{S3_BUCKET}/gold/fpgrowth/cohort\"\n",
    "\n",
    "# Local data path\n",
    "LOCAL_DATA_PATH = project_root / \"data\" / \"gold\" / \"cohorts_F1120\"\n",
    "\n",
    "# Create logger\n",
    "logger = logging.getLogger('cohort_fpgrowth')\n",
    "logger.setLevel(logging.INFO)\n",
    "if not logger.handlers:\n",
    "    handler = logging.StreamHandler()\n",
    "    formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
    "    handler.setFormatter(formatter)\n",
    "    logger.addHandler(handler)\n",
    "\n",
    "print(f\"‚úì Min Support: {MIN_SUPPORT}\")\n",
    "print(f\"‚úì Min Confidence: {MIN_CONFIDENCE}\")\n",
    "print(f\"‚úì Max Workers: {MAX_WORKERS}\")\n",
    "print(f\"‚úì Cohorts: {COHORTS_TO_PROCESS}\")\n",
    "print(f\"‚úì S3 Output: {S3_OUTPUT_BASE}\")\n",
    "print(f\"‚úì Local Data: {LOCAL_DATA_PATH}\")\n",
    "print(f\"‚úì Local Data Exists: {LOCAL_DATA_PATH.exists()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Discover Available Cohorts\n",
    "\n",
    "Scan local data to find all available cohort combinations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discover_cohorts(local_data_path, cohort_filter=None):\n",
    "    \"\"\"\n",
    "    Discover all available cohort combinations from local data.\n",
    "    \"\"\"\n",
    "    cohort_jobs = []\n",
    "    \n",
    "    for cohort_dir in local_data_path.glob(\"cohort_name=*\"):\n",
    "        cohort_name = cohort_dir.name.replace(\"cohort_name=\", \"\")\n",
    "        \n",
    "        # Filter if specified\n",
    "        if cohort_filter and cohort_name not in cohort_filter:\n",
    "            continue\n",
    "        \n",
    "        for year_dir in cohort_dir.glob(\"event_year=*\"):\n",
    "            event_year = year_dir.name.replace(\"event_year=\", \"\")\n",
    "            \n",
    "            for age_dir in year_dir.glob(\"age_band=*\"):\n",
    "                age_band = age_dir.name.replace(\"age_band=\", \"\")\n",
    "                \n",
    "                # Check if cohort file exists\n",
    "                cohort_file = age_dir / \"cohort.parquet\"\n",
    "                if cohort_file.exists():\n",
    "                    cohort_jobs.append({\n",
    "                        'cohort': cohort_name,\n",
    "                        'age_band': age_band,\n",
    "                        'event_year': event_year,\n",
    "                        'local_path': str(cohort_file)\n",
    "                    })\n",
    "    \n",
    "    return cohort_jobs\n",
    "\n",
    "# Discover available cohorts\n",
    "cohort_jobs = discover_cohorts(LOCAL_DATA_PATH, cohort_filter=COHORTS_TO_PROCESS)\n",
    "\n",
    "print(f\"\\nüìä Discovered Cohorts:\")\n",
    "print(f\"  Total combinations: {len(cohort_jobs)}\")\n",
    "\n",
    "# Group by cohort\n",
    "cohort_counts = {}\n",
    "for job in cohort_jobs:\n",
    "    cohort_counts[job['cohort']] = cohort_counts.get(job['cohort'], 0) + 1\n",
    "\n",
    "for cohort, count in cohort_counts.items():\n",
    "    print(f\"  {cohort}: {count} combinations\")\n",
    "\n",
    "print(f\"\\n  Sample jobs:\")\n",
    "for job in cohort_jobs[:5]:\n",
    "    print(f\"    {job['cohort']}/{job['age_band']}/{job['event_year']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Define Cohort Processing Function\n",
    "\n",
    "Create a function to process a single cohort with FP-Growth.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_single_cohort(job):\n",
    "    \"\"\"\n",
    "    Process a single cohort with FP-Growth analysis.\n",
    "    \"\"\"\n",
    "    cohort = job['cohort']\n",
    "    age_band = job['age_band']\n",
    "    event_year = job['event_year']\n",
    "    local_path = job['local_path']\n",
    "    \n",
    "    # Create cohort-specific logger\n",
    "    cohort_logger = logging.getLogger(f\"cohort_{cohort}_{age_band}_{event_year}\")\n",
    "    cohort_logger.setLevel(logging.INFO)\n",
    "    \n",
    "    try:\n",
    "        start_time = time.time()\n",
    "        cohort_logger.info(f\"Processing {cohort}/{age_band}/{event_year}\")\n",
    "        \n",
    "        # Load cohort data\n",
    "        con = get_duckdb_connection(logger=cohort_logger)\n",
    "        df = con.execute(f\"\"\"\n",
    "            SELECT mi_person_key, drug_name\n",
    "            FROM read_parquet('{local_path}')\n",
    "            WHERE drug_name IS NOT NULL \n",
    "              AND drug_name != ''\n",
    "              AND event_type = 'PHARMACY'\n",
    "        \"\"\").df()\n",
    "        con.close()\n",
    "        \n",
    "        if df.empty:\n",
    "            cohort_logger.warning(f\"No pharmacy data for {cohort}/{age_band}/{event_year}\")\n",
    "            return (cohort, age_band, event_year, False, \"No data\")\n",
    "        \n",
    "        # Create transactions (patient-level drug lists)\n",
    "        cohort_logger.info(f\"Building transactions...\")\n",
    "        grouped = (\n",
    "            df.groupby(\"mi_person_key\")[\"drug_name\"]\n",
    "            .agg(lambda rows: sorted({\n",
    "                f\"drug_{str(d).strip().lower()}\"\n",
    "                for d in rows if pd.notnull(d) and str(d).strip()\n",
    "            }))\n",
    "            .reset_index()\n",
    "            .rename(columns={\"drug_name\": \"drug_tokens\"})\n",
    "        )\n",
    "        \n",
    "        transactions = [tokens for tokens in grouped[\"drug_tokens\"].tolist() if len(tokens) > 0]\n",
    "        \n",
    "        if not transactions:\n",
    "            cohort_logger.warning(f\"No valid transactions for {cohort}/{age_band}/{event_year}\")\n",
    "            return (cohort, age_band, event_year, False, \"No transactions\")\n",
    "        \n",
    "        cohort_logger.info(f\"Running FP-Growth with {len(transactions)} transactions...\")\n",
    "        \n",
    "        # Run FP-Growth with fallback\n",
    "        itemsets, rules = run_fpgrowth_drug_token_with_fallback(\n",
    "            transactions=transactions,\n",
    "            min_support_threshold=MIN_SUPPORT,\n",
    "            timeout_seconds=TIMEOUT_SECONDS,\n",
    "            top_k=TOP_K,\n",
    "            logger=cohort_logger\n",
    "        )\n",
    "        \n",
    "        if itemsets is None or itemsets.empty:\n",
    "            cohort_logger.warning(f\"FP-Growth returned no itemsets for {cohort}/{age_band}/{event_year}\")\n",
    "            return (cohort, age_band, event_year, False, \"No itemsets\")\n",
    "        \n",
    "        # Convert frozensets for JSON serialization\n",
    "        itemsets_json = itemsets.copy()\n",
    "        itemsets_json['itemsets'] = itemsets_json['itemsets'].apply(lambda x: list(x))\n",
    "        \n",
    "        rules_json = pd.DataFrame()\n",
    "        if rules is not None and not rules.empty:\n",
    "            rules_json = rules.copy()\n",
    "            rules_json['antecedents'] = rules_json['antecedents'].apply(lambda x: list(x))\n",
    "            rules_json['consequents'] = rules_json['consequents'].apply(lambda x: list(x))\n",
    "        \n",
    "        # Save to S3\n",
    "        s3_base = f\"{S3_OUTPUT_BASE}/cohort_name={cohort}/age_band={age_band}/event_year={event_year}\"\n",
    "        \n",
    "        # Save itemsets\n",
    "        itemsets_path = f\"{s3_base}/itemsets.json\"\n",
    "        save_to_s3_json(itemsets_json.to_dict(orient='records'), itemsets_path)\n",
    "        \n",
    "        # Save rules\n",
    "        if not rules_json.empty:\n",
    "            rules_path = f\"{s3_base}/rules.json\"\n",
    "            save_to_s3_json(rules_json.to_dict(orient='records'), rules_path)\n",
    "        \n",
    "        # Save summary\n",
    "        summary = {\n",
    "            'timestamp': datetime.now().isoformat(),\n",
    "            'cohort': cohort,\n",
    "            'age_band': age_band,\n",
    "            'event_year': event_year,\n",
    "            'total_patients': len(transactions),\n",
    "            'total_itemsets': len(itemsets),\n",
    "            'total_rules': len(rules) if rules is not None else 0,\n",
    "            'min_support': MIN_SUPPORT,\n",
    "            'min_confidence': MIN_CONFIDENCE\n",
    "        }\n",
    "        summary_path = f\"{s3_base}/summary.json\"\n",
    "        save_to_s3_json(summary, summary_path)\n",
    "        \n",
    "        elapsed = time.time() - start_time\n",
    "        cohort_logger.info(f\"‚úì Completed {cohort}/{age_band}/{event_year} in {elapsed:.1f}s\")\n",
    "        \n",
    "        return (cohort, age_band, event_year, True, f\"{len(itemsets)} itemsets, {len(rules) if rules is not None else 0} rules\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        cohort_logger.error(f\"Error processing {cohort}/{age_band}/{event_year}: {e}\")\n",
    "        return (cohort, age_band, event_year, False, str(e))\n",
    "\n",
    "print(\"‚úì Cohort processing function defined\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Process Cohorts in Parallel\n",
    "\n",
    "Run FP-Growth for all cohort combinations using parallel processing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(f\"Processing {len(cohort_jobs)} cohorts with {MAX_WORKERS} workers...\")\n",
    "start_time = time.time()\n",
    "\n",
    "results = []\n",
    "completed = 0\n",
    "failed = 0\n",
    "\n",
    "with ProcessPoolExecutor(max_workers=MAX_WORKERS) as executor:\n",
    "    # Submit all jobs\n",
    "    future_to_job = {executor.submit(process_single_cohort, job): job for job in cohort_jobs}\n",
    "    \n",
    "    # Process results as they complete\n",
    "    for future in as_completed(future_to_job):\n",
    "        job = future_to_job[future]\n",
    "        try:\n",
    "            cohort, age_band, event_year, success, message = future.result()\n",
    "            results.append({\n",
    "                'cohort': cohort,\n",
    "                'age_band': age_band,\n",
    "                'event_year': event_year,\n",
    "                'success': success,\n",
    "                'message': message\n",
    "            })\n",
    "            \n",
    "            if success:\n",
    "                completed += 1\n",
    "                logger.info(f\"[{completed + failed}/{len(cohort_jobs)}] ‚úì {cohort}/{age_band}/{event_year}: {message}\")\n",
    "            else:\n",
    "                failed += 1\n",
    "                logger.warning(f\"[{completed + failed}/{len(cohort_jobs)}] ‚úó {cohort}/{age_band}/{event_year}: {message}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            failed += 1\n",
    "            logger.error(f\"[{completed + failed}/{len(cohort_jobs)}] ‚úó {job['cohort']}/{job['age_band']}/{job['event_year']}: {e}\")\n",
    "            results.append({\n",
    "                'cohort': job['cohort'],\n",
    "                'age_band': job['age_band'],\n",
    "                'event_year': job['event_year'],\n",
    "                'success': False,\n",
    "                'message': str(e)\n",
    "            })\n",
    "\n",
    "elapsed = time.time() - start_time\n",
    "\n",
    "print(f\"\\nüìä Processing Complete:\")\n",
    "print(f\"  Total jobs: {len(cohort_jobs)}\")\n",
    "print(f\"  Successful: {completed}\")\n",
    "print(f\"  Failed: {failed}\")\n",
    "print(f\"  Success rate: {completed/len(cohort_jobs)*100:.1f}%\")\n",
    "print(f\"  Total time: {elapsed:.1f}s\")\n",
    "print(f\"  Avg time per cohort: {elapsed/len(cohort_jobs):.1f}s\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Analyze Results\n",
    "\n",
    "Review processing results and identify any issues.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert results to DataFrame for analysis\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "print(\"\\nüìä Results by Cohort:\")\n",
    "print(results_df.groupby('cohort')['success'].agg(['count', 'sum', lambda x: f\"{x.mean()*100:.1f}%\"]).rename(columns={'sum': 'successful', '<lambda_0>': 'success_rate'}))\n",
    "\n",
    "print(\"\\n‚ùå Failed Jobs:\")\n",
    "failed_df = results_df[~results_df['success']]\n",
    "if not failed_df.empty:\n",
    "    print(failed_df[['cohort', 'age_band', 'event_year', 'message']])\n",
    "else:\n",
    "    print(\"  None! All jobs completed successfully.\")\n",
    "\n",
    "print(\"\\n‚úì Successful Jobs Sample:\")\n",
    "success_df = results_df[results_df['success']]\n",
    "if not success_df.empty:\n",
    "    print(success_df[['cohort', 'age_band', 'event_year', 'message']].head(10))\n",
    "else:\n",
    "    print(\"  No successful jobs.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Next Steps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"COHORT FPGROWTH ANALYSIS - SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nüìä Processing Statistics:\")\n",
    "print(f\"  Total cohort combinations: {len(cohort_jobs)}\")\n",
    "print(f\"  Successfully processed: {completed}\")\n",
    "print(f\"  Failed: {failed}\")\n",
    "print(f\"  Success rate: {completed/len(cohort_jobs)*100:.1f}%\")\n",
    "print(f\"  Processing time: {elapsed:.1f}s ({elapsed/60:.1f}min)\")\n",
    "\n",
    "print(f\"\\nüîç FP-Growth Configuration:\")\n",
    "print(f\"  Min support: {MIN_SUPPORT} ({MIN_SUPPORT*100:.1f}%)\")\n",
    "print(f\"  Min confidence: {MIN_CONFIDENCE} ({MIN_CONFIDENCE*100:.1f}%)\")\n",
    "print(f\"  Top K itemsets: {TOP_K}\")\n",
    "print(f\"  Parallel workers: {MAX_WORKERS}\")\n",
    "\n",
    "print(f\"\\nüíæ Output Location:\")\n",
    "print(f\"  S3 Base: {S3_OUTPUT_BASE}\")\n",
    "print(f\"  Structure: cohort_name=<name>/age_band=<band>/event_year=<year>/\")\n",
    "print(f\"  Files per cohort:\")\n",
    "print(f\"    - itemsets.json (frequent drug combinations)\")\n",
    "print(f\"    - rules.json (association rules)\")\n",
    "print(f\"    - summary.json (metadata)\")\n",
    "\n",
    "print(f\"\\nüéØ Next Steps:\")\n",
    "print(f\"  1. Load cohort-specific itemsets for BupaR process mining\")\n",
    "print(f\"  2. Compare patterns between OPIOID_ED and ED_NON_OPIOID cohorts\")\n",
    "print(f\"  3. Use association rules for pathway analysis\")\n",
    "print(f\"  4. Filter features for cohort-specific CatBoost models\")\n",
    "print(f\"  5. Create network visualizations for cohort-specific patterns\")\n",
    "\n",
    "print(f\"\\nüìù Example Usage:\")\n",
    "print(f\"  # Load results for a specific cohort\")\n",
    "print(f\"  from helpers_1997_13.s3_utils import load_from_s3_json\")\n",
    "print(f\"  itemsets = load_from_s3_json('{S3_OUTPUT_BASE}/cohort_name=opioid_ed/age_band=65-74/event_year=2020/itemsets.json')\")\n",
    "\n",
    "print(f\"\\n‚úì Analysis complete: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(\"=\"*80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
