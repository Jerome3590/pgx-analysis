{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Global FPGrowth Feature Importance Analysis\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook performs **global FPGrowth analysis** across all cohorts to create universal encoding features for machine learning models. The analysis covers three item types:\n",
    "\n",
    "1. **Drug Names**: Pharmacy events (drug co-prescriptions)\n",
    "2. **ICD Codes**: Diagnosis codes (condition associations)\n",
    "3. **CPT Codes**: Procedure codes (treatment patterns)\n",
    "\n",
    "## Use Cases\n",
    "\n",
    "- **CatBoost Feature Engineering**: Creates consistent encodings across training/validation/test sets\n",
    "- **Population-Level Insights**: Discovers association patterns across all patients\n",
    "- **Feature Importance**: Identifies most frequent patterns in the population\n",
    "\n",
    "## Key Outputs (per item type)\n",
    "\n",
    "Each item type gets its own folder with:\n",
    "- **Global Encoding Map**: Universal encodings for ML\n",
    "- **Frequent Itemsets**: Combinations that appear frequently\n",
    "- **Association Rules**: Co-occurrence patterns\n",
    "- **Summary Metrics**: Processing statistics\n",
    "\n",
    "## S3 Output Structure\n",
    "\n",
    "```\n",
    "gold/fpgrowth/global/\n",
    "\u251c\u2500\u2500 drug_name/\n",
    "\u2502   \u251c\u2500\u2500 encoding_map.json\n",
    "\u2502   \u251c\u2500\u2500 itemsets.json\n",
    "\u2502   \u251c\u2500\u2500 rules.json\n",
    "\u2502   \u2514\u2500\u2500 metrics.json\n",
    "\u251c\u2500\u2500 icd_code/\n",
    "\u2502   \u2514\u2500\u2500 (same files)\n",
    "\u2514\u2500\u2500 cpt_code/\n",
    "    \u2514\u2500\u2500 (same files)\n",
    "```\n",
    "\n",
    "## Parameters\n",
    "\n",
    "- **Min Support**: 0.005 (items must appear in 0.5% of transactions)\n",
    "- **Min Confidence**: 0.01 (rules must have 1% confidence)\n",
    "- **Data Source**: Local cohort data from `data/gold/cohorts_F1120/`\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import logging\n",
    "from pathlib import Path\n",
    "import time\n",
    "\n",
    "# MLxtend for FP-Growth\n",
    "from mlxtend.frequent_patterns import fpgrowth, association_rules\n",
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "\n",
    "# Project root\n",
    "project_root = Path.cwd().parent if Path.cwd().name == '3_fpgrowth_analysis' else Path.cwd()\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "# Project utilities\n",
    "from helpers_1997_13.common_imports import s3_client, S3_BUCKET\n",
    "from helpers_1997_13.duckdb_utils import get_duckdb_connection\n",
    "from helpers_1997_13.s3_utils import save_to_s3_json, save_to_s3_parquet\n",
    "from helpers_1997_13.drug_utils import encode_drug_name\n",
    "from helpers_1997_13.visualization_utils import create_network_visualization\n",
    "\n",
    "print(f\"\u2713 Project root: {project_root}\")\n",
    "print(f\"\u2713 All imports successful\")\n",
    "print(f\"\u2713 Timestamp: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FP-Growth parameters\n",
    "MIN_SUPPORT = 0.01  # 0.5% support threshold\n",
    "MIN_CONFIDENCE = 0.01  # 1% confidence threshold\n",
    "TOP_K = 50  # Top K itemsets to analyze\n",
    "\n",
    "# Item types to process\n",
    "ITEM_TYPES = ['drug_name', 'icd_code', 'cpt_code']\n",
    "\n",
    "# S3 output base path\n",
    "S3_OUTPUT_BASE = f\"s3://{S3_BUCKET}/gold/fpgrowth/global\"\n",
    "\n",
    "# Local data path\n",
    "LOCAL_DATA_PATH = project_root / \"data\" / \"gold\" / \"cohorts_F1120\"\n",
    "\n",
    "# Create logger\n",
    "logger = logging.getLogger('global_fpgrowth')\n",
    "logger.setLevel(logging.INFO)\n",
    "if not logger.handlers:\n",
    "    handler = logging.StreamHandler()\n",
    "    formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
    "    handler.setFormatter(formatter)\n",
    "    logger.addHandler(handler)\n",
    "\n",
    "print(f\"\u2713 Min Support: {MIN_SUPPORT}\")\n",
    "print(f\"\u2713 Min Confidence: {MIN_CONFIDENCE}\")\n",
    "print(f\"\u2713 Item Types: {ITEM_TYPES}\")\n",
    "print(f\"\u2713 S3 Output: {S3_OUTPUT_BASE}\")\n",
    "print(f\"\u2713 Local Data: {LOCAL_DATA_PATH}\")\n",
    "print(f\"\u2713 Local Data Exists: {LOCAL_DATA_PATH.exists()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Define Item Extraction Functions\n",
    "\n",
    "Create functions to extract different item types from cohort data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_global_items(local_data_path, item_type, logger):\n",
    "    \"\"\"\n",
    "    Extract all unique items of specified type from local cohort data.\n",
    "    \n",
    "    Args:\n",
    "        item_type: 'drug_name', 'icd_code', or 'cpt_code'\n",
    "    \"\"\"\n",
    "    logger.info(f\"Extracting global {item_type}s from local cohort data...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Get DuckDB connection\n",
    "    con = get_duckdb_connection(logger=logger)\n",
    "    \n",
    "    # Build glob pattern for all parquet files\n",
    "    parquet_pattern = str(local_data_path / \"**\" / \"cohort.parquet\")\n",
    "    \n",
    "    # Build query based on item type\n",
    "    if item_type == 'drug_name':\n",
    "        query = f\"\"\"\n",
    "        SELECT DISTINCT drug_name as item\n",
    "        FROM read_parquet('{parquet_pattern}', hive_partitioning=1)\n",
    "        WHERE drug_name IS NOT NULL \n",
    "          AND drug_name != ''\n",
    "          AND event_type = 'pharmacy'\n",
    "        ORDER BY item\n",
    "        \"\"\"\n",
    "    elif item_type == 'icd_code':\n",
    "        # Collect from all ICD diagnosis columns\n",
    "        query = f\"\"\"\n",
    "        WITH all_icds AS (\n",
    "            SELECT primary_icd_diagnosis_code as icd FROM read_parquet('{parquet_pattern}', hive_partitioning=1) WHERE primary_icd_diagnosis_code IS NOT NULL AND event_type = 'medical'\n",
    "            UNION ALL\n",
    "            SELECT two_icd_diagnosis_code as icd FROM read_parquet('{parquet_pattern}', hive_partitioning=1) WHERE two_icd_diagnosis_code IS NOT NULL AND event_type = 'medical'\n",
    "            UNION ALL\n",
    "            SELECT three_icd_diagnosis_code as icd FROM read_parquet('{parquet_pattern}', hive_partitioning=1) WHERE three_icd_diagnosis_code IS NOT NULL AND event_type = 'medical'\n",
    "            UNION ALL\n",
    "            SELECT four_icd_diagnosis_code as icd FROM read_parquet('{parquet_pattern}', hive_partitioning=1) WHERE four_icd_diagnosis_code IS NOT NULL AND event_type = 'medical'\n",
    "            UNION ALL\n",
    "            SELECT five_icd_diagnosis_code as icd FROM read_parquet('{parquet_pattern}', hive_partitioning=1) WHERE five_icd_diagnosis_code IS NOT NULL AND event_type = 'medical'\n",
    "        )\n",
    "        SELECT DISTINCT icd as item FROM all_icds WHERE icd != '' ORDER BY item\n",
    "        \"\"\"\n",
    "    elif item_type == 'cpt_code':\n",
    "        query = f\"\"\"\n",
    "        SELECT DISTINCT procedure_code as item\n",
    "        FROM read_parquet('{parquet_pattern}', hive_partitioning=1)\n",
    "        WHERE procedure_code IS NOT NULL \n",
    "          AND procedure_code != ''\n",
    "          AND event_type = 'medical'\n",
    "        ORDER BY item\n",
    "        \"\"\"\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown item_type: {item_type}\")\n",
    "    \n",
    "    logger.info(f\"Running query for {item_type}...\")\n",
    "    df = con.execute(query).df()\n",
    "    con.close()\n",
    "    \n",
    "    items = df['item'].tolist()\n",
    "    \n",
    "    elapsed = time.time() - start_time\n",
    "    logger.info(f\"\u2713 Extracted {len(items):,} unique {item_type}s in {elapsed:.1f}s\")\n",
    "    \n",
    "    return items\n",
    "\n",
    "# Test extraction function\n",
    "print(\"Testing item extraction...\")\n",
    "test_items = extract_global_items(LOCAL_DATA_PATH, 'drug_name', logger)\n",
    "print(f\"\u2713 Found {len(test_items):,} drugs\")\n",
    "print(f\"  Sample: {test_items[:5]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Define Transaction Creation Functions\n",
    "\n",
    "Create patient-level transactions for each item type.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_global_transactions(local_data_path, item_type, logger):\n",
    "    \"\"\"\n",
    "    Create patient-level transactions from local cohort data.\n",
    "    \n",
    "    Args:\n",
    "        item_type: 'drug_name', 'icd_code', or 'cpt_code'\n",
    "    \"\"\"\n",
    "    logger.info(f\"Creating global {item_type} transactions...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Get DuckDB connection\n",
    "    con = get_duckdb_connection(logger=logger)\n",
    "    \n",
    "    # Build glob pattern for all parquet files\n",
    "    parquet_pattern = str(local_data_path / \"**\" / \"cohort.parquet\")\n",
    "    \n",
    "    # Build query based on item type\n",
    "    if item_type == 'drug_name':\n",
    "        query = f\"\"\"\n",
    "        SELECT mi_person_key, drug_name as item\n",
    "        FROM read_parquet('{parquet_pattern}', hive_partitioning=1)\n",
    "        WHERE drug_name IS NOT NULL AND drug_name != '' AND event_type = 'pharmacy'\n",
    "        \"\"\"\n",
    "    elif item_type == 'icd_code':\n",
    "        query = f\"\"\"\n",
    "        WITH all_icds AS (\n",
    "            SELECT mi_person_key, primary_icd_diagnosis_code as icd FROM read_parquet('{parquet_pattern}', hive_partitioning=1) WHERE primary_icd_diagnosis_code IS NOT NULL AND event_type = 'medical'\n",
    "            UNION ALL\n",
    "            SELECT mi_person_key, two_icd_diagnosis_code as icd FROM read_parquet('{parquet_pattern}', hive_partitioning=1) WHERE two_icd_diagnosis_code IS NOT NULL AND event_type = 'medical'\n",
    "            UNION ALL\n",
    "            SELECT mi_person_key, three_icd_diagnosis_code as icd FROM read_parquet('{parquet_pattern}', hive_partitioning=1) WHERE three_icd_diagnosis_code IS NOT NULL AND event_type = 'medical'\n",
    "        )\n",
    "        SELECT mi_person_key, icd as item FROM all_icds WHERE icd != ''\n",
    "        \"\"\"\n",
    "    elif item_type == 'cpt_code':\n",
    "        query = f\"\"\"\n",
    "        SELECT mi_person_key, procedure_code as item\n",
    "        FROM read_parquet('{parquet_pattern}', hive_partitioning=1)\n",
    "        WHERE procedure_code IS NOT NULL AND procedure_code != '' AND event_type = 'medical'\n",
    "        \"\"\"\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown item_type: {item_type}\")\n",
    "    \n",
    "    logger.info(f\"Loading {item_type} events...\")\n",
    "    df = con.execute(query).df()\n",
    "    con.close()\n",
    "    \n",
    "    # Group by patient and create item lists\n",
    "    logger.info(f\"Grouping by patient...\")\n",
    "    transactions = (\n",
    "        df.groupby('mi_person_key')['item']\n",
    "        .apply(lambda x: sorted(set(x.tolist())))\n",
    "        .tolist()\n",
    "    )\n",
    "    \n",
    "    elapsed = time.time() - start_time\n",
    "    logger.info(f\"\u2713 Created {len(transactions):,} patient transactions in {elapsed:.1f}s\")\n",
    "    \n",
    "    return transactions\n",
    "\n",
    "print(\"\u2713 Transaction creation function defined\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Process All Item Types\n",
    "\n",
    "Run FP-Growth analysis for each item type (drug_name, icd_code, cpt_code).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_item_type(item_type, local_data_path, s3_output_base, min_support, min_confidence, logger):\n",
    "    \"\"\"\n",
    "    Process a single item type end-to-end: extract, create transactions, run FP-Growth, save results.\n",
    "    \"\"\"\n",
    "    logger.info(f\"\\n{'='*80}\")\n",
    "    logger.info(f\"Processing {item_type.upper()}\")\n",
    "    logger.info(f\"{'='*80}\")\n",
    "    \n",
    "    overall_start = time.time()\n",
    "    \n",
    "    # Step 1: Extract items\n",
    "    items = extract_global_items(local_data_path, item_type, logger)\n",
    "    \n",
    "    # Step 2: Create transactions\n",
    "    transactions = create_global_transactions(local_data_path, item_type, logger)\n",
    "    \n",
    "    # Step 3: Encode transactions\n",
    "    logger.info(f\"Encoding {len(transactions):,} transactions...\")\n",
    "    te = TransactionEncoder()\n",
    "    te_ary = te.fit(transactions).transform(transactions)\n",
    "    df_encoded = pd.DataFrame(te_ary, columns=te.columns_)\n",
    "    logger.info(f\"\u2713 Encoded to {df_encoded.shape} matrix\")\n",
    "    \n",
    "    # Step 4: Run FP-Growth\n",
    "    logger.info(f\"Running FP-Growth...\")\n",
    "    itemsets = fpgrowth(df_encoded, min_support=min_support, use_colnames=True)\n",
    "    itemsets = itemsets.sort_values('support', ascending=False).reset_index(drop=True)\n",
    "    logger.info(f\"\u2713 Found {len(itemsets):,} frequent itemsets\")\n",
    "    \n",
    "    # Step 5: Generate association rules\n",
    "    logger.info(f\"Generating association rules...\")\n",
    "    try:\n",
    "        rules = association_rules(itemsets, metric=\"confidence\", min_threshold=min_confidence)\n",
    "        rules = rules.sort_values('lift', ascending=False).reset_index(drop=True)\n",
    "        logger.info(f\"\u2713 Generated {len(rules):,} association rules\")\n",
    "    except ValueError as e:\n",
    "        logger.warning(f\"Could not generate rules: {e}\")\n",
    "        rules = pd.DataFrame()\n",
    "    \n",
    "    # Step 6: Create encoding map\n",
    "    logger.info(f\"Creating encoding map...\")\n",
    "    encoding_map = {}\n",
    "    for item in items:\n",
    "        support = 0.0\n",
    "        matching = itemsets[itemsets['itemsets'].apply(lambda x: item in x)]\n",
    "        if not matching.empty:\n",
    "            support = matching['support'].max()\n",
    "        \n",
    "        confidence = 0.0\n",
    "        if not rules.empty:\n",
    "            matching_rules = rules[\n",
    "                rules['antecedents'].apply(lambda x: item in x) |\n",
    "                rules['consequents'].apply(lambda x: item in x)\n",
    "            ]\n",
    "            if not matching_rules.empty:\n",
    "                confidence = matching_rules['confidence'].max()\n",
    "        \n",
    "        # Simple encoding: item_supportXXX_confidenceYYY\n",
    "        encoding = f\"{item}_{int(support*1000):04d}_{int(confidence*1000):04d}\"\n",
    "        encoding_map[item] = encoding\n",
    "    \n",
    "    logger.info(f\"\u2713 Created encoding map with {len(encoding_map):,} items\")\n",
    "    \n",
    "    # Step 7: Save to S3\n",
    "    s3_folder = f\"{s3_output_base}/{item_type}\"\n",
    "    logger.info(f\"Saving to {s3_folder}...\")\n",
    "    \n",
    "    # Convert frozensets to lists\n",
    "    itemsets_json = itemsets.copy()\n",
    "    itemsets_json['itemsets'] = itemsets_json['itemsets'].apply(lambda x: list(x))\n",
    "    \n",
    "    # Save encoding map\n",
    "    encoding_path = f\"{s3_folder}/encoding_map.json\"\n",
    "    save_to_s3_json(encoding_map, encoding_path)\n",
    "    \n",
    "    # Save itemsets\n",
    "    itemsets_path = f\"{s3_folder}/itemsets.json\"\n",
    "    save_to_s3_json(itemsets_json.to_dict(orient='records'), itemsets_path)\n",
    "    \n",
    "    # Save rules\n",
    "    if not rules.empty:\n",
    "        rules_json = rules.copy()\n",
    "        rules_json['antecedents'] = rules_json['antecedents'].apply(lambda x: list(x))\n",
    "        rules_json['consequents'] = rules_json['consequents'].apply(lambda x: list(x))\n",
    "        rules_path = f\"{s3_folder}/rules.json\"\n",
    "        save_to_s3_json(rules_json.to_dict(orient='records'), rules_path)\n",
    "    \n",
    "    # Save metrics\n",
    "    summary = {\n",
    "        'timestamp': datetime.now().isoformat(),\n",
    "        'item_type': item_type,\n",
    "        'total_items': len(items),\n",
    "        'total_patients': len(transactions),\n",
    "        'total_itemsets': len(itemsets),\n",
    "        'total_rules': len(rules),\n",
    "        'min_support': min_support,\n",
    "        'min_confidence': min_confidence,\n",
    "        'avg_items_per_patient': float(np.mean([len(t) for t in transactions]))\n",
    "    }\n",
    "    metrics_path = f\"{s3_folder}/metrics.json\"\n",
    "    save_to_s3_json(summary, metrics_path)\n",
    "    \n",
    "    elapsed = time.time() - overall_start\n",
    "    logger.info(f\"\u2713 {item_type} complete in {elapsed:.1f}s ({elapsed/60:.1f}min)\")\n",
    "    \n",
    "    return {\n",
    "        'item_type': item_type,\n",
    "        'total_items': len(items),\n",
    "        'total_patients': len(transactions),\n",
    "        'total_itemsets': len(itemsets),\n",
    "        'total_rules': len(rules),\n",
    "        'elapsed_seconds': elapsed,\n",
    "        's3_folder': s3_folder\n",
    "    }\n",
    "\n",
    "# Process all item types\n",
    "print(\"\\n\ud83d\ude80 Starting FP-Growth analysis for all item types...\\n\")\n",
    "results = []\n",
    "\n",
    "for item_type in ITEM_TYPES:\n",
    "    try:\n",
    "        result = process_item_type(item_type, LOCAL_DATA_PATH, S3_OUTPUT_BASE, MIN_SUPPORT, MIN_CONFIDENCE, logger)\n",
    "        results.append(result)\n",
    "        print(f\"\\n\u2705 {item_type}: {result['total_itemsets']:,} itemsets, {result['total_rules']:,} rules\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"\u274c Failed to process {item_type}: {e}\")\n",
    "        results.append({'item_type': item_type, 'error': str(e)})\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ALL ITEM TYPES PROCESSED\")\n",
    "print(\"=\"*80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## View Results\n",
    "\n",
    "Check the processing output above for results from each item type."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}