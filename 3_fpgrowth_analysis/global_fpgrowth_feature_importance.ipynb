{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Global FPGrowth Feature Importance Analysis\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook performs **global FPGrowth analysis** across all cohorts to create universal encoding features for machine learning models. The analysis covers three item types:\n",
    "\n",
    "1. **Drug Names**: Pharmacy events (drug co-prescriptions)\n",
    "2. **ICD Codes**: Diagnosis codes (condition associations)\n",
    "3. **CPT Codes**: Procedure codes (treatment patterns)\n",
    "\n",
    "## Use Cases\n",
    "\n",
    "- **CatBoost Feature Engineering**: Creates consistent encodings across training/validation/test sets\n",
    "- **Population-Level Insights**: Discovers association patterns across all patients\n",
    "- **Feature Importance**: Identifies most frequent patterns in the population\n",
    "\n",
    "## Key Outputs (per item type)\n",
    "\n",
    "Each item type gets its own folder with:\n",
    "- **Global Encoding Map**: Universal encodings for ML\n",
    "- **Frequent Itemsets**: Combinations that appear frequently\n",
    "- **Association Rules**: Co-occurrence patterns\n",
    "- **Summary Metrics**: Processing statistics\n",
    "\n",
    "## S3 Output Structure\n",
    "\n",
    "```\n",
    "gold/fpgrowth/global/\n",
    "‚îú‚îÄ‚îÄ drug_name/\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ encoding_map.json\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ itemsets.json\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ rules.json\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ metrics.json\n",
    "‚îú‚îÄ‚îÄ icd_code/\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ (same files)\n",
    "‚îî‚îÄ‚îÄ cpt_code/\n",
    "    ‚îî‚îÄ‚îÄ (same files)\n",
    "```\n",
    "\n",
    "## Parameters\n",
    "\n",
    "- **Min Support**: 0.005 (items must appear in 0.5% of transactions)\n",
    "- **Min Confidence**: 0.01 (rules must have 1% confidence)\n",
    "- **Data Source**: Local cohort data from `data/gold/cohorts_F1120/`\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import logging\n",
    "from pathlib import Path\n",
    "import time\n",
    "\n",
    "# MLxtend for FP-Growth\n",
    "from mlxtend.frequent_patterns import fpgrowth, association_rules\n",
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "\n",
    "# Project root\n",
    "project_root = Path.cwd().parent if Path.cwd().name == '3_fpgrowth_analysis' else Path.cwd()\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "# Project utilities\n",
    "from helpers_1997_13.common_imports import s3_client, S3_BUCKET\n",
    "from helpers_1997_13.duckdb_utils import get_duckdb_connection\n",
    "from helpers_1997_13.s3_utils import save_to_s3_json, save_to_s3_parquet\n",
    "from helpers_1997_13.drug_utils import encode_drug_name\n",
    "from helpers_1997_13.visualization_utils import create_network_visualization\n",
    "\n",
    "print(f\"‚úì Project root: {project_root}\")\n",
    "print(f\"‚úì All imports successful\")\n",
    "print(f\"‚úì Timestamp: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FP-Growth parameters\n",
    "MIN_SUPPORT = 0.005  # 0.5% support threshold\n",
    "MIN_CONFIDENCE = 0.01  # 1% confidence threshold\n",
    "TOP_K = 50  # Top K itemsets to analyze\n",
    "\n",
    "# Item types to process\n",
    "ITEM_TYPES = ['drug_name', 'icd_code', 'cpt_code']\n",
    "\n",
    "# S3 output base path\n",
    "S3_OUTPUT_BASE = f\"s3://{S3_BUCKET}/gold/fpgrowth/global\"\n",
    "\n",
    "# Local data path\n",
    "LOCAL_DATA_PATH = project_root / \"data\" / \"gold\" / \"cohorts_F1120\"\n",
    "\n",
    "# Create logger\n",
    "logger = logging.getLogger('global_fpgrowth')\n",
    "logger.setLevel(logging.INFO)\n",
    "if not logger.handlers:\n",
    "    handler = logging.StreamHandler()\n",
    "    formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
    "    handler.setFormatter(formatter)\n",
    "    logger.addHandler(handler)\n",
    "\n",
    "print(f\"‚úì Min Support: {MIN_SUPPORT}\")\n",
    "print(f\"‚úì Min Confidence: {MIN_CONFIDENCE}\")\n",
    "print(f\"‚úì Item Types: {ITEM_TYPES}\")\n",
    "print(f\"‚úì S3 Output: {S3_OUTPUT_BASE}\")\n",
    "print(f\"‚úì Local Data: {LOCAL_DATA_PATH}\")\n",
    "print(f\"‚úì Local Data Exists: {LOCAL_DATA_PATH.exists()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Define Item Extraction Functions\n",
    "\n",
    "Create functions to extract different item types from cohort data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_global_items(local_data_path, item_type, logger):\n",
    "    \"\"\"\n",
    "    Extract all unique items of specified type from local cohort data.\n",
    "    \n",
    "    Args:\n",
    "        item_type: 'drug_name', 'icd_code', or 'cpt_code'\n",
    "    \"\"\"\n",
    "    logger.info(f\"Extracting global {item_type}s from local cohort data...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Get DuckDB connection\n",
    "    con = get_duckdb_connection(logger=logger)\n",
    "    \n",
    "    # Build glob pattern for all parquet files\n",
    "    parquet_pattern = str(local_data_path / \"**\" / \"cohort.parquet\")\n",
    "    \n",
    "    # Build query based on item type\n",
    "    if item_type == 'drug_name':\n",
    "        query = f\"\"\"\n",
    "        SELECT DISTINCT drug_name as item\n",
    "        FROM read_parquet('{parquet_pattern}', hive_partitioning=1)\n",
    "        WHERE drug_name IS NOT NULL \n",
    "          AND drug_name != ''\n",
    "          AND event_type = 'PHARMACY'\n",
    "        ORDER BY item\n",
    "        \"\"\"\n",
    "    elif item_type == 'icd_code':\n",
    "        # Collect from all ICD diagnosis columns\n",
    "        query = f\"\"\"\n",
    "        WITH all_icds AS (\n",
    "            SELECT primary_icd_diagnosis_code as icd FROM read_parquet('{parquet_pattern}', hive_partitioning=1) WHERE primary_icd_diagnosis_code IS NOT NULL AND event_type = 'MEDICAL'\n",
    "            UNION ALL\n",
    "            SELECT two_icd_diagnosis_code as icd FROM read_parquet('{parquet_pattern}', hive_partitioning=1) WHERE two_icd_diagnosis_code IS NOT NULL AND event_type = 'MEDICAL'\n",
    "            UNION ALL\n",
    "            SELECT three_icd_diagnosis_code as icd FROM read_parquet('{parquet_pattern}', hive_partitioning=1) WHERE three_icd_diagnosis_code IS NOT NULL AND event_type = 'MEDICAL'\n",
    "            UNION ALL\n",
    "            SELECT four_icd_diagnosis_code as icd FROM read_parquet('{parquet_pattern}', hive_partitioning=1) WHERE four_icd_diagnosis_code IS NOT NULL AND event_type = 'MEDICAL'\n",
    "            UNION ALL\n",
    "            SELECT five_icd_diagnosis_code as icd FROM read_parquet('{parquet_pattern}', hive_partitioning=1) WHERE five_icd_diagnosis_code IS NOT NULL AND event_type = 'MEDICAL'\n",
    "        )\n",
    "        SELECT DISTINCT icd as item FROM all_icds WHERE icd != '' ORDER BY item\n",
    "        \"\"\"\n",
    "    elif item_type == 'cpt_code':\n",
    "        query = f\"\"\"\n",
    "        SELECT DISTINCT procedure_code as item\n",
    "        FROM read_parquet('{parquet_pattern}', hive_partitioning=1)\n",
    "        WHERE procedure_code IS NOT NULL \n",
    "          AND procedure_code != ''\n",
    "          AND event_type = 'MEDICAL'\n",
    "        ORDER BY item\n",
    "        \"\"\"\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown item_type: {item_type}\")\n",
    "    \n",
    "    logger.info(f\"Running query for {item_type}...\")\n",
    "    df = con.execute(query).df()\n",
    "    con.close()\n",
    "    \n",
    "    items = df['item'].tolist()\n",
    "    \n",
    "    elapsed = time.time() - start_time\n",
    "    logger.info(f\"‚úì Extracted {len(items):,} unique {item_type}s in {elapsed:.1f}s\")\n",
    "    \n",
    "    return items\n",
    "\n",
    "# Test extraction function\n",
    "print(\"Testing item extraction...\")\n",
    "test_items = extract_global_items(LOCAL_DATA_PATH, 'drug_name', logger)\n",
    "print(f\"‚úì Found {len(test_items):,} drugs\")\n",
    "print(f\"  Sample: {test_items[:5]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Define Transaction Creation Functions\n",
    "\n",
    "Create patient-level transactions for each item type.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_global_transactions(local_data_path, item_type, logger):\n",
    "    \"\"\"\n",
    "    Create patient-level transactions from local cohort data.\n",
    "    \n",
    "    Args:\n",
    "        item_type: 'drug_name', 'icd_code', or 'cpt_code'\n",
    "    \"\"\"\n",
    "    logger.info(f\"Creating global {item_type} transactions...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Get DuckDB connection\n",
    "    con = get_duckdb_connection(logger=logger)\n",
    "    \n",
    "    # Build glob pattern for all parquet files\n",
    "    parquet_pattern = str(local_data_path / \"**\" / \"cohort.parquet\")\n",
    "    \n",
    "    # Build query based on item type\n",
    "    if item_type == 'drug_name':\n",
    "        query = f\"\"\"\n",
    "        SELECT mi_person_key, drug_name as item\n",
    "        FROM read_parquet('{parquet_pattern}', hive_partitioning=1)\n",
    "        WHERE drug_name IS NOT NULL AND drug_name != '' AND event_type = 'PHARMACY'\n",
    "        \"\"\"\n",
    "    elif item_type == 'icd_code':\n",
    "        query = f\"\"\"\n",
    "        WITH all_icds AS (\n",
    "            SELECT mi_person_key, primary_icd_diagnosis_code as icd FROM read_parquet('{parquet_pattern}', hive_partitioning=1) WHERE primary_icd_diagnosis_code IS NOT NULL AND event_type = 'MEDICAL'\n",
    "            UNION ALL\n",
    "            SELECT mi_person_key, two_icd_diagnosis_code as icd FROM read_parquet('{parquet_pattern}', hive_partitioning=1) WHERE two_icd_diagnosis_code IS NOT NULL AND event_type = 'MEDICAL'\n",
    "            UNION ALL\n",
    "            SELECT mi_person_key, three_icd_diagnosis_code as icd FROM read_parquet('{parquet_pattern}', hive_partitioning=1) WHERE three_icd_diagnosis_code IS NOT NULL AND event_type = 'MEDICAL'\n",
    "        )\n",
    "        SELECT mi_person_key, icd as item FROM all_icds WHERE icd != ''\n",
    "        \"\"\"\n",
    "    elif item_type == 'cpt_code':\n",
    "        query = f\"\"\"\n",
    "        SELECT mi_person_key, procedure_code as item\n",
    "        FROM read_parquet('{parquet_pattern}', hive_partitioning=1)\n",
    "        WHERE procedure_code IS NOT NULL AND procedure_code != '' AND event_type = 'MEDICAL'\n",
    "        \"\"\"\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown item_type: {item_type}\")\n",
    "    \n",
    "    logger.info(f\"Loading {item_type} events...\")\n",
    "    df = con.execute(query).df()\n",
    "    con.close()\n",
    "    \n",
    "    # Group by patient and create item lists\n",
    "    logger.info(f\"Grouping by patient...\")\n",
    "    transactions = (\n",
    "        df.groupby('mi_person_key')['item']\n",
    "        .apply(lambda x: sorted(set(x.tolist())))\n",
    "        .tolist()\n",
    "    )\n",
    "    \n",
    "    elapsed = time.time() - start_time\n",
    "    logger.info(f\"‚úì Created {len(transactions):,} patient transactions in {elapsed:.1f}s\")\n",
    "    \n",
    "    return transactions\n",
    "\n",
    "print(\"‚úì Transaction creation function defined\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Process All Item Types\n",
    "\n",
    "Run FP-Growth analysis for each item type (drug_name, icd_code, cpt_code).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_item_type(item_type, local_data_path, s3_output_base, min_support, min_confidence, logger):\n",
    "    \"\"\"\n",
    "    Process a single item type end-to-end: extract, create transactions, run FP-Growth, save results.\n",
    "    \"\"\"\n",
    "    logger.info(f\"\\n{'='*80}\")\n",
    "    logger.info(f\"Processing {item_type.upper()}\")\n",
    "    logger.info(f\"{'='*80}\")\n",
    "    \n",
    "    overall_start = time.time()\n",
    "    \n",
    "    # Step 1: Extract items\n",
    "    items = extract_global_items(local_data_path, item_type, logger)\n",
    "    \n",
    "    # Step 2: Create transactions\n",
    "    transactions = create_global_transactions(local_data_path, item_type, logger)\n",
    "    \n",
    "    # Step 3: Encode transactions\n",
    "    logger.info(f\"Encoding {len(transactions):,} transactions...\")\n",
    "    te = TransactionEncoder()\n",
    "    te_ary = te.fit(transactions).transform(transactions)\n",
    "    df_encoded = pd.DataFrame(te_ary, columns=te.columns_)\n",
    "    logger.info(f\"‚úì Encoded to {df_encoded.shape} matrix\")\n",
    "    \n",
    "    # Step 4: Run FP-Growth\n",
    "    logger.info(f\"Running FP-Growth...\")\n",
    "    itemsets = fpgrowth(df_encoded, min_support=min_support, use_colnames=True)\n",
    "    itemsets = itemsets.sort_values('support', ascending=False).reset_index(drop=True)\n",
    "    logger.info(f\"‚úì Found {len(itemsets):,} frequent itemsets\")\n",
    "    \n",
    "    # Step 5: Generate association rules\n",
    "    logger.info(f\"Generating association rules...\")\n",
    "    try:\n",
    "        rules = association_rules(itemsets, metric=\"confidence\", min_threshold=min_confidence)\n",
    "        rules = rules.sort_values('lift', ascending=False).reset_index(drop=True)\n",
    "        logger.info(f\"‚úì Generated {len(rules):,} association rules\")\n",
    "    except ValueError as e:\n",
    "        logger.warning(f\"Could not generate rules: {e}\")\n",
    "        rules = pd.DataFrame()\n",
    "    \n",
    "    # Step 6: Create encoding map\n",
    "    logger.info(f\"Creating encoding map...\")\n",
    "    encoding_map = {}\n",
    "    for item in items:\n",
    "        support = 0.0\n",
    "        matching = itemsets[itemsets['itemsets'].apply(lambda x: item in x)]\n",
    "        if not matching.empty:\n",
    "            support = matching['support'].max()\n",
    "        \n",
    "        confidence = 0.0\n",
    "        if not rules.empty:\n",
    "            matching_rules = rules[\n",
    "                rules['antecedents'].apply(lambda x: item in x) |\n",
    "                rules['consequents'].apply(lambda x: item in x)\n",
    "            ]\n",
    "            if not matching_rules.empty:\n",
    "                confidence = matching_rules['confidence'].max()\n",
    "        \n",
    "        # Simple encoding: item_supportXXX_confidenceYYY\n",
    "        encoding = f\"{item}_{int(support*1000):04d}_{int(confidence*1000):04d}\"\n",
    "        encoding_map[item] = encoding\n",
    "    \n",
    "    logger.info(f\"‚úì Created encoding map with {len(encoding_map):,} items\")\n",
    "    \n",
    "    # Step 7: Save to S3\n",
    "    s3_folder = f\"{s3_output_base}/{item_type}\"\n",
    "    logger.info(f\"Saving to {s3_folder}...\")\n",
    "    \n",
    "    # Convert frozensets to lists\n",
    "    itemsets_json = itemsets.copy()\n",
    "    itemsets_json['itemsets'] = itemsets_json['itemsets'].apply(lambda x: list(x))\n",
    "    \n",
    "    # Save encoding map\n",
    "    encoding_path = f\"{s3_folder}/encoding_map.json\"\n",
    "    save_to_s3_json(encoding_map, encoding_path)\n",
    "    \n",
    "    # Save itemsets\n",
    "    itemsets_path = f\"{s3_folder}/itemsets.json\"\n",
    "    save_to_s3_json(itemsets_json.to_dict(orient='records'), itemsets_path)\n",
    "    \n",
    "    # Save rules\n",
    "    if not rules.empty:\n",
    "        rules_json = rules.copy()\n",
    "        rules_json['antecedents'] = rules_json['antecedents'].apply(lambda x: list(x))\n",
    "        rules_json['consequents'] = rules_json['consequents'].apply(lambda x: list(x))\n",
    "        rules_path = f\"{s3_folder}/rules.json\"\n",
    "        save_to_s3_json(rules_json.to_dict(orient='records'), rules_path)\n",
    "    \n",
    "    # Save metrics\n",
    "    summary = {\n",
    "        'timestamp': datetime.now().isoformat(),\n",
    "        'item_type': item_type,\n",
    "        'total_items': len(items),\n",
    "        'total_patients': len(transactions),\n",
    "        'total_itemsets': len(itemsets),\n",
    "        'total_rules': len(rules),\n",
    "        'min_support': min_support,\n",
    "        'min_confidence': min_confidence,\n",
    "        'avg_items_per_patient': float(np.mean([len(t) for t in transactions]))\n",
    "    }\n",
    "    metrics_path = f\"{s3_folder}/metrics.json\"\n",
    "    save_to_s3_json(summary, metrics_path)\n",
    "    \n",
    "    elapsed = time.time() - overall_start\n",
    "    logger.info(f\"‚úì {item_type} complete in {elapsed:.1f}s ({elapsed/60:.1f}min)\")\n",
    "    \n",
    "    return {\n",
    "        'item_type': item_type,\n",
    "        'total_items': len(items),\n",
    "        'total_patients': len(transactions),\n",
    "        'total_itemsets': len(itemsets),\n",
    "        'total_rules': len(rules),\n",
    "        'elapsed_seconds': elapsed,\n",
    "        's3_folder': s3_folder\n",
    "    }\n",
    "\n",
    "# Process all item types\n",
    "print(\"\\nüöÄ Starting FP-Growth analysis for all item types...\\n\")\n",
    "results = []\n",
    "\n",
    "for item_type in ITEM_TYPES:\n",
    "    try:\n",
    "        result = process_item_type(item_type, LOCAL_DATA_PATH, S3_OUTPUT_BASE, MIN_SUPPORT, MIN_CONFIDENCE, logger)\n",
    "        results.append(result)\n",
    "        print(f\"\\n‚úÖ {item_type}: {result['total_itemsets']:,} itemsets, {result['total_rules']:,} rules\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"‚ùå Failed to process {item_type}: {e}\")\n",
    "        results.append({'item_type': item_type, 'error': str(e)})\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ALL ITEM TYPES PROCESSED\")\n",
    "print(\"=\"*80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Run FP-Growth Algorithm\n",
    "\n",
    "Apply FP-Growth to discover frequent drug itemsets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(f\"Running FP-Growth with min_support={MIN_SUPPORT}...\")\n",
    "start_time = time.time()\n",
    "\n",
    "# Run FP-Growth\n",
    "frequent_itemsets = fpgrowth(df_encoded, min_support=MIN_SUPPORT, use_colnames=True)\n",
    "\n",
    "# Sort by support\n",
    "frequent_itemsets = frequent_itemsets.sort_values('support', ascending=False).reset_index(drop=True)\n",
    "\n",
    "elapsed = time.time() - start_time\n",
    "logger.info(f\"‚úì Found {len(frequent_itemsets):,} frequent itemsets in {elapsed:.1f}s\")\n",
    "\n",
    "print(f\"\\nüìä Frequent Itemsets:\")\n",
    "print(f\"  Total itemsets: {len(frequent_itemsets):,}\")\n",
    "print(f\"  Size 1 (single drugs): {(frequent_itemsets['itemsets'].apply(len) == 1).sum():,}\")\n",
    "print(f\"  Size 2 (pairs): {(frequent_itemsets['itemsets'].apply(len) == 2).sum():,}\")\n",
    "print(f\"  Size 3+: {(frequent_itemsets['itemsets'].apply(len) >= 3).sum():,}\")\n",
    "print(f\"\\n  Top 10 frequent itemsets:\")\n",
    "print(frequent_itemsets.head(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Generate Association Rules\n",
    "\n",
    "Create association rules from frequent itemsets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(f\"Generating association rules with min_confidence={MIN_CONFIDENCE}...\")\n",
    "start_time = time.time()\n",
    "\n",
    "# Generate rules\n",
    "try:\n",
    "    rules = association_rules(frequent_itemsets, metric=\"confidence\", min_threshold=MIN_CONFIDENCE)\n",
    "    rules = rules.sort_values('lift', ascending=False).reset_index(drop=True)\n",
    "    \n",
    "    elapsed = time.time() - start_time\n",
    "    logger.info(f\"‚úì Generated {len(rules):,} association rules in {elapsed:.1f}s\")\n",
    "    \n",
    "    print(f\"\\nüìä Association Rules:\")\n",
    "    print(f\"  Total rules: {len(rules):,}\")\n",
    "    print(f\"  Avg confidence: {rules['confidence'].mean():.3f}\")\n",
    "    print(f\"  Avg lift: {rules['lift'].mean():.2f}\")\n",
    "    print(f\"\\n  Top 10 rules by lift:\")\n",
    "    print(rules[['antecedents', 'consequents', 'support', 'confidence', 'lift']].head(10))\n",
    "    \n",
    "except ValueError as e:\n",
    "    logger.warning(f\"Could not generate rules: {e}\")\n",
    "    rules = pd.DataFrame()\n",
    "    print(f\"\\n‚ö†Ô∏è No association rules generated (itemsets may be too sparse)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Create Global Drug Encoding Map\n",
    "\n",
    "Generate universal drug encodings with FP-Growth metrics for ML features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_global_encoding_map(drug_names, itemsets_df, rules_df, logger):\n",
    "    \"\"\"\n",
    "    Create global drug encoding map with FP-Growth metrics.\n",
    "    \"\"\"\n",
    "    logger.info(\"Creating global drug encoding map...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    encoding_map = {}\n",
    "    \n",
    "    for drug in drug_names:\n",
    "        # Get support from itemsets\n",
    "        support = 0.0\n",
    "        matching_itemsets = itemsets_df[itemsets_df['itemsets'].apply(lambda x: drug in x)]\n",
    "        if not matching_itemsets.empty:\n",
    "            support = matching_itemsets['support'].max()\n",
    "        \n",
    "        # Get confidence from rules\n",
    "        confidence = 0.0\n",
    "        if not rules_df.empty:\n",
    "            matching_rules = rules_df[\n",
    "                rules_df['antecedents'].apply(lambda x: drug in x) |\n",
    "                rules_df['consequents'].apply(lambda x: drug in x)\n",
    "            ]\n",
    "            if not matching_rules.empty:\n",
    "                confidence = matching_rules['confidence'].max()\n",
    "        \n",
    "        # Create encoding\n",
    "        encoding = encode_drug_name(drug, support=support, confidence=confidence)\n",
    "        encoding_map[drug] = encoding\n",
    "    \n",
    "    elapsed = time.time() - start_time\n",
    "    logger.info(f\"‚úì Created encoding map for {len(encoding_map):,} drugs in {elapsed:.1f}s\")\n",
    "    \n",
    "    return encoding_map\n",
    "\n",
    "# Create encoding map\n",
    "encoding_map = create_global_encoding_map(global_drug_names, frequent_itemsets, rules, logger)\n",
    "\n",
    "print(f\"\\nüìä Global Drug Encoding Map:\")\n",
    "print(f\"  Total drugs encoded: {len(encoding_map):,}\")\n",
    "print(f\"\\n  Sample encodings (first 10 drugs):\")\n",
    "for i, (drug, encoding) in enumerate(list(encoding_map.items())[:10], 1):\n",
    "    print(f\"    {i}. {drug[:30]:30s} ‚Üí {encoding}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Save Results to S3\n",
    "\n",
    "Upload all results to S3 for downstream analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(\"Saving results to S3...\")\n",
    "start_time = time.time()\n",
    "\n",
    "# Convert frozensets to lists for JSON serialization\n",
    "itemsets_json = frequent_itemsets.copy()\n",
    "itemsets_json['itemsets'] = itemsets_json['itemsets'].apply(lambda x: list(x))\n",
    "\n",
    "if not rules.empty:\n",
    "    rules_json = rules.copy()\n",
    "    rules_json['antecedents'] = rules_json['antecedents'].apply(lambda x: list(x))\n",
    "    rules_json['consequents'] = rules_json['consequents'].apply(lambda x: list(x))\n",
    "else:\n",
    "    rules_json = pd.DataFrame()\n",
    "\n",
    "# Save to S3\n",
    "results_saved = {}\n",
    "\n",
    "try:\n",
    "    # 1. Drug encoding map\n",
    "    encoding_path = f\"{S3_OUTPUT_BASE}/drug_encoding_map.json\"\n",
    "    save_to_s3_json(encoding_map, encoding_path)\n",
    "    results_saved['encoding_map'] = encoding_path\n",
    "    logger.info(f\"‚úì Saved encoding map to {encoding_path}\")\n",
    "    \n",
    "    # 2. Frequent itemsets\n",
    "    itemsets_path = f\"{S3_OUTPUT_BASE}/global_itemsets.json\"\n",
    "    save_to_s3_json(itemsets_json.to_dict(orient='records'), itemsets_path)\n",
    "    results_saved['itemsets'] = itemsets_path\n",
    "    logger.info(f\"‚úì Saved itemsets to {itemsets_path}\")\n",
    "    \n",
    "    # 3. Association rules\n",
    "    if not rules_json.empty:\n",
    "        rules_path = f\"{S3_OUTPUT_BASE}/global_rules.json\"\n",
    "        save_to_s3_json(rules_json.to_dict(orient='records'), rules_path)\n",
    "        results_saved['rules'] = rules_path\n",
    "        logger.info(f\"‚úì Saved rules to {rules_path}\")\n",
    "    \n",
    "    # 4. Summary metrics\n",
    "    summary = {\n",
    "        'timestamp': datetime.now().isoformat(),\n",
    "        'total_drugs': len(global_drug_names),\n",
    "        'total_patients': len(transactions),\n",
    "        'total_itemsets': len(frequent_itemsets),\n",
    "        'total_rules': len(rules),\n",
    "        'min_support': MIN_SUPPORT,\n",
    "        'min_confidence': MIN_CONFIDENCE,\n",
    "        'avg_drugs_per_patient': float(np.mean([len(t) for t in transactions])),\n",
    "        'output_paths': results_saved\n",
    "    }\n",
    "    \n",
    "    summary_path = f\"{S3_OUTPUT_BASE}/global_metrics.json\"\n",
    "    save_to_s3_json(summary, summary_path)\n",
    "    results_saved['summary'] = summary_path\n",
    "    logger.info(f\"‚úì Saved summary to {summary_path}\")\n",
    "    \n",
    "    elapsed = time.time() - start_time\n",
    "    logger.info(f\"‚úì All results saved to S3 in {elapsed:.1f}s\")\n",
    "    \n",
    "    print(f\"\\n‚úÖ Results saved to S3:\")\n",
    "    for result_type, path in results_saved.items():\n",
    "        print(f\"  {result_type}: {path}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    logger.error(f\"Error saving to S3: {e}\")\n",
    "    print(f\"\\n‚ùå Error saving to S3: {e}\")\n",
    "    raise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Next Steps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"GLOBAL FPGROWTH ANALYSIS - SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nüìä Dataset Statistics:\")\n",
    "print(f\"  Total unique drugs: {len(global_drug_names):,}\")\n",
    "print(f\"  Total patients: {len(transactions):,}\")\n",
    "print(f\"  Avg drugs/patient: {np.mean([len(t) for t in transactions]):.1f}\")\n",
    "\n",
    "print(f\"\\nüîç FP-Growth Results:\")\n",
    "print(f\"  Min support: {MIN_SUPPORT} ({MIN_SUPPORT*100:.2f}%)\")\n",
    "print(f\"  Min confidence: {MIN_CONFIDENCE} ({MIN_CONFIDENCE*100:.1f}%)\")\n",
    "print(f\"  Frequent itemsets: {len(frequent_itemsets):,}\")\n",
    "print(f\"  Association rules: {len(rules):,}\")\n",
    "\n",
    "print(f\"\\nüíæ Output Files (S3):\")\n",
    "for result_type, path in results_saved.items():\n",
    "    print(f\"  {result_type}: {path}\")\n",
    "\n",
    "print(f\"\\nüéØ Next Steps:\")\n",
    "print(f\"  1. Load encoding map in CatBoost: load_from_s3_json('{results_saved['encoding_map']}')\")\n",
    "print(f\"  2. Use encodings for feature engineering in ML models\")\n",
    "print(f\"  3. Analyze association rules for drug interaction patterns\")\n",
    "print(f\"  4. Run cohort-specific FPGrowth analysis for detailed insights\")\n",
    "\n",
    "print(f\"\\n‚úì Analysis complete: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(\"=\"*80)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
