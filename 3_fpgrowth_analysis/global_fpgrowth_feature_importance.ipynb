{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f8e7c275",
   "metadata": {},
   "source": [
    "# Global FPGrowth Feature Importance Analysis - Target-Focused\n",
    "\n",
    "**Purpose:** Target-focused pattern mining - discovers patterns that PREDICT outcomes  \n",
    "**Updated:** November 24, 2025  \n",
    "**Hardware:** x2iedn.8xlarge (32 vCPUs, 1024 GiB RAM, NVMe SSD)  \n",
    "**Data:** `/mnt/nvme/cohorts/` (instance storage for fast I/O)  \n",
    "**Output:** `s3://pgxdatalake/gold/fpgrowth/global/{item_type}/`\n",
    "\n",
    "## üéØ Target-Focused Approach\n",
    "\n",
    "**NEW:** Instead of ALL rules, generates ONLY predictive rules:\n",
    "- **`rules_TARGET_ICD.json`** - Patterns predicting opioid dependence (F11.20-F11.29)\n",
    "- **`rules_TARGET_ED.json`** - Patterns predicting ED visits (HCG Lines: P51, O11, P33)\n",
    "- **`rules_CONTROL.json`** - Baseline patterns (non-target, for comparison)\n",
    "\n",
    "## Key Features\n",
    "\n",
    "‚úÖ **Target-Focused** - Predictive rules only, not descriptive  \n",
    "‚úÖ **Three Item Types** - Drugs, ICD codes, CPT codes  \n",
    "‚úÖ **Global Patterns** - 5.7M patients across all cohorts  \n",
    "‚úÖ **Quality Over Quantity** - 50% confidence (not 1%!)  \n",
    "‚úÖ **Comparative Analysis** - Target vs Control differences\n",
    "\n",
    "## Methodology\n",
    "\n",
    "For each item type (drug_name, icd_code, cpt_code):\n",
    "1. Extract all unique items from cohort data\n",
    "2. Add target markers (TARGET_ICD, TARGET_ED) to patient transactions\n",
    "3. Create patient-level transactions\n",
    "4. Run FP-Growth to find frequent itemsets\n",
    "5. Generate ALL association rules\n",
    "6. **Split rules**: Target-predicting vs Control (baseline)\n",
    "7. **Split target rules**: ICD vs ED outcomes\n",
    "8. Save three separate files per item type\n",
    "\n",
    "## Expected Runtime (x2iedn.8xlarge)\n",
    "\n",
    "### Parallel Execution (3 workers, all running simultaneously):\n",
    "\n",
    "- **Drug names**: 15-25 min (28K items, 5.7M transactions)\n",
    "- **ICD codes**: 15-25 min (20K items, 5.7M transactions)\n",
    "- **CPT codes**: 20-35 min (15K items, more complex patterns)\n",
    "- **Total (PARALLEL)**: **‚ö° 20-35 minutes** (longest job determines runtime!)\n",
    "\n",
    "**Sequential would be**: 50-85 min  \n",
    "**Speedup**: **2.5-3x faster** with parallel execution!\n",
    "\n",
    "**Cost**: ~$2-3 on Spot pricing (~$6-7 on-demand)\n",
    "\n",
    "### Why Parallel Works Here:\n",
    "- ‚úÖ 1024 GiB RAM ‚Üí ~340 GB per worker (plenty!)\n",
    "- ‚úÖ 32 vCPUs ‚Üí ~10 cores per worker\n",
    "- ‚úÖ Independent jobs ‚Üí no data sharing needed\n",
    "- ‚úÖ NVMe SSD ‚Üí fast I/O even with 3 concurrent reads\n",
    "\n",
    "## Data Scale\n",
    "\n",
    "- **Total Events**: 947 million\n",
    "- **Patients**: 5.7 million\n",
    "- **Unique Drugs**: ~28,000\n",
    "- **Unique ICD Codes**: ~20,000\n",
    "- **Unique CPT Codes**: ~15,000\n",
    "- **Data Location**: `/mnt/nvme/cohorts/` (NVMe SSD for fast I/O)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c526ba4",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a6f9a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì All imports successful\n",
      "‚úì Project root: /home/pgx3874/pgx-analysis\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import time\n",
    "import json\n",
    "import logging\n",
    "from pathlib import Path\n",
    "from typing import List, Dict\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "import pandas as pd\n",
    "import boto3\n",
    "from mlxtend.frequent_patterns import fpgrowth, association_rules\n",
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "\n",
    "# Add project root to path\n",
    "project_root = Path.cwd().parent if Path.cwd().name == '3_fpgrowth_analysis' else Path.cwd()\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "from helpers_1997_13.duckdb_utils import get_duckdb_connection\n",
    "\n",
    "print(\"‚úì All imports successful (including ProcessPoolExecutor for parallel execution)\")\n",
    "print(f\"‚úì Project root: {project_root}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f19b589",
   "metadata": {},
   "source": [
    "## 2. Configuration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20f8f5db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Min Support: 0.01\n",
      "‚úì Min Confidence: 0.01\n",
      "‚úì Item Types: ['drug_name', 'icd_code', 'cpt_code']\n",
      "‚úì S3 Output: s3://pgxdatalake/gold/fpgrowth/global\n",
      "‚úì Local Data: /home/pgx3874/pgx-analysis/data\n",
      "‚úì Local Data Exists: True\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# EC2 CONFIGURATION (32 cores, 1TB RAM)\n",
    "# =============================================================================\n",
    "\n",
    "# FP-Growth parameters (quality-focused for ML features)\n",
    "MIN_SUPPORT = 0.01       # Items must appear in 1% of patients (5.7M patients = 57K occurrences)\n",
    "MIN_CONFIDENCE = 0.4     # 40% confidence - meaningful associations for CatBoost\n",
    "\n",
    "# Item-specific thresholds (balance coverage vs quality)\n",
    "MIN_CONFIDENCE_CPT = 0.5 # 50% confidence for CPT - strong procedure associations\n",
    "MIN_SUPPORT_CPT = 0.02   # 2% support for CPT - focuses on common procedures\n",
    "\n",
    "# Rule limits (quality over quantity)\n",
    "MAX_RULES_PER_ITEM_TYPE = 5000  # Top 5000 rules by lift (for ML feature engineering)\n",
    "\n",
    "# Target-focused rule mining (NEW!)\n",
    "TARGET_FOCUSED = True  # Only generate rules that predict target outcomes\n",
    "TARGET_ICD_CODES = ['F11.20', 'F11.21', 'F11.22', 'F11.23', 'F11.24', 'F11.25', 'F11.29']  # Opioid dependence codes\n",
    "TARGET_HCG_LINES = [\n",
    "    \"P51 - ER Visits and Observation Care\",\n",
    "    \"O11 - Emergency Room\",\n",
    "    \"P33 - Urgent Care Visits\"\n",
    "]  # ED visits (HCG Line codes - matches phase2_event_processing.py)\n",
    "TARGET_PREFIXES = ['TARGET_ICD:', 'TARGET_ED:']  # Prefixes for target items in transactions\n",
    "\n",
    "# Item types to process\n",
    "ITEM_TYPES = ['drug_name', 'icd_code', 'cpt_code']\n",
    "\n",
    "# Paths (x2iedn.8xlarge optimized)\n",
    "S3_OUTPUT_BASE = \"s3://pgxdatalake/gold/fpgrowth/global\"\n",
    "LOCAL_DATA_PATH = Path(\"/mnt/nvme/cohorts\")  # Instance storage (NVMe SSD for fast I/O)\n",
    "\n",
    "# Setup logger with file output (prevents Jupyter rate limit issues)\n",
    "logger = logging.getLogger('global_fpgrowth')\n",
    "logger.setLevel(logging.INFO)\n",
    "logger.handlers.clear()  # Clear any existing handlers\n",
    "\n",
    "# File handler - full logs to file\n",
    "log_file = project_root / \"3_fpgrowth_analysis\" / \"global_fpgrowth_execution.log\"\n",
    "file_handler = logging.FileHandler(log_file)\n",
    "file_handler.setLevel(logging.INFO)\n",
    "file_handler.setFormatter(logging.Formatter('%(asctime)s - %(levelname)s - %(message)s'))\n",
    "logger.addHandler(file_handler)\n",
    "\n",
    "# Console handler - only major milestones (prevents Jupyter rate limit)\n",
    "console_handler = logging.StreamHandler()\n",
    "console_handler.setLevel(logging.WARNING)  # Only warnings/errors to console\n",
    "console_handler.setFormatter(logging.Formatter('%(asctime)s - %(levelname)s - %(message)s'))\n",
    "logger.addHandler(console_handler)\n",
    "\n",
    "print(f\"‚úì Min Support (drug/ICD): {MIN_SUPPORT} (1% = ~57K patients)\")\n",
    "print(f\"‚úì Min Support (CPT): {MIN_SUPPORT_CPT} (2% = ~114K patients)\")\n",
    "print(f\"‚úì Min Confidence (drug/ICD): {MIN_CONFIDENCE} (40% - meaningful associations)\")\n",
    "print(f\"‚úì Min Confidence (CPT): {MIN_CONFIDENCE_CPT} (50% - strong procedure patterns)\")\n",
    "print(f\"‚úì Max Rules per Item Type: {MAX_RULES_PER_ITEM_TYPE:,} (top by lift)\")\n",
    "print(f\"‚úì Item Types: {ITEM_TYPES}\")\n",
    "print(f\"‚úì S3 Output: {S3_OUTPUT_BASE}\")\n",
    "print(f\"‚úì Local Data: {LOCAL_DATA_PATH}\")\n",
    "print(f\"‚úì Local Data Exists: {LOCAL_DATA_PATH.exists()}\")\n",
    "print(f\"‚úì Detailed logs ‚Üí {log_file}\")\n",
    "print(f\"‚úì Console output: WARNING level only (check log file for progress)\")\n",
    "print(\"\\nüéØ Quality-Focused Global Analysis:\")\n",
    "print(\"  - 5.7M patients = strong statistical power\")\n",
    "print(\"  - High confidence (40-50%) = meaningful patterns for CatBoost\")\n",
    "print(\"  - Top 5,000 rules per type = comprehensive yet manageable\")\n",
    "print(\"  - Encoding maps for all frequent items (for feature engineering)\")\n",
    "print(f\"\\nüéØ TARGET-FOCUSED RULE MINING: {'ENABLED' if TARGET_FOCUSED else 'DISABLED'}\")\n",
    "if TARGET_FOCUSED:\n",
    "    print(f\"  - Target ICD codes: {TARGET_ICD_CODES}\")\n",
    "    print(f\"  - Target HCG lines (ED visits): {TARGET_HCG_LINES}\")\n",
    "    print(\"  - Only generates rules that PREDICT target outcomes\")\n",
    "    print(\"  - Example: {Metoprolol, Gabapentin} ‚Üí {TARGET_ICD:OPIOID_DEPENDENCE}\")\n",
    "    print(\"  - Example: {99213: Office Visit, J0670: Morphine} ‚Üí {TARGET_ED:EMERGENCY_DEPT}\")\n",
    "    print(\"  ‚úÖ Focus on PREDICTIVE patterns across 5.7M patients\")\n",
    "    print(\"  ‚úÖ Better CatBoost features (what predicts target?)\")\n",
    "    print(\"  ‚úÖ Interpretable global patterns (risk factors)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38d439c9",
   "metadata": {},
   "source": [
    "## 3. Define Helper Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "63384d4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Helper functions defined\n"
     ]
    }
   ],
   "source": [
    "def extract_global_items(local_data_path: Path, item_type: str, logger: logging.Logger) -> List[str]:\n",
    "    \"\"\"Extract all unique items of specified type from local cohort data.\"\"\"\n",
    "    logger.info(f\"Extracting global {item_type}s from local cohort data...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    con = get_duckdb_connection(logger=logger)\n",
    "    parquet_pattern = str(local_data_path / \"**\" / \"cohort.parquet\")\n",
    "    \n",
    "    if item_type == 'drug_name':\n",
    "        query = f\"\"\"\n",
    "        SELECT DISTINCT drug_name as item\n",
    "        FROM read_parquet('{parquet_pattern}', hive_partitioning=1)\n",
    "        WHERE drug_name IS NOT NULL AND drug_name != '' AND event_type = 'pharmacy'\n",
    "        ORDER BY item\n",
    "        \"\"\"\n",
    "    elif item_type == 'icd_code':\n",
    "        query = f\"\"\"\n",
    "        WITH all_icds AS (\n",
    "            SELECT primary_icd_diagnosis_code as icd FROM read_parquet('{parquet_pattern}', hive_partitioning=1) \n",
    "            WHERE primary_icd_diagnosis_code IS NOT NULL AND event_type = 'medical'\n",
    "            UNION ALL\n",
    "            SELECT two_icd_diagnosis_code as icd FROM read_parquet('{parquet_pattern}', hive_partitioning=1) \n",
    "            WHERE two_icd_diagnosis_code IS NOT NULL AND event_type = 'medical'\n",
    "            UNION ALL\n",
    "            SELECT three_icd_diagnosis_code as icd FROM read_parquet('{parquet_pattern}', hive_partitioning=1) \n",
    "            WHERE three_icd_diagnosis_code IS NOT NULL AND event_type = 'medical'\n",
    "            UNION ALL\n",
    "            SELECT four_icd_diagnosis_code as icd FROM read_parquet('{parquet_pattern}', hive_partitioning=1) \n",
    "            WHERE four_icd_diagnosis_code IS NOT NULL AND event_type = 'medical'\n",
    "            UNION ALL\n",
    "            SELECT five_icd_diagnosis_code as icd FROM read_parquet('{parquet_pattern}', hive_partitioning=1) \n",
    "            WHERE five_icd_diagnosis_code IS NOT NULL AND event_type = 'medical'\n",
    "        )\n",
    "        SELECT DISTINCT icd as item FROM all_icds WHERE icd != '' ORDER BY item\n",
    "        \"\"\"\n",
    "    elif item_type == 'cpt_code':\n",
    "        query = f\"\"\"\n",
    "        SELECT DISTINCT procedure_code as item\n",
    "        FROM read_parquet('{parquet_pattern}', hive_partitioning=1)\n",
    "        WHERE procedure_code IS NOT NULL AND procedure_code != '' AND event_type = 'medical'\n",
    "        ORDER BY item\n",
    "        \"\"\"\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown item_type: {item_type}\")\n",
    "    \n",
    "    logger.info(f\"Running query for {item_type}...\")\n",
    "    df = con.execute(query).df()\n",
    "    con.close()\n",
    "    items = df['item'].tolist()\n",
    "    \n",
    "    elapsed = time.time() - start_time\n",
    "    logger.info(f\"‚úì Extracted {len(items):,} unique {item_type}s in {elapsed:.1f}s\")\n",
    "    return items\n",
    "\n",
    "\n",
    "def create_global_transactions(local_data_path: Path, item_type: str, logger: logging.Logger) -> List[List[str]]:\n",
    "    \"\"\"Create patient-level transactions from local cohort data.\"\"\"\n",
    "    logger.info(f\"Creating global {item_type} transactions...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    con = get_duckdb_connection(logger=logger)\n",
    "    parquet_pattern = str(local_data_path / \"**\" / \"cohort.parquet\")\n",
    "    \n",
    "    if item_type == 'drug_name':\n",
    "        query = f\"\"\"\n",
    "        SELECT mi_person_key, drug_name as item\n",
    "        FROM read_parquet('{parquet_pattern}', hive_partitioning=1)\n",
    "        WHERE drug_name IS NOT NULL AND drug_name != '' AND event_type = 'pharmacy'\n",
    "        \"\"\"\n",
    "    elif item_type == 'icd_code':\n",
    "        query = f\"\"\"\n",
    "        WITH all_icds AS (\n",
    "            SELECT mi_person_key, primary_icd_diagnosis_code as icd FROM read_parquet('{parquet_pattern}', hive_partitioning=1) \n",
    "            WHERE primary_icd_diagnosis_code IS NOT NULL AND event_type = 'medical'\n",
    "            UNION ALL\n",
    "            SELECT mi_person_key, two_icd_diagnosis_code as icd FROM read_parquet('{parquet_pattern}', hive_partitioning=1) \n",
    "            WHERE two_icd_diagnosis_code IS NOT NULL AND event_type = 'medical'\n",
    "            UNION ALL\n",
    "            SELECT mi_person_key, three_icd_diagnosis_code as icd FROM read_parquet('{parquet_pattern}', hive_partitioning=1) \n",
    "            WHERE three_icd_diagnosis_code IS NOT NULL AND event_type = 'medical'\n",
    "            UNION ALL\n",
    "            SELECT mi_person_key, four_icd_diagnosis_code as icd FROM read_parquet('{parquet_pattern}', hive_partitioning=1) \n",
    "            WHERE four_icd_diagnosis_code IS NOT NULL AND event_type = 'medical'\n",
    "            UNION ALL\n",
    "            SELECT mi_person_key, five_icd_diagnosis_code as icd FROM read_parquet('{parquet_pattern}', hive_partitioning=1) \n",
    "            WHERE five_icd_diagnosis_code IS NOT NULL AND event_type = 'medical'\n",
    "        )\n",
    "        SELECT mi_person_key, icd as item FROM all_icds WHERE icd != ''\n",
    "        \"\"\"\n",
    "    elif item_type == 'cpt_code':\n",
    "        query = f\"\"\"\n",
    "        SELECT mi_person_key, procedure_code as item\n",
    "        FROM read_parquet('{parquet_pattern}', hive_partitioning=1)\n",
    "        WHERE procedure_code IS NOT NULL AND procedure_code != '' AND event_type = 'medical'\n",
    "        \"\"\"\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown item_type: {item_type}\")\n",
    "    \n",
    "    logger.info(f\"Loading {item_type} events...\")\n",
    "    df = con.execute(query).df()\n",
    "    con.close()\n",
    "    \n",
    "    logger.info(f\"Grouping by patient...\")\n",
    "    transactions = (\n",
    "        df.groupby('mi_person_key')['item']\n",
    "        .apply(lambda x: sorted(set(x.tolist())))\n",
    "        .tolist()\n",
    "    )\n",
    "    \n",
    "    elapsed = time.time() - start_time\n",
    "    logger.info(f\"‚úì Created {len(transactions):,} patient transactions in {elapsed:.1f}s\")\n",
    "    return transactions\n",
    "\n",
    "print(\"‚úì Helper functions defined\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccc1346a",
   "metadata": {},
   "source": [
    "## 4. FP-Growth Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5415ef4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì process_item_type function defined\n"
     ]
    }
   ],
   "source": [
    "def process_item_type(item_type, local_data_path, s3_output_base, min_support, min_confidence, logger):\n",
    "    \"\"\"Process a single item type: extract, encode, FP-Growth, save to S3.\"\"\"\n",
    "    logger.info(f\"\\n{'='*80}\")\n",
    "    logger.info(f\"Processing {item_type.upper()}\")\n",
    "    logger.info(f\"{'='*80}\")\n",
    "    overall_start = time.time()\n",
    "    \n",
    "    try:\n",
    "        # Extract items\n",
    "        items = extract_global_items(local_data_path, item_type, logger)\n",
    "        \n",
    "        # Create transactions\n",
    "        transactions = create_global_transactions(local_data_path, item_type, logger)\n",
    "        \n",
    "        # Encode transactions\n",
    "        logger.info(f\"Encoding {len(transactions):,} transactions...\")\n",
    "        encode_start = time.time()\n",
    "        te = TransactionEncoder()\n",
    "        te_ary = te.fit(transactions).transform(transactions)\n",
    "        df_encoded = pd.DataFrame(te_ary, columns=te.columns_)\n",
    "        encode_time = time.time() - encode_start\n",
    "        logger.info(f\"‚úì Encoded to {df_encoded.shape} matrix in {encode_time:.1f}s\")\n",
    "        \n",
    "        # Run FP-Growth (use item-specific parameters)\n",
    "        actual_min_support = MIN_SUPPORT_CPT if item_type == 'cpt_code' else min_support\n",
    "        logger.info(f\"Running FP-Growth (min_support={actual_min_support})...\")\n",
    "        fpgrowth_start = time.time()\n",
    "        itemsets = fpgrowth(df_encoded, min_support=actual_min_support, use_colnames=True)\n",
    "        itemsets = itemsets.sort_values('support', ascending=False).reset_index(drop=True)\n",
    "        fpgrowth_time = time.time() - fpgrowth_start\n",
    "        logger.info(f\"‚úì Found {len(itemsets):,} frequent itemsets in {fpgrowth_time:.1f}s\")\n",
    "        \n",
    "        # Generate association rules (with item-specific confidence and limits)\n",
    "        actual_min_confidence = MIN_CONFIDENCE_CPT if item_type == 'cpt_code' else min_confidence\n",
    "        logger.info(f\"Generating association rules (min_confidence={actual_min_confidence})...\")\n",
    "        rules_start = time.time()\n",
    "        \n",
    "        try:\n",
    "            all_rules = association_rules(itemsets, metric=\"confidence\", min_threshold=actual_min_confidence)\n",
    "            \n",
    "            if len(all_rules) > 0:\n",
    "                # Split rules: target-predicting vs control (non-target)\n",
    "                if TARGET_FOCUSED:\n",
    "                    # Target rules: consequent contains target marker\n",
    "                    target_mask = all_rules['consequents'].apply(\n",
    "                        lambda x: any(item.startswith(tuple(TARGET_PREFIXES)) for item in x)\n",
    "                    )\n",
    "                    rules_target = all_rules[target_mask].copy()\n",
    "                    rules_control = all_rules[~target_mask].copy()\n",
    "                    \n",
    "                    logger.info(f\"Split: {len(rules_target):,} target rules, {len(rules_control):,} control rules\")\n",
    "                    \n",
    "                    # Limit both sets to top N by lift\n",
    "                    if len(rules_target) > 0:\n",
    "                        rules_target = rules_target.sort_values('lift', ascending=False)\n",
    "                        if len(rules_target) > MAX_RULES_PER_ITEM_TYPE:\n",
    "                            logger.info(f\"Keeping top {MAX_RULES_PER_ITEM_TYPE:,} target rules (from {len(rules_target):,})\")\n",
    "                            rules_target = rules_target.head(MAX_RULES_PER_ITEM_TYPE)\n",
    "                        rules_target = rules_target.reset_index(drop=True)\n",
    "                    \n",
    "                    if len(rules_control) > 0:\n",
    "                        rules_control = rules_control.sort_values('lift', ascending=False)\n",
    "                        if len(rules_control) > MAX_RULES_PER_ITEM_TYPE:\n",
    "                            logger.info(f\"Keeping top {MAX_RULES_PER_ITEM_TYPE:,} control rules (from {len(rules_control):,})\")\n",
    "                            rules_control = rules_control.head(MAX_RULES_PER_ITEM_TYPE)\n",
    "                        rules_control = rules_control.reset_index(drop=True)\n",
    "                    \n",
    "                    # Keep target rules as main 'rules' for backward compatibility\n",
    "                    rules = rules_target\n",
    "                else:\n",
    "                    # Not target-focused: all rules are kept\n",
    "                    rules = all_rules.sort_values('lift', ascending=False).head(MAX_RULES_PER_ITEM_TYPE).reset_index(drop=True)\n",
    "                    rules_control = pd.DataFrame()\n",
    "                \n",
    "                logger.info(f\"Final: {len(rules):,} target rules, {len(rules_control):,} control rules\")\n",
    "            else:\n",
    "                logger.info(f\"No rules met confidence threshold of {actual_min_confidence}\")\n",
    "                rules = pd.DataFrame()\n",
    "                rules_control = pd.DataFrame()\n",
    "                \n",
    "            rules_time = time.time() - rules_start\n",
    "            logger.info(f\"‚úì Rule generation completed in {rules_time:.1f}s\")\n",
    "            \n",
    "        except MemoryError as e:\n",
    "            logger.error(f\"MemoryError during rule generation\")\n",
    "            rules = pd.DataFrame()\n",
    "            rules_control = pd.DataFrame()\n",
    "            rules_time = time.time() - rules_start\n",
    "        \n",
    "        # Create encoding map\n",
    "        encoding_map = {}\n",
    "        for idx, row in itemsets.iterrows():\n",
    "            if len(row['itemsets']) == 1:\n",
    "                item = list(row['itemsets'])[0]\n",
    "                encoding_map[item] = {'support': float(row['support']), 'rank': int(idx)}\n",
    "        logger.info(f\"‚úì Created encoding map with {len(encoding_map):,} items\")\n",
    "        \n",
    "        # Save to S3\n",
    "        logger.info(f\"Saving results to S3...\")\n",
    "        s3_client = boto3.client('s3')\n",
    "        prefix = f\"gold/fpgrowth/global/{item_type}\"\n",
    "        \n",
    "        # Convert frozensets to lists\n",
    "        itemsets_json = itemsets.copy()\n",
    "        itemsets_json['itemsets'] = itemsets_json['itemsets'].apply(list)\n",
    "        \n",
    "        # Prepare rules for saving (split target rules by type, plus control)\n",
    "        rules_by_target = {}\n",
    "        \n",
    "        # Process target rules (split by ICD vs ED)\n",
    "        if not rules.empty:\n",
    "            rules_json = rules.copy()\n",
    "            rules_json['antecedents'] = rules_json['antecedents'].apply(list)\n",
    "            rules_json['consequents'] = rules_json['consequents'].apply(list)\n",
    "            \n",
    "            # Split target rules by outcome type\n",
    "            rules_by_target['TARGET_ICD'] = rules_json[\n",
    "                rules_json['consequents'].apply(lambda x: any('TARGET_ICD:' in str(item) for item in x))\n",
    "            ]\n",
    "            rules_by_target['TARGET_ED'] = rules_json[\n",
    "                rules_json['consequents'].apply(lambda x: any('TARGET_ED:' in str(item) for item in x))\n",
    "            ]\n",
    "        \n",
    "        # Process control rules (non-target patterns)\n",
    "        if not rules_control.empty:\n",
    "            rules_control_json = rules_control.copy()\n",
    "            rules_control_json['antecedents'] = rules_control_json['antecedents'].apply(list)\n",
    "            rules_control_json['consequents'] = rules_control_json['consequents'].apply(list)\n",
    "            rules_by_target['CONTROL'] = rules_control_json\n",
    "        \n",
    "        if rules_by_target:\n",
    "            logger.info(f\"Prepared for S3: {len(rules_by_target.get('TARGET_ICD', pd.DataFrame())):,} ICD, \"\n",
    "                       f\"{len(rules_by_target.get('TARGET_ED', pd.DataFrame())):,} ED, \"\n",
    "                       f\"{len(rules_by_target.get('CONTROL', pd.DataFrame())):,} control\")\n",
    "        \n",
    "        # Upload files\n",
    "        s3_client.put_object(Bucket='pgxdatalake', Key=f\"{prefix}/encoding_map.json\", \n",
    "                            Body=json.dumps(encoding_map, indent=2))\n",
    "        s3_client.put_object(Bucket='pgxdatalake', Key=f\"{prefix}/itemsets.json\", \n",
    "                            Body=itemsets_json.to_json(orient='records', indent=2))\n",
    "        \n",
    "        # Save rules by target type (separate files)\n",
    "        if rules_by_target:\n",
    "            for target_type, target_rules in rules_by_target.items():\n",
    "                if not target_rules.empty:\n",
    "                    s3_client.put_object(\n",
    "                        Bucket='pgxdatalake', \n",
    "                        Key=f\"{prefix}/rules_{target_type}.json\",\n",
    "                        Body=target_rules.to_json(orient='records', indent=2)\n",
    "                    )\n",
    "                    logger.info(f\"‚úì Saved {len(target_rules):,} {target_type} rules to S3\")\n",
    "        else:\n",
    "            logger.info(f\"‚úì No rules to save (empty rules)\")\n",
    "        \n",
    "        # Save metrics\n",
    "        metrics = {\n",
    "            'item_type': item_type, \n",
    "            'min_support': actual_min_support, \n",
    "            'min_confidence': actual_min_confidence,\n",
    "            'max_rules_limit': MAX_RULES_PER_ITEM_TYPE,\n",
    "            'rules_truncated': len(rules) == MAX_RULES_PER_ITEM_TYPE if len(rules) > 0 else False,\n",
    "            'unique_items': len(items), \n",
    "            'total_transactions': len(transactions),\n",
    "            'frequent_itemsets': len(itemsets), \n",
    "            'association_rules': len(rules),\n",
    "            'rules_by_target': {\n",
    "                'TARGET_ICD': len(rules_by_target.get('TARGET_ICD', pd.DataFrame())),\n",
    "                'TARGET_ED': len(rules_by_target.get('TARGET_ED', pd.DataFrame())),\n",
    "                'CONTROL': len(rules_by_target.get('CONTROL', pd.DataFrame()))\n",
    "            } if rules_by_target else {'TARGET_ICD': 0, 'TARGET_ED': 0, 'CONTROL': 0},\n",
    "            'encoding_map_size': len(encoding_map),\n",
    "            'target_focused': TARGET_FOCUSED,\n",
    "            'target_icd_codes': TARGET_ICD_CODES if TARGET_FOCUSED else None,\n",
    "            'target_hcg_lines': TARGET_HCG_LINES if TARGET_FOCUSED else None,\n",
    "            'processing_time_seconds': {'total': time.time() - overall_start}\n",
    "        }\n",
    "        s3_client.put_object(Bucket='pgxdatalake', Key=f\"{prefix}/metrics.json\", \n",
    "                            Body=json.dumps(metrics, indent=2))\n",
    "        \n",
    "        logger.info(f\"‚úì {item_type.upper()} COMPLETE - {len(itemsets):,} itemsets, {len(rules):,} rules\")\n",
    "        return metrics\n",
    "    except Exception as e:\n",
    "        logger.error(f\"‚úó Failed: {e}\", exc_info=True)\n",
    "        return {'item_type': item_type, 'error': str(e)}\n",
    "\n",
    "print(\"‚úì process_item_type function defined\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc0d78d1",
   "metadata": {},
   "source": [
    "## 4. Execute Analysis\n",
    "\n",
    "Process all item types sequentially to avoid OOM errors.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb4ef966",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fba0f3c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9beab471",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-23 21:50:40,601 - INFO - \n",
      "================================================================================\n",
      "2025-11-23 21:50:40,602 - INFO - GLOBAL FPGROWTH ANALYSIS - START\n",
      "2025-11-23 21:50:40,602 - INFO - ================================================================================\n",
      "2025-11-23 21:50:40,602 - INFO - Item types: ['drug_name', 'icd_code', 'cpt_code']\n",
      "2025-11-23 21:50:40,603 - INFO - Min support: 0.01\n",
      "2025-11-23 21:50:40,603 - INFO - Min confidence: 0.01\n",
      "2025-11-23 21:50:40,603 - INFO - \n",
      "================================================================================\n",
      "2025-11-23 21:50:40,603 - INFO - Processing DRUG_NAME\n",
      "2025-11-23 21:50:40,604 - INFO - ================================================================================\n",
      "2025-11-23 21:50:40,604 - INFO - Extracting global drug_names from local cohort data...\n",
      "2025-11-23 21:50:40,805 - INFO - ‚úÖ Simple DuckDB connection created - 1 thread per worker (for multiprocessing)\n",
      "2025-11-23 21:50:40,805 - INFO - Running query for drug_name...\n",
      "2025-11-23 21:50:41,862 - INFO - ‚úì Extracted 12,783 unique drug_names in 1.3s\n",
      "2025-11-23 21:50:41,862 - INFO - Creating global drug_name transactions...\n",
      "2025-11-23 21:50:41,897 - INFO - ‚úÖ Simple DuckDB connection created - 1 thread per worker (for multiprocessing)\n",
      "2025-11-23 21:50:41,898 - INFO - Loading drug_name events...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6d815679af24c8da68701b6a29867fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "2025-11-23 21:50:55,007 - INFO - Grouping by patient...\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"GLOBAL FPGROWTH ANALYSIS - START\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Item types: {ITEM_TYPES}\")\n",
    "print(f\"Min support: {MIN_SUPPORT}\")\n",
    "print(f\"Min confidence: {MIN_CONFIDENCE}\")\n",
    "print(f\"Detailed progress ‚Üí Check log file\")\n",
    "print()\n",
    "\n",
    "logger.info(f\"\\n{'='*80}\")\n",
    "logger.info(f\"GLOBAL FPGROWTH ANALYSIS - START\")\n",
    "logger.info(f\"{'='*80}\")\n",
    "logger.info(f\"Item types: {ITEM_TYPES}\")\n",
    "logger.info(f\"Min support: {MIN_SUPPORT}\")\n",
    "logger.info(f\"Min confidence: {MIN_CONFIDENCE}\")\n",
    "\n",
    "# Helper function to check S3 existence\n",
    "def check_s3_results_exist(s3_output_base: str, item_type: str) -> bool:\n",
    "    \"\"\"Check if results already exist in S3 (by checking for metrics.json).\"\"\"\n",
    "    s3 = boto3.client('s3')\n",
    "    key = f\"gold/fpgrowth/global/{item_type}/metrics.json\"\n",
    "    try:\n",
    "        s3.head_object(Bucket='pgxdatalake', Key=key)\n",
    "        return True\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "overall_start = time.time()\n",
    "all_metrics = []\n",
    "skipped = 0\n",
    "\n",
    "# Check which item types need processing (skip if already in S3)\n",
    "items_to_process = []\n",
    "for item_type in ITEM_TYPES:\n",
    "    print(f\"Checking {item_type.upper()}...\")\n",
    "    if check_s3_results_exist(S3_OUTPUT_BASE, item_type):\n",
    "        print(f\"  ‚è≠ Already exists in S3 - SKIPPING\")\n",
    "        logger.info(f\"Skipping {item_type} - results already exist in S3\")\n",
    "        skipped += 1\n",
    "        all_metrics.append({'item_type': item_type, 'status': 'skipped'})\n",
    "    else:\n",
    "        print(f\"  ‚ñ∂ Queued for processing\")\n",
    "        items_to_process.append(item_type)\n",
    "\n",
    "# Process item types SEQUENTIALLY (prevents OOM errors - each job needs ~300-500 GB peak)\n",
    "if items_to_process:\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"SEQUENTIAL PROCESSING: {len(items_to_process)} item types\")\n",
    "    print(f\"Processing one at a time to avoid memory exhaustion\")\n",
    "    print(f\"Expected runtime: 50-85 minutes total\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    # Process each item type sequentially\n",
    "    for idx, item_type in enumerate(items_to_process, 1):\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"Processing {idx}/{len(items_to_process)}: {item_type.upper()}\")\n",
    "        print(f\"{'='*80}\\n\")\n",
    "        \n",
    "        try:\n",
    "            # Use item-specific parameters\n",
    "            actual_min_support = MIN_SUPPORT_CPT if item_type == 'cpt_code' else MIN_SUPPORT\n",
    "            actual_min_confidence = MIN_CONFIDENCE_CPT if item_type == 'cpt_code' else MIN_CONFIDENCE\n",
    "            \n",
    "            metrics = process_item_type(\n",
    "                item_type=item_type,\n",
    "                local_data_path=LOCAL_DATA_PATH,\n",
    "                s3_output_base=S3_OUTPUT_BASE,\n",
    "                min_support=actual_min_support,\n",
    "                min_confidence=actual_min_confidence,\n",
    "                logger=logger\n",
    "            )\n",
    "            all_metrics.append(metrics)\n",
    "            \n",
    "            if 'error' not in metrics:\n",
    "                print(f\"\\n‚úì {item_type.UPPER()} COMPLETE:\")\n",
    "                print(f\"  - Frequent itemsets: {metrics['frequent_itemsets']:,}\")\n",
    "                print(f\"  - Association rules: {metrics['association_rules']:,}\")\n",
    "                print(f\"  - TARGET_ICD rules: {metrics.get('rules_by_target', {}).get('TARGET_ICD', 0):,}\")\n",
    "                print(f\"  - TARGET_ED rules: {metrics.get('rules_by_target', {}).get('TARGET_ED', 0):,}\")\n",
    "                print(f\"  - CONTROL rules: {metrics.get('rules_by_target', {}).get('CONTROL', 0):,}\")\n",
    "                print(f\"  - Runtime: {metrics.get('total_time_seconds', 0):.1f}s\")\n",
    "            else:\n",
    "                print(f\"\\n‚úó {item_type.UPPER()} FAILED: {metrics['error']}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"\\n‚úó {item_type.UPPER()} EXCEPTION: {e}\")\n",
    "            logger.error(f\"Exception processing {item_type}: {e}\", exc_info=True)\n",
    "            all_metrics.append({'item_type': item_type, 'error': str(e)})\n",
    "            \n",
    "        print(f\"\\nCompleted {idx}/{len(items_to_process)} item types\")\n",
    "        print(\"=\"*80)\n",
    "else:\n",
    "    print(\"\\n‚è≠ All item types already exist in S3 - nothing to process\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f928ea05-d090-4bf8-bd7c-6a821ab42386",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "93b5de5c",
   "metadata": {},
   "source": [
    "## 5. Auto-Shutdown EC2 Instance (Optional)\n",
    "\n",
    "Set `SHUTDOWN_EC2 = True` to automatically stop the EC2 instance after analysis completes.\n",
    "\n",
    "**Note:** This is a **STOP** (not terminate), so you can restart the instance later.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a627b303",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# EC2 AUTO-SHUTDOWN (OPTIONAL)\n",
    "# =============================================================================\n",
    "# Set SHUTDOWN_EC2 = True to enable, False to disable\n",
    "SHUTDOWN_EC2 = False  # Change to True to enable auto-shutdown\n",
    "\n",
    "if SHUTDOWN_EC2:\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"Shutting down EC2 instance...\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    import subprocess\n",
    "    import requests\n",
    "    import shutil\n",
    "    \n",
    "    # Get instance ID from EC2 metadata service\n",
    "    try:\n",
    "        response = requests.get(\n",
    "            \"http://169.254.169.254/latest/meta-data/instance-id\",\n",
    "            timeout=2\n",
    "        )\n",
    "        if response.status_code == 200:\n",
    "            instance_id = response.text.strip()\n",
    "            print(f\"Instance ID: {instance_id}\")\n",
    "            \n",
    "            # Find AWS CLI\n",
    "            aws_cmd = shutil.which(\"aws\")\n",
    "            if not aws_cmd:\n",
    "                # Try common paths\n",
    "                for path in [\"/usr/local/bin/aws\", \"/usr/bin/aws\", \n",
    "                           \"/home/ec2-user/.local/bin/aws\", \n",
    "                           \"/home/ubuntu/.local/bin/aws\"]:\n",
    "                    if Path(path).exists():\n",
    "                        aws_cmd = path\n",
    "                        break\n",
    "            \n",
    "            if aws_cmd:\n",
    "                # Stop the instance (use terminate-instances for permanent deletion)\n",
    "                shutdown_cmd = [aws_cmd, \"ec2\", \"stop-instances\", \"--instance-ids\", instance_id]\n",
    "                \n",
    "                print(f\"Running: {' '.join(shutdown_cmd)}\")\n",
    "                result = subprocess.run(shutdown_cmd, capture_output=True, text=True)\n",
    "                \n",
    "                if result.returncode == 0:\n",
    "                    print(\"‚úì EC2 instance stop command sent successfully\")\n",
    "                    print(\"Instance will stop in a few moments.\")\n",
    "                    print(\"Note: This is a STOP (not terminate), so you can restart it later.\")\n",
    "                    if result.stdout:\n",
    "                        print(f\"\\nAWS Response:\\n{result.stdout}\")\n",
    "                else:\n",
    "                    print(f\"‚úó EC2 stop command failed with exit code {result.returncode}\")\n",
    "                    if result.stderr:\n",
    "                        print(f\"Error: {result.stderr}\")\n",
    "                    print(\"Check AWS credentials and IAM permissions.\")\n",
    "            else:\n",
    "                print(\"‚úó AWS CLI not found. Cannot shutdown instance.\")\n",
    "                print(\"Install AWS CLI or ensure it's in your PATH.\")\n",
    "                print(\"Manual shutdown: aws ec2 stop-instances --instance-ids \" + instance_id)\n",
    "        else:\n",
    "            print(f\"‚úó Metadata service returned status code {response.status_code}\")\n",
    "            print(\"Could not retrieve instance ID.\")\n",
    "    \n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(\"‚úó Could not retrieve instance ID from metadata service.\")\n",
    "        print(f\"Error: {e}\")\n",
    "        print(\"If running on EC2, check that metadata service is accessible.\")\n",
    "        print(\"\\nManual shutdown command:\")\n",
    "        print(\"  aws ec2 stop-instances --instance-ids <your-instance-id>\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"‚úó Unexpected error during shutdown: {e}\")\n",
    "\n",
    "else:\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"EC2 Auto-Shutdown: DISABLED\")\n",
    "    print(\"=\"*80)\n",
    "    print(\"To enable auto-shutdown, set SHUTDOWN_EC2 = True in this cell.\")\n",
    "    print(\"Instance will continue running.\")\n",
    "    print(\"\\nTo manually stop this instance later:\")\n",
    "    print(\"  aws ec2 stop-instances --instance-ids $(ec2-metadata --instance-id | cut -d ' ' -f 2)\")\n",
    "    print(\"Or use AWS Console: EC2 > Instances > Select instance > Instance State > Stop\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}