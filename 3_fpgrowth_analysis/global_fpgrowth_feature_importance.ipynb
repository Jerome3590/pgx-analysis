{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f8e7c275",
   "metadata": {},
   "source": [
    "# Global FPGrowth Feature Importance Analysis\n",
    "\n",
    "**Purpose:** Population-level frequent pattern mining for ML feature engineering  \n",
    "**Updated:** November 23, 2025  \n",
    "**Hardware:** Optimized for EC2 (32 cores, 1TB RAM)  \n",
    "**Output:** `s3://pgxdatalake/gold/fpgrowth/global/{item_type}/`\n",
    "\n",
    "## Key Features\n",
    "\n",
    "‚úÖ **Three Item Types** - Drugs, ICD codes, CPT codes  \n",
    "‚úÖ **Global Patterns** - Discovers patterns across all 5.7M patients  \n",
    "‚úÖ **ML Feature Engineering** - Creates encoding maps for CatBoost  \n",
    "‚úÖ **Association Rules** - Identifies co-occurrence relationships  \n",
    "‚úÖ **Memory Optimized** - Configurable support thresholds\n",
    "\n",
    "## Methodology\n",
    "\n",
    "For each item type (drug_name, icd_code, cpt_code):\n",
    "1. Extract all unique items from cohort data\n",
    "2. Create patient-level transactions (lists of items per patient)\n",
    "3. Encode transactions into binary matrix\n",
    "4. Run FP-Growth algorithm to find frequent itemsets\n",
    "5. Generate association rules from frequent itemsets\n",
    "6. Create encoding map for ML models\n",
    "7. Save all outputs to S3\n",
    "\n",
    "## Expected Runtime (EC2: 32 cores, 1TB RAM)\n",
    "\n",
    "- **MIN_SUPPORT=0.01**: ~2-3 hours (12,783 drugs ‚Üí 300-500 itemsets)\n",
    "- **MIN_SUPPORT=0.005**: ~4-6 hours (more itemsets, more rules)\n",
    "- **Total for all 3 types**: ~6-18 hours\n",
    "\n",
    "## Data Scale\n",
    "\n",
    "- **Total Events**: 947 million\n",
    "- **Patients**: 5.7 million\n",
    "- **Unique Drugs**: 12,783\n",
    "- **Unique ICD Codes**: ~10,000\n",
    "- **Unique CPT Codes**: ~5,000\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c526ba4",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "80a6f9a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì All imports successful\n",
      "‚úì Project root: /home/pgx3874/pgx-analysis\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import time\n",
    "import json\n",
    "import logging\n",
    "from pathlib import Path\n",
    "from typing import List, Dict\n",
    "import pandas as pd\n",
    "import boto3\n",
    "from mlxtend.frequent_patterns import fpgrowth, association_rules\n",
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "\n",
    "# Add project root to path\n",
    "project_root = Path.cwd().parent if Path.cwd().name == '3_fpgrowth_analysis' else Path.cwd()\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "from helpers_1997_13.duckdb_utils import get_duckdb_connection\n",
    "\n",
    "print(\"‚úì All imports successful\")\n",
    "print(f\"‚úì Project root: {project_root}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f19b589",
   "metadata": {},
   "source": [
    "## 2. Configuration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20f8f5db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Min Support: 0.01\n",
      "‚úì Min Confidence: 0.01\n",
      "‚úì Item Types: ['drug_name', 'icd_code', 'cpt_code']\n",
      "‚úì S3 Output: s3://pgxdatalake/gold/fpgrowth/global\n",
      "‚úì Local Data: /home/pgx3874/pgx-analysis/data\n",
      "‚úì Local Data Exists: True\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# EC2 CONFIGURATION (32 cores, 1TB RAM)\n",
    "# =============================================================================\n",
    "\n",
    "# FP-Growth parameters (quality-focused for ML features)\n",
    "MIN_SUPPORT = 0.01       # Items must appear in 1% of patients (5.7M patients = 57K occurrences)\n",
    "MIN_CONFIDENCE = 0.4     # 40% confidence - meaningful associations for CatBoost\n",
    "\n",
    "# Item-specific thresholds (balance coverage vs quality)\n",
    "MIN_CONFIDENCE_CPT = 0.5 # 50% confidence for CPT - strong procedure associations\n",
    "MIN_SUPPORT_CPT = 0.02   # 2% support for CPT - focuses on common procedures\n",
    "\n",
    "# Rule limits (quality over quantity)\n",
    "MAX_RULES_PER_ITEM_TYPE = 5000  # Top 5000 rules by lift (for ML feature engineering)\n",
    "\n",
    "# Target-focused rule mining (NEW!)\n",
    "TARGET_FOCUSED = True  # Only generate rules that predict target outcomes\n",
    "TARGET_ICD_CODES = ['F11.20', 'F11.21', 'F11.22', 'F11.23', 'F11.24', 'F11.25', 'F11.29']  # Opioid dependence codes\n",
    "TARGET_HCG_LINES = [\n",
    "    \"P51 - ER Visits and Observation Care\",\n",
    "    \"O11 - Emergency Room\",\n",
    "    \"P33 - Urgent Care Visits\"\n",
    "]  # ED visits (HCG Line codes - matches phase2_event_processing.py)\n",
    "TARGET_PREFIXES = ['TARGET_ICD:', 'TARGET_ED:']  # Prefixes for target items in transactions\n",
    "\n",
    "# Item types to process\n",
    "ITEM_TYPES = ['drug_name', 'icd_code', 'cpt_code']\n",
    "\n",
    "# Paths\n",
    "S3_OUTPUT_BASE = \"s3://pgxdatalake/gold/fpgrowth/global\"\n",
    "LOCAL_DATA_PATH = project_root / \"data\"\n",
    "\n",
    "# Setup logger with file output (prevents Jupyter rate limit issues)\n",
    "logger = logging.getLogger('global_fpgrowth')\n",
    "logger.setLevel(logging.INFO)\n",
    "logger.handlers.clear()  # Clear any existing handlers\n",
    "\n",
    "# File handler - full logs to file\n",
    "log_file = project_root / \"3_fpgrowth_analysis\" / \"global_fpgrowth_execution.log\"\n",
    "file_handler = logging.FileHandler(log_file)\n",
    "file_handler.setLevel(logging.INFO)\n",
    "file_handler.setFormatter(logging.Formatter('%(asctime)s - %(levelname)s - %(message)s'))\n",
    "logger.addHandler(file_handler)\n",
    "\n",
    "# Console handler - only major milestones (prevents Jupyter rate limit)\n",
    "console_handler = logging.StreamHandler()\n",
    "console_handler.setLevel(logging.WARNING)  # Only warnings/errors to console\n",
    "console_handler.setFormatter(logging.Formatter('%(asctime)s - %(levelname)s - %(message)s'))\n",
    "logger.addHandler(console_handler)\n",
    "\n",
    "print(f\"‚úì Min Support (drug/ICD): {MIN_SUPPORT} (1% = ~57K patients)\")\n",
    "print(f\"‚úì Min Support (CPT): {MIN_SUPPORT_CPT} (2% = ~114K patients)\")\n",
    "print(f\"‚úì Min Confidence (drug/ICD): {MIN_CONFIDENCE} (40% - meaningful associations)\")\n",
    "print(f\"‚úì Min Confidence (CPT): {MIN_CONFIDENCE_CPT} (50% - strong procedure patterns)\")\n",
    "print(f\"‚úì Max Rules per Item Type: {MAX_RULES_PER_ITEM_TYPE:,} (top by lift)\")\n",
    "print(f\"‚úì Item Types: {ITEM_TYPES}\")\n",
    "print(f\"‚úì S3 Output: {S3_OUTPUT_BASE}\")\n",
    "print(f\"‚úì Local Data: {LOCAL_DATA_PATH}\")\n",
    "print(f\"‚úì Local Data Exists: {LOCAL_DATA_PATH.exists()}\")\n",
    "print(f\"‚úì Detailed logs ‚Üí {log_file}\")\n",
    "print(f\"‚úì Console output: WARNING level only (check log file for progress)\")\n",
    "print(\"\\nüéØ Quality-Focused Global Analysis:\")\n",
    "print(\"  - 5.7M patients = strong statistical power\")\n",
    "print(\"  - High confidence (40-50%) = meaningful patterns for CatBoost\")\n",
    "print(\"  - Top 5,000 rules per type = comprehensive yet manageable\")\n",
    "print(\"  - Encoding maps for all frequent items (for feature engineering)\")\n",
    "print(f\"\\nüéØ TARGET-FOCUSED RULE MINING: {'ENABLED' if TARGET_FOCUSED else 'DISABLED'}\")\n",
    "if TARGET_FOCUSED:\n",
    "    print(f\"  - Target ICD codes: {TARGET_ICD_CODES}\")\n",
    "    print(f\"  - Target HCG lines (ED visits): {TARGET_HCG_LINES}\")\n",
    "    print(\"  - Only generates rules that PREDICT target outcomes\")\n",
    "    print(\"  - Example: {Metoprolol, Gabapentin} ‚Üí {TARGET_ICD:OPIOID_DEPENDENCE}\")\n",
    "    print(\"  - Example: {99213: Office Visit, J0670: Morphine} ‚Üí {TARGET_ED:EMERGENCY_DEPT}\")\n",
    "    print(\"  ‚úÖ Focus on PREDICTIVE patterns across 5.7M patients\")\n",
    "    print(\"  ‚úÖ Better CatBoost features (what predicts target?)\")\n",
    "    print(\"  ‚úÖ Interpretable global patterns (risk factors)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38d439c9",
   "metadata": {},
   "source": [
    "## 3. Define Helper Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "63384d4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Helper functions defined\n"
     ]
    }
   ],
   "source": [
    "def extract_global_items(local_data_path: Path, item_type: str, logger: logging.Logger) -> List[str]:\n",
    "    \"\"\"Extract all unique items of specified type from local cohort data.\"\"\"\n",
    "    logger.info(f\"Extracting global {item_type}s from local cohort data...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    con = get_duckdb_connection(logger=logger)\n",
    "    parquet_pattern = str(local_data_path / \"**\" / \"cohort.parquet\")\n",
    "    \n",
    "    if item_type == 'drug_name':\n",
    "        query = f\"\"\"\n",
    "        SELECT DISTINCT drug_name as item\n",
    "        FROM read_parquet('{parquet_pattern}', hive_partitioning=1)\n",
    "        WHERE drug_name IS NOT NULL AND drug_name != '' AND event_type = 'pharmacy'\n",
    "        ORDER BY item\n",
    "        \"\"\"\n",
    "    elif item_type == 'icd_code':\n",
    "        query = f\"\"\"\n",
    "        WITH all_icds AS (\n",
    "            SELECT primary_icd_diagnosis_code as icd FROM read_parquet('{parquet_pattern}', hive_partitioning=1) \n",
    "            WHERE primary_icd_diagnosis_code IS NOT NULL AND event_type = 'medical'\n",
    "            UNION ALL\n",
    "            SELECT two_icd_diagnosis_code as icd FROM read_parquet('{parquet_pattern}', hive_partitioning=1) \n",
    "            WHERE two_icd_diagnosis_code IS NOT NULL AND event_type = 'medical'\n",
    "            UNION ALL\n",
    "            SELECT three_icd_diagnosis_code as icd FROM read_parquet('{parquet_pattern}', hive_partitioning=1) \n",
    "            WHERE three_icd_diagnosis_code IS NOT NULL AND event_type = 'medical'\n",
    "            UNION ALL\n",
    "            SELECT four_icd_diagnosis_code as icd FROM read_parquet('{parquet_pattern}', hive_partitioning=1) \n",
    "            WHERE four_icd_diagnosis_code IS NOT NULL AND event_type = 'medical'\n",
    "            UNION ALL\n",
    "            SELECT five_icd_diagnosis_code as icd FROM read_parquet('{parquet_pattern}', hive_partitioning=1) \n",
    "            WHERE five_icd_diagnosis_code IS NOT NULL AND event_type = 'medical'\n",
    "        )\n",
    "        SELECT DISTINCT icd as item FROM all_icds WHERE icd != '' ORDER BY item\n",
    "        \"\"\"\n",
    "    elif item_type == 'cpt_code':\n",
    "        query = f\"\"\"\n",
    "        SELECT DISTINCT procedure_code as item\n",
    "        FROM read_parquet('{parquet_pattern}', hive_partitioning=1)\n",
    "        WHERE procedure_code IS NOT NULL AND procedure_code != '' AND event_type = 'medical'\n",
    "        ORDER BY item\n",
    "        \"\"\"\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown item_type: {item_type}\")\n",
    "    \n",
    "    logger.info(f\"Running query for {item_type}...\")\n",
    "    df = con.execute(query).df()\n",
    "    con.close()\n",
    "    items = df['item'].tolist()\n",
    "    \n",
    "    elapsed = time.time() - start_time\n",
    "    logger.info(f\"‚úì Extracted {len(items):,} unique {item_type}s in {elapsed:.1f}s\")\n",
    "    return items\n",
    "\n",
    "\n",
    "def create_global_transactions(local_data_path: Path, item_type: str, logger: logging.Logger) -> List[List[str]]:\n",
    "    \"\"\"Create patient-level transactions from local cohort data.\"\"\"\n",
    "    logger.info(f\"Creating global {item_type} transactions...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    con = get_duckdb_connection(logger=logger)\n",
    "    parquet_pattern = str(local_data_path / \"**\" / \"cohort.parquet\")\n",
    "    \n",
    "    if item_type == 'drug_name':\n",
    "        query = f\"\"\"\n",
    "        SELECT mi_person_key, drug_name as item\n",
    "        FROM read_parquet('{parquet_pattern}', hive_partitioning=1)\n",
    "        WHERE drug_name IS NOT NULL AND drug_name != '' AND event_type = 'pharmacy'\n",
    "        \"\"\"\n",
    "    elif item_type == 'icd_code':\n",
    "        query = f\"\"\"\n",
    "        WITH all_icds AS (\n",
    "            SELECT mi_person_key, primary_icd_diagnosis_code as icd FROM read_parquet('{parquet_pattern}', hive_partitioning=1) \n",
    "            WHERE primary_icd_diagnosis_code IS NOT NULL AND event_type = 'medical'\n",
    "            UNION ALL\n",
    "            SELECT mi_person_key, two_icd_diagnosis_code as icd FROM read_parquet('{parquet_pattern}', hive_partitioning=1) \n",
    "            WHERE two_icd_diagnosis_code IS NOT NULL AND event_type = 'medical'\n",
    "            UNION ALL\n",
    "            SELECT mi_person_key, three_icd_diagnosis_code as icd FROM read_parquet('{parquet_pattern}', hive_partitioning=1) \n",
    "            WHERE three_icd_diagnosis_code IS NOT NULL AND event_type = 'medical'\n",
    "            UNION ALL\n",
    "            SELECT mi_person_key, four_icd_diagnosis_code as icd FROM read_parquet('{parquet_pattern}', hive_partitioning=1) \n",
    "            WHERE four_icd_diagnosis_code IS NOT NULL AND event_type = 'medical'\n",
    "            UNION ALL\n",
    "            SELECT mi_person_key, five_icd_diagnosis_code as icd FROM read_parquet('{parquet_pattern}', hive_partitioning=1) \n",
    "            WHERE five_icd_diagnosis_code IS NOT NULL AND event_type = 'medical'\n",
    "        )\n",
    "        SELECT mi_person_key, icd as item FROM all_icds WHERE icd != ''\n",
    "        \"\"\"\n",
    "    elif item_type == 'cpt_code':\n",
    "        query = f\"\"\"\n",
    "        SELECT mi_person_key, procedure_code as item\n",
    "        FROM read_parquet('{parquet_pattern}', hive_partitioning=1)\n",
    "        WHERE procedure_code IS NOT NULL AND procedure_code != '' AND event_type = 'medical'\n",
    "        \"\"\"\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown item_type: {item_type}\")\n",
    "    \n",
    "    logger.info(f\"Loading {item_type} events...\")\n",
    "    df = con.execute(query).df()\n",
    "    con.close()\n",
    "    \n",
    "    logger.info(f\"Grouping by patient...\")\n",
    "    transactions = (\n",
    "        df.groupby('mi_person_key')['item']\n",
    "        .apply(lambda x: sorted(set(x.tolist())))\n",
    "        .tolist()\n",
    "    )\n",
    "    \n",
    "    elapsed = time.time() - start_time\n",
    "    logger.info(f\"‚úì Created {len(transactions):,} patient transactions in {elapsed:.1f}s\")\n",
    "    return transactions\n",
    "\n",
    "print(\"‚úì Helper functions defined\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccc1346a",
   "metadata": {},
   "source": [
    "## 4. FP-Growth Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5415ef4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì process_item_type function defined\n"
     ]
    }
   ],
   "source": [
    "def process_item_type(item_type, local_data_path, s3_output_base, min_support, min_confidence, logger):\n",
    "    \"\"\"Process a single item type: extract, encode, FP-Growth, save to S3.\"\"\"\n",
    "    logger.info(f\"\\n{'='*80}\")\n",
    "    logger.info(f\"Processing {item_type.upper()}\")\n",
    "    logger.info(f\"{'='*80}\")\n",
    "    overall_start = time.time()\n",
    "    \n",
    "    try:\n",
    "        # Extract items\n",
    "        items = extract_global_items(local_data_path, item_type, logger)\n",
    "        \n",
    "        # Create transactions\n",
    "        transactions = create_global_transactions(local_data_path, item_type, logger)\n",
    "        \n",
    "        # Encode transactions\n",
    "        logger.info(f\"Encoding {len(transactions):,} transactions...\")\n",
    "        encode_start = time.time()\n",
    "        te = TransactionEncoder()\n",
    "        te_ary = te.fit(transactions).transform(transactions)\n",
    "        df_encoded = pd.DataFrame(te_ary, columns=te.columns_)\n",
    "        encode_time = time.time() - encode_start\n",
    "        logger.info(f\"‚úì Encoded to {df_encoded.shape} matrix in {encode_time:.1f}s\")\n",
    "        \n",
    "        # Run FP-Growth (use item-specific parameters)\n",
    "        actual_min_support = MIN_SUPPORT_CPT if item_type == 'cpt_code' else min_support\n",
    "        logger.info(f\"Running FP-Growth (min_support={actual_min_support})...\")\n",
    "        fpgrowth_start = time.time()\n",
    "        itemsets = fpgrowth(df_encoded, min_support=actual_min_support, use_colnames=True)\n",
    "        itemsets = itemsets.sort_values('support', ascending=False).reset_index(drop=True)\n",
    "        fpgrowth_time = time.time() - fpgrowth_start\n",
    "        logger.info(f\"‚úì Found {len(itemsets):,} frequent itemsets in {fpgrowth_time:.1f}s\")\n",
    "        \n",
    "        # Generate association rules (with item-specific confidence and limits)\n",
    "        actual_min_confidence = MIN_CONFIDENCE_CPT if item_type == 'cpt_code' else min_confidence\n",
    "        logger.info(f\"Generating association rules (min_confidence={actual_min_confidence})...\")\n",
    "        rules_start = time.time()\n",
    "        \n",
    "        try:\n",
    "            rules = association_rules(itemsets, metric=\"confidence\", min_threshold=actual_min_confidence)\n",
    "            \n",
    "            if len(rules) > 0:\n",
    "                # Filter for target-focused rules (only rules predicting targets)\n",
    "                if TARGET_FOCUSED:\n",
    "                    pre_filter_count = len(rules)\n",
    "                    # Keep only rules where consequent contains a target item\n",
    "                    rules = rules[rules['consequents'].apply(\n",
    "                        lambda x: any(item.startswith(tuple(TARGET_PREFIXES)) for item in x)\n",
    "                    )]\n",
    "                    logger.info(f\"Target filter: {pre_filter_count:,} ‚Üí {len(rules):,} rules (predicting target outcomes)\")\n",
    "                \n",
    "                if len(rules) > 0:\n",
    "                    # Sort by lift and limit to top N\n",
    "                    rules = rules.sort_values('lift', ascending=False)\n",
    "                    \n",
    "                    if len(rules) > MAX_RULES_PER_ITEM_TYPE:\n",
    "                        logger.info(f\"Keeping top {MAX_RULES_PER_ITEM_TYPE:,} rules by lift (from {len(rules):,})\")\n",
    "                        rules = rules.head(MAX_RULES_PER_ITEM_TYPE)\n",
    "                    else:\n",
    "                        logger.info(f\"Keeping all {len(rules):,} target-focused rules\")\n",
    "                    \n",
    "                    rules = rules.reset_index(drop=True)\n",
    "                else:\n",
    "                    logger.info(f\"No rules predict target outcomes\")\n",
    "            else:\n",
    "                logger.info(f\"No rules met confidence threshold of {actual_min_confidence}\")\n",
    "                \n",
    "            rules_time = time.time() - rules_start\n",
    "            logger.info(f\"‚úì Rule generation completed in {rules_time:.1f}s\")\n",
    "            \n",
    "        except MemoryError as e:\n",
    "            logger.error(f\"MemoryError during rule generation\")\n",
    "            rules = pd.DataFrame()\n",
    "            rules_time = time.time() - rules_start\n",
    "        \n",
    "        # Create encoding map\n",
    "        encoding_map = {}\n",
    "        for idx, row in itemsets.iterrows():\n",
    "            if len(row['itemsets']) == 1:\n",
    "                item = list(row['itemsets'])[0]\n",
    "                encoding_map[item] = {'support': float(row['support']), 'rank': int(idx)}\n",
    "        logger.info(f\"‚úì Created encoding map with {len(encoding_map):,} items\")\n",
    "        \n",
    "        # Save to S3\n",
    "        logger.info(f\"Saving results to S3...\")\n",
    "        s3_client = boto3.client('s3')\n",
    "        prefix = f\"gold/fpgrowth/global/{item_type}\"\n",
    "        \n",
    "        # Convert frozensets to lists\n",
    "        itemsets_json = itemsets.copy()\n",
    "        itemsets_json['itemsets'] = itemsets_json['itemsets'].apply(list)\n",
    "        \n",
    "        # Split rules by target type for separate files\n",
    "        rules_by_target = {}\n",
    "        if not rules.empty:\n",
    "            rules_json = rules.copy()\n",
    "            rules_json['antecedents'] = rules_json['antecedents'].apply(list)\n",
    "            rules_json['consequents'] = rules_json['consequents'].apply(list)\n",
    "            \n",
    "            # Separate rules by target type\n",
    "            rules_by_target['TARGET_ICD'] = rules_json[\n",
    "                rules_json['consequents'].apply(lambda x: any('TARGET_ICD:' in str(item) for item in x))\n",
    "            ]\n",
    "            rules_by_target['TARGET_ED'] = rules_json[\n",
    "                rules_json['consequents'].apply(lambda x: any('TARGET_ED:' in str(item) for item in x))\n",
    "            ]\n",
    "            rules_by_target['ALL'] = rules_json  # Keep all rules together too\n",
    "            \n",
    "            logger.info(f\"Split rules: {len(rules_by_target['TARGET_ICD'])} ICD, {len(rules_by_target['TARGET_ED'])} ED\")\n",
    "        \n",
    "        # Upload files\n",
    "        s3_client.put_object(Bucket='pgxdatalake', Key=f\"{prefix}/encoding_map.json\", \n",
    "                            Body=json.dumps(encoding_map, indent=2))\n",
    "        s3_client.put_object(Bucket='pgxdatalake', Key=f\"{prefix}/itemsets.json\", \n",
    "                            Body=itemsets_json.to_json(orient='records', indent=2))\n",
    "        \n",
    "        # Save rules by target type (separate files)\n",
    "        if rules_by_target:\n",
    "            for target_type, target_rules in rules_by_target.items():\n",
    "                if not target_rules.empty:\n",
    "                    s3_client.put_object(\n",
    "                        Bucket='pgxdatalake', \n",
    "                        Key=f\"{prefix}/rules_{target_type}.json\",\n",
    "                        Body=target_rules.to_json(orient='records', indent=2)\n",
    "                    )\n",
    "                    logger.info(f\"‚úì Saved {len(target_rules):,} {target_type} rules to S3\")\n",
    "        else:\n",
    "            logger.info(f\"‚úì No rules to save (empty rules)\")\n",
    "        \n",
    "        # Save metrics\n",
    "        metrics = {\n",
    "            'item_type': item_type, \n",
    "            'min_support': actual_min_support, \n",
    "            'min_confidence': actual_min_confidence,\n",
    "            'max_rules_limit': MAX_RULES_PER_ITEM_TYPE,\n",
    "            'rules_truncated': len(rules) == MAX_RULES_PER_ITEM_TYPE if len(rules) > 0 else False,\n",
    "            'unique_items': len(items), \n",
    "            'total_transactions': len(transactions),\n",
    "            'frequent_itemsets': len(itemsets), \n",
    "            'association_rules': len(rules),\n",
    "            'rules_by_target': {\n",
    "                'TARGET_ICD': len(rules_by_target.get('TARGET_ICD', pd.DataFrame())),\n",
    "                'TARGET_ED': len(rules_by_target.get('TARGET_ED', pd.DataFrame()))\n",
    "            } if rules_by_target else {'TARGET_ICD': 0, 'TARGET_ED': 0},\n",
    "            'encoding_map_size': len(encoding_map),\n",
    "            'target_focused': TARGET_FOCUSED,\n",
    "            'target_icd_codes': TARGET_ICD_CODES if TARGET_FOCUSED else None,\n",
    "            'target_hcg_lines': TARGET_HCG_LINES if TARGET_FOCUSED else None,\n",
    "            'processing_time_seconds': {'total': time.time() - overall_start}\n",
    "        }\n",
    "        s3_client.put_object(Bucket='pgxdatalake', Key=f\"{prefix}/metrics.json\", \n",
    "                            Body=json.dumps(metrics, indent=2))\n",
    "        \n",
    "        logger.info(f\"‚úì {item_type.upper()} COMPLETE - {len(itemsets):,} itemsets, {len(rules):,} rules\")\n",
    "        return metrics\n",
    "    except Exception as e:\n",
    "        logger.error(f\"‚úó Failed: {e}\", exc_info=True)\n",
    "        return {'item_type': item_type, 'error': str(e)}\n",
    "\n",
    "print(\"‚úì process_item_type function defined\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc0d78d1",
   "metadata": {},
   "source": [
    "## 5. Execute Analysis\n",
    "\n",
    "Process all item types sequentially.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9beab471",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-23 21:50:40,601 - INFO - \n",
      "================================================================================\n",
      "2025-11-23 21:50:40,602 - INFO - GLOBAL FPGROWTH ANALYSIS - START\n",
      "2025-11-23 21:50:40,602 - INFO - ================================================================================\n",
      "2025-11-23 21:50:40,602 - INFO - Item types: ['drug_name', 'icd_code', 'cpt_code']\n",
      "2025-11-23 21:50:40,603 - INFO - Min support: 0.01\n",
      "2025-11-23 21:50:40,603 - INFO - Min confidence: 0.01\n",
      "2025-11-23 21:50:40,603 - INFO - \n",
      "================================================================================\n",
      "2025-11-23 21:50:40,603 - INFO - Processing DRUG_NAME\n",
      "2025-11-23 21:50:40,604 - INFO - ================================================================================\n",
      "2025-11-23 21:50:40,604 - INFO - Extracting global drug_names from local cohort data...\n",
      "2025-11-23 21:50:40,805 - INFO - ‚úÖ Simple DuckDB connection created - 1 thread per worker (for multiprocessing)\n",
      "2025-11-23 21:50:40,805 - INFO - Running query for drug_name...\n",
      "2025-11-23 21:50:41,862 - INFO - ‚úì Extracted 12,783 unique drug_names in 1.3s\n",
      "2025-11-23 21:50:41,862 - INFO - Creating global drug_name transactions...\n",
      "2025-11-23 21:50:41,897 - INFO - ‚úÖ Simple DuckDB connection created - 1 thread per worker (for multiprocessing)\n",
      "2025-11-23 21:50:41,898 - INFO - Loading drug_name events...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6d815679af24c8da68701b6a29867fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "2025-11-23 21:50:55,007 - INFO - Grouping by patient...\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"GLOBAL FPGROWTH ANALYSIS - START\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Item types: {ITEM_TYPES}\")\n",
    "print(f\"Min support: {MIN_SUPPORT}\")\n",
    "print(f\"Min confidence: {MIN_CONFIDENCE}\")\n",
    "print(f\"Detailed progress ‚Üí Check log file\")\n",
    "print()\n",
    "\n",
    "logger.info(f\"\\n{'='*80}\")\n",
    "logger.info(f\"GLOBAL FPGROWTH ANALYSIS - START\")\n",
    "logger.info(f\"{'='*80}\")\n",
    "logger.info(f\"Item types: {ITEM_TYPES}\")\n",
    "logger.info(f\"Min support: {MIN_SUPPORT}\")\n",
    "logger.info(f\"Min confidence: {MIN_CONFIDENCE}\")\n",
    "\n",
    "# Helper function to check S3 existence\n",
    "def check_s3_results_exist(s3_output_base: str, item_type: str) -> bool:\n",
    "    \"\"\"Check if results already exist in S3 (by checking for metrics.json).\"\"\"\n",
    "    s3 = boto3.client('s3')\n",
    "    key = f\"gold/fpgrowth/global/{item_type}/metrics.json\"\n",
    "    try:\n",
    "        s3.head_object(Bucket='pgxdatalake', Key=key)\n",
    "        return True\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "overall_start = time.time()\n",
    "all_metrics = []\n",
    "skipped = 0\n",
    "\n",
    "for idx, item_type in enumerate(ITEM_TYPES, 1):\n",
    "    print(f\"\\n[{idx}/{len(ITEM_TYPES)}] Checking {item_type.upper()}...\")\n",
    "    \n",
    "    # Check if already completed\n",
    "    if check_s3_results_exist(S3_OUTPUT_BASE, item_type):\n",
    "        print(f\"‚è≠ {item_type.upper()} already exists in S3 - SKIPPING\")\n",
    "        logger.info(f\"Skipping {item_type} - results already exist in S3\")\n",
    "        skipped += 1\n",
    "        all_metrics.append({'item_type': item_type, 'status': 'skipped'})\n",
    "        continue\n",
    "    \n",
    "    print(f\"‚ñ∂ Processing {item_type.upper()}...\")\n",
    "    item_start = time.time()\n",
    "    \n",
    "    metrics = process_item_type(\n",
    "        item_type=item_type,\n",
    "        local_data_path=LOCAL_DATA_PATH,\n",
    "        s3_output_base=S3_OUTPUT_BASE,\n",
    "        min_support=MIN_SUPPORT,\n",
    "        min_confidence=MIN_CONFIDENCE,\n",
    "        logger=logger\n",
    "    )\n",
    "    all_metrics.append(metrics)\n",
    "    \n",
    "    item_elapsed = time.time() - item_start\n",
    "    if 'error' not in metrics:\n",
    "        print(f\"‚úì {item_type.upper()} complete: {metrics['frequent_itemsets']:,} itemsets, \"\n",
    "              f\"{metrics['association_rules']:,} rules ({item_elapsed/60:.1f}min)\")\n",
    "    else:\n",
    "        print(f\"‚úó {item_type.upper()} failed: {metrics['error']}\")\n",
    "\n",
    "total_elapsed = time.time() - overall_start\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"GLOBAL FPGROWTH ANALYSIS - COMPLETE\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Total time: {total_elapsed:.1f}s ({total_elapsed/60:.1f}min)\")\n",
    "print(f\"Processed: {len([m for m in all_metrics if m.get('status') != 'skipped'])}\")\n",
    "print(f\"Skipped: {skipped}\")\n",
    "print(\"\\nResults Summary:\")\n",
    "for m in all_metrics:\n",
    "    if m.get('status') == 'skipped':\n",
    "        print(f\"  ‚è≠ {m['item_type']}: SKIPPED (already in S3)\")\n",
    "    elif 'error' not in m:\n",
    "        print(f\"  ‚úì {m['item_type']}: {m['frequent_itemsets']:,} itemsets, {m['association_rules']:,} rules\")\n",
    "    else:\n",
    "        print(f\"  ‚úó {m['item_type']}: ERROR - {m['error']}\")\n",
    "\n",
    "logger.info(f\"\\n{'='*80}\")\n",
    "logger.info(f\"GLOBAL FPGROWTH ANALYSIS - COMPLETE\")\n",
    "logger.info(f\"{'='*80}\")\n",
    "logger.info(f\"Total processing time: {total_elapsed:.1f}s ({total_elapsed/60:.1f}min)\")\n",
    "logger.info(f\"\\nResults Summary:\")\n",
    "for m in all_metrics:\n",
    "    if 'error' not in m:\n",
    "        logger.info(f\"  {m['item_type']}: {m['frequent_itemsets']:,} itemsets, {m['association_rules']:,} rules\")\n",
    "    else:\n",
    "        logger.info(f\"  {m['item_type']}: ERROR - {m['error']}\")\n",
    "\n",
    "print(\"\\n‚úì Analysis complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f928ea05-d090-4bf8-bd7c-6a821ab42386",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
