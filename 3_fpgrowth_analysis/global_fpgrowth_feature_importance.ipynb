{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Global FPGrowth Feature Importance Analysis\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook performs **global FPGrowth analysis** across all cohorts to create universal drug encoding features for machine learning models. The results are used for:\n",
    "\n",
    "1. **CatBoost Feature Engineering**: Creates consistent drug encodings across training/validation/test sets\n",
    "2. **Population-Level Insights**: Discovers drug association patterns across all patients\n",
    "3. **Feature Importance**: Identifies which drug patterns are most frequent in the population\n",
    "\n",
    "## Key Outputs\n",
    "\n",
    "- **Global Drug Encoding Map**: Universal drug encodings for ML (`gold/fpgrowth/global/drug_encoding_map.json`)\n",
    "- **Frequent Itemsets**: Drug combinations that appear frequently (`gold/fpgrowth/global/global_itemsets.json`)\n",
    "- **Association Rules**: Drug co-prescription patterns (`gold/fpgrowth/global/global_rules.json`)\n",
    "- **Network Visualization**: Interactive drug association network (`gold/fpgrowth/global/drug_network.html`)\n",
    "\n",
    "## Parameters\n",
    "\n",
    "- **Min Support**: 0.005 (drugs must appear in 0.5% of transactions)\n",
    "- **Min Confidence**: 0.01 (rules must have 1% confidence)\n",
    "- **Data Source**: Local cohort data from `data/gold/cohorts_F1120/`\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import logging\n",
    "from pathlib import Path\n",
    "import time\n",
    "\n",
    "# MLxtend for FP-Growth\n",
    "from mlxtend.frequent_patterns import fpgrowth, association_rules\n",
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "\n",
    "# Project root\n",
    "project_root = Path.cwd().parent if Path.cwd().name == '3_fpgrowth_analysis' else Path.cwd()\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "# Project utilities\n",
    "from helpers_1997_13.common_imports import s3_client, S3_BUCKET\n",
    "from helpers_1997_13.duckdb_utils import get_duckdb_connection\n",
    "from helpers_1997_13.s3_utils import save_to_s3_json, save_to_s3_parquet\n",
    "from helpers_1997_13.drug_utils import encode_drug_name\n",
    "from helpers_1997_13.visualization_utils import create_network_visualization\n",
    "\n",
    "print(f\"‚úì Project root: {project_root}\")\n",
    "print(f\"‚úì All imports successful\")\n",
    "print(f\"‚úì Timestamp: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FP-Growth parameters\n",
    "MIN_SUPPORT = 0.005  # 0.5% support threshold\n",
    "MIN_CONFIDENCE = 0.01  # 1% confidence threshold\n",
    "TOP_K = 50  # Top K itemsets to analyze\n",
    "\n",
    "# S3 output paths\n",
    "S3_OUTPUT_BASE = f\"s3://{S3_BUCKET}/gold/fpgrowth/global\"\n",
    "\n",
    "# Local data path\n",
    "LOCAL_DATA_PATH = project_root / \"data\" / \"gold\" / \"cohorts_F1120\"\n",
    "\n",
    "# Create logger\n",
    "logger = logging.getLogger('global_fpgrowth')\n",
    "logger.setLevel(logging.INFO)\n",
    "if not logger.handlers:\n",
    "    handler = logging.StreamHandler()\n",
    "    formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
    "    handler.setFormatter(formatter)\n",
    "    logger.addHandler(handler)\n",
    "\n",
    "print(f\"‚úì Min Support: {MIN_SUPPORT}\")\n",
    "print(f\"‚úì Min Confidence: {MIN_CONFIDENCE}\")\n",
    "print(f\"‚úì S3 Output: {S3_OUTPUT_BASE}\")\n",
    "print(f\"‚úì Local Data: {LOCAL_DATA_PATH}\")\n",
    "print(f\"‚úì Local Data Exists: {LOCAL_DATA_PATH.exists()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Extract All Drug Names from Cohorts\n",
    "\n",
    "Load all cohort data and extract unique drug names to create the global drug universe.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_global_drug_names(local_data_path, logger):\n",
    "    \"\"\"\n",
    "    Extract all unique drug names from local cohort data.\n",
    "    \"\"\"\n",
    "    logger.info(\"Extracting global drug names from local cohort data...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Get DuckDB connection\n",
    "    con = get_duckdb_connection(logger=logger)\n",
    "    \n",
    "    # Build glob pattern for all parquet files\n",
    "    parquet_pattern = str(local_data_path / \"**\" / \"cohort.parquet\")\n",
    "    \n",
    "    query = f\"\"\"\n",
    "    SELECT DISTINCT drug_name\n",
    "    FROM read_parquet('{parquet_pattern}', hive_partitioning=1)\n",
    "    WHERE drug_name IS NOT NULL \n",
    "      AND drug_name != ''\n",
    "      AND event_type = 'PHARMACY'\n",
    "    ORDER BY drug_name\n",
    "    \"\"\"\n",
    "    \n",
    "    logger.info(f\"Running query...\")\n",
    "    df = con.execute(query).df()\n",
    "    con.close()\n",
    "    \n",
    "    drug_names = df['drug_name'].tolist()\n",
    "    \n",
    "    elapsed = time.time() - start_time\n",
    "    logger.info(f\"‚úì Extracted {len(drug_names):,} unique drug names in {elapsed:.1f}s\")\n",
    "    \n",
    "    return drug_names\n",
    "\n",
    "# Execute extraction\n",
    "global_drug_names = extract_global_drug_names(LOCAL_DATA_PATH, logger)\n",
    "\n",
    "print(f\"\\nüìä Global Drug Statistics:\")\n",
    "print(f\"  Total unique drugs: {len(global_drug_names):,}\")\n",
    "print(f\"\\n  Sample drugs (first 10):\")\n",
    "for i, drug in enumerate(global_drug_names[:10], 1):\n",
    "    print(f\"    {i}. {drug}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Create Global Drug Transactions\n",
    "\n",
    "Create patient-level drug transactions for FP-Growth algorithm.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_global_drug_transactions(local_data_path, logger):\n",
    "    \"\"\"\n",
    "    Create patient-level drug transactions from local cohort data.\n",
    "    \"\"\"\n",
    "    logger.info(\"Creating global drug transactions...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Get DuckDB connection\n",
    "    con = get_duckdb_connection(logger=logger)\n",
    "    \n",
    "    # Build glob pattern for all parquet files\n",
    "    parquet_pattern = str(local_data_path / \"**\" / \"cohort.parquet\")\n",
    "    \n",
    "    query = f\"\"\"\n",
    "    SELECT \n",
    "        mi_person_key,\n",
    "        drug_name\n",
    "    FROM read_parquet('{parquet_pattern}', hive_partitioning=1)\n",
    "    WHERE drug_name IS NOT NULL \n",
    "      AND drug_name != ''\n",
    "      AND event_type = 'PHARMACY'\n",
    "    \"\"\"\n",
    "    \n",
    "    logger.info(f\"Loading pharmacy events...\")\n",
    "    df = con.execute(query).df()\n",
    "    con.close()\n",
    "    \n",
    "    # Group by patient and create drug lists\n",
    "    logger.info(f\"Grouping by patient...\")\n",
    "    transactions = (\n",
    "        df.groupby('mi_person_key')['drug_name']\n",
    "        .apply(lambda x: sorted(set(x.tolist())))\n",
    "        .tolist()\n",
    "    )\n",
    "    \n",
    "    elapsed = time.time() - start_time\n",
    "    logger.info(f\"‚úì Created {len(transactions):,} patient transactions in {elapsed:.1f}s\")\n",
    "    \n",
    "    return transactions\n",
    "\n",
    "# Execute transaction creation\n",
    "transactions = create_global_drug_transactions(LOCAL_DATA_PATH, logger)\n",
    "\n",
    "print(f\"\\nüìä Transaction Statistics:\")\n",
    "print(f\"  Total patients: {len(transactions):,}\")\n",
    "print(f\"  Avg drugs per patient: {np.mean([len(t) for t in transactions]):.1f}\")\n",
    "print(f\"  Median drugs per patient: {np.median([len(t) for t in transactions]):.0f}\")\n",
    "print(f\"  Max drugs per patient: {max([len(t) for t in transactions])}\")\n",
    "print(f\"\\n  Sample transaction (first patient):\")\n",
    "print(f\"    Patient has {len(transactions[0])} drugs: {transactions[0][:5]}...\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Encode Transactions for FP-Growth\n",
    "\n",
    "Use TransactionEncoder to convert transactions into one-hot encoded format.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(\"Encoding transactions...\")\n",
    "start_time = time.time()\n",
    "\n",
    "# Create TransactionEncoder\n",
    "te = TransactionEncoder()\n",
    "te_ary = te.fit(transactions).transform(transactions)\n",
    "df_encoded = pd.DataFrame(te_ary, columns=te.columns_)\n",
    "\n",
    "elapsed = time.time() - start_time\n",
    "logger.info(f\"‚úì Encoded transactions in {elapsed:.1f}s\")\n",
    "\n",
    "print(f\"\\nüìä Encoded Transaction Matrix:\")\n",
    "print(f\"  Shape: {df_encoded.shape} (patients √ó drugs)\")\n",
    "print(f\"  Memory: {df_encoded.memory_usage(deep=True).sum() / 1024**2:.1f} MB\")\n",
    "print(f\"\\n  Sample (first 5 patients √ó first 5 drugs):\")\n",
    "print(df_encoded.iloc[:5, :5])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Run FP-Growth Algorithm\n",
    "\n",
    "Apply FP-Growth to discover frequent drug itemsets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(f\"Running FP-Growth with min_support={MIN_SUPPORT}...\")\n",
    "start_time = time.time()\n",
    "\n",
    "# Run FP-Growth\n",
    "frequent_itemsets = fpgrowth(df_encoded, min_support=MIN_SUPPORT, use_colnames=True)\n",
    "\n",
    "# Sort by support\n",
    "frequent_itemsets = frequent_itemsets.sort_values('support', ascending=False).reset_index(drop=True)\n",
    "\n",
    "elapsed = time.time() - start_time\n",
    "logger.info(f\"‚úì Found {len(frequent_itemsets):,} frequent itemsets in {elapsed:.1f}s\")\n",
    "\n",
    "print(f\"\\nüìä Frequent Itemsets:\")\n",
    "print(f\"  Total itemsets: {len(frequent_itemsets):,}\")\n",
    "print(f\"  Size 1 (single drugs): {(frequent_itemsets['itemsets'].apply(len) == 1).sum():,}\")\n",
    "print(f\"  Size 2 (pairs): {(frequent_itemsets['itemsets'].apply(len) == 2).sum():,}\")\n",
    "print(f\"  Size 3+: {(frequent_itemsets['itemsets'].apply(len) >= 3).sum():,}\")\n",
    "print(f\"\\n  Top 10 frequent itemsets:\")\n",
    "print(frequent_itemsets.head(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Generate Association Rules\n",
    "\n",
    "Create association rules from frequent itemsets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(f\"Generating association rules with min_confidence={MIN_CONFIDENCE}...\")\n",
    "start_time = time.time()\n",
    "\n",
    "# Generate rules\n",
    "try:\n",
    "    rules = association_rules(frequent_itemsets, metric=\"confidence\", min_threshold=MIN_CONFIDENCE)\n",
    "    rules = rules.sort_values('lift', ascending=False).reset_index(drop=True)\n",
    "    \n",
    "    elapsed = time.time() - start_time\n",
    "    logger.info(f\"‚úì Generated {len(rules):,} association rules in {elapsed:.1f}s\")\n",
    "    \n",
    "    print(f\"\\nüìä Association Rules:\")\n",
    "    print(f\"  Total rules: {len(rules):,}\")\n",
    "    print(f\"  Avg confidence: {rules['confidence'].mean():.3f}\")\n",
    "    print(f\"  Avg lift: {rules['lift'].mean():.2f}\")\n",
    "    print(f\"\\n  Top 10 rules by lift:\")\n",
    "    print(rules[['antecedents', 'consequents', 'support', 'confidence', 'lift']].head(10))\n",
    "    \n",
    "except ValueError as e:\n",
    "    logger.warning(f\"Could not generate rules: {e}\")\n",
    "    rules = pd.DataFrame()\n",
    "    print(f\"\\n‚ö†Ô∏è No association rules generated (itemsets may be too sparse)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Create Global Drug Encoding Map\n",
    "\n",
    "Generate universal drug encodings with FP-Growth metrics for ML features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_global_encoding_map(drug_names, itemsets_df, rules_df, logger):\n",
    "    \"\"\"\n",
    "    Create global drug encoding map with FP-Growth metrics.\n",
    "    \"\"\"\n",
    "    logger.info(\"Creating global drug encoding map...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    encoding_map = {}\n",
    "    \n",
    "    for drug in drug_names:\n",
    "        # Get support from itemsets\n",
    "        support = 0.0\n",
    "        matching_itemsets = itemsets_df[itemsets_df['itemsets'].apply(lambda x: drug in x)]\n",
    "        if not matching_itemsets.empty:\n",
    "            support = matching_itemsets['support'].max()\n",
    "        \n",
    "        # Get confidence from rules\n",
    "        confidence = 0.0\n",
    "        if not rules_df.empty:\n",
    "            matching_rules = rules_df[\n",
    "                rules_df['antecedents'].apply(lambda x: drug in x) |\n",
    "                rules_df['consequents'].apply(lambda x: drug in x)\n",
    "            ]\n",
    "            if not matching_rules.empty:\n",
    "                confidence = matching_rules['confidence'].max()\n",
    "        \n",
    "        # Create encoding\n",
    "        encoding = encode_drug_name(drug, support=support, confidence=confidence)\n",
    "        encoding_map[drug] = encoding\n",
    "    \n",
    "    elapsed = time.time() - start_time\n",
    "    logger.info(f\"‚úì Created encoding map for {len(encoding_map):,} drugs in {elapsed:.1f}s\")\n",
    "    \n",
    "    return encoding_map\n",
    "\n",
    "# Create encoding map\n",
    "encoding_map = create_global_encoding_map(global_drug_names, frequent_itemsets, rules, logger)\n",
    "\n",
    "print(f\"\\nüìä Global Drug Encoding Map:\")\n",
    "print(f\"  Total drugs encoded: {len(encoding_map):,}\")\n",
    "print(f\"\\n  Sample encodings (first 10 drugs):\")\n",
    "for i, (drug, encoding) in enumerate(list(encoding_map.items())[:10], 1):\n",
    "    print(f\"    {i}. {drug[:30]:30s} ‚Üí {encoding}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Save Results to S3\n",
    "\n",
    "Upload all results to S3 for downstream analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(\"Saving results to S3...\")\n",
    "start_time = time.time()\n",
    "\n",
    "# Convert frozensets to lists for JSON serialization\n",
    "itemsets_json = frequent_itemsets.copy()\n",
    "itemsets_json['itemsets'] = itemsets_json['itemsets'].apply(lambda x: list(x))\n",
    "\n",
    "if not rules.empty:\n",
    "    rules_json = rules.copy()\n",
    "    rules_json['antecedents'] = rules_json['antecedents'].apply(lambda x: list(x))\n",
    "    rules_json['consequents'] = rules_json['consequents'].apply(lambda x: list(x))\n",
    "else:\n",
    "    rules_json = pd.DataFrame()\n",
    "\n",
    "# Save to S3\n",
    "results_saved = {}\n",
    "\n",
    "try:\n",
    "    # 1. Drug encoding map\n",
    "    encoding_path = f\"{S3_OUTPUT_BASE}/drug_encoding_map.json\"\n",
    "    save_to_s3_json(encoding_map, encoding_path)\n",
    "    results_saved['encoding_map'] = encoding_path\n",
    "    logger.info(f\"‚úì Saved encoding map to {encoding_path}\")\n",
    "    \n",
    "    # 2. Frequent itemsets\n",
    "    itemsets_path = f\"{S3_OUTPUT_BASE}/global_itemsets.json\"\n",
    "    save_to_s3_json(itemsets_json.to_dict(orient='records'), itemsets_path)\n",
    "    results_saved['itemsets'] = itemsets_path\n",
    "    logger.info(f\"‚úì Saved itemsets to {itemsets_path}\")\n",
    "    \n",
    "    # 3. Association rules\n",
    "    if not rules_json.empty:\n",
    "        rules_path = f\"{S3_OUTPUT_BASE}/global_rules.json\"\n",
    "        save_to_s3_json(rules_json.to_dict(orient='records'), rules_path)\n",
    "        results_saved['rules'] = rules_path\n",
    "        logger.info(f\"‚úì Saved rules to {rules_path}\")\n",
    "    \n",
    "    # 4. Summary metrics\n",
    "    summary = {\n",
    "        'timestamp': datetime.now().isoformat(),\n",
    "        'total_drugs': len(global_drug_names),\n",
    "        'total_patients': len(transactions),\n",
    "        'total_itemsets': len(frequent_itemsets),\n",
    "        'total_rules': len(rules),\n",
    "        'min_support': MIN_SUPPORT,\n",
    "        'min_confidence': MIN_CONFIDENCE,\n",
    "        'avg_drugs_per_patient': float(np.mean([len(t) for t in transactions])),\n",
    "        'output_paths': results_saved\n",
    "    }\n",
    "    \n",
    "    summary_path = f\"{S3_OUTPUT_BASE}/global_metrics.json\"\n",
    "    save_to_s3_json(summary, summary_path)\n",
    "    results_saved['summary'] = summary_path\n",
    "    logger.info(f\"‚úì Saved summary to {summary_path}\")\n",
    "    \n",
    "    elapsed = time.time() - start_time\n",
    "    logger.info(f\"‚úì All results saved to S3 in {elapsed:.1f}s\")\n",
    "    \n",
    "    print(f\"\\n‚úÖ Results saved to S3:\")\n",
    "    for result_type, path in results_saved.items():\n",
    "        print(f\"  {result_type}: {path}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    logger.error(f\"Error saving to S3: {e}\")\n",
    "    print(f\"\\n‚ùå Error saving to S3: {e}\")\n",
    "    raise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Next Steps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"GLOBAL FPGROWTH ANALYSIS - SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nüìä Dataset Statistics:\")\n",
    "print(f\"  Total unique drugs: {len(global_drug_names):,}\")\n",
    "print(f\"  Total patients: {len(transactions):,}\")\n",
    "print(f\"  Avg drugs/patient: {np.mean([len(t) for t in transactions]):.1f}\")\n",
    "\n",
    "print(f\"\\nüîç FP-Growth Results:\")\n",
    "print(f\"  Min support: {MIN_SUPPORT} ({MIN_SUPPORT*100:.2f}%)\")\n",
    "print(f\"  Min confidence: {MIN_CONFIDENCE} ({MIN_CONFIDENCE*100:.1f}%)\")\n",
    "print(f\"  Frequent itemsets: {len(frequent_itemsets):,}\")\n",
    "print(f\"  Association rules: {len(rules):,}\")\n",
    "\n",
    "print(f\"\\nüíæ Output Files (S3):\")\n",
    "for result_type, path in results_saved.items():\n",
    "    print(f\"  {result_type}: {path}\")\n",
    "\n",
    "print(f\"\\nüéØ Next Steps:\")\n",
    "print(f\"  1. Load encoding map in CatBoost: load_from_s3_json('{results_saved['encoding_map']}')\")\n",
    "print(f\"  2. Use encodings for feature engineering in ML models\")\n",
    "print(f\"  3. Analyze association rules for drug interaction patterns\")\n",
    "print(f\"  4. Run cohort-specific FPGrowth analysis for detailed insights\")\n",
    "\n",
    "print(f\"\\n‚úì Analysis complete: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(\"=\"*80)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
