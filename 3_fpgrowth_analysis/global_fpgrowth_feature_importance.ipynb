{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f8e7c275",
   "metadata": {},
   "source": [
    "# Global FPGrowth Feature Importance Analysis\n",
    "\n",
    "**Purpose:** Population-level frequent pattern mining for ML feature engineering  \n",
    "**Updated:** November 23, 2025  \n",
    "**Hardware:** Optimized for EC2 (32 cores, 1TB RAM)  \n",
    "**Output:** `s3://pgxdatalake/gold/fpgrowth/global/{item_type}/`\n",
    "\n",
    "## Key Features\n",
    "\n",
    "✅ **Three Item Types** - Drugs, ICD codes, CPT codes  \n",
    "✅ **Global Patterns** - Discovers patterns across all 5.7M patients  \n",
    "✅ **ML Feature Engineering** - Creates encoding maps for CatBoost  \n",
    "✅ **Association Rules** - Identifies co-occurrence relationships  \n",
    "✅ **Memory Optimized** - Configurable support thresholds\n",
    "\n",
    "## Methodology\n",
    "\n",
    "For each item type (drug_name, icd_code, cpt_code):\n",
    "1. Extract all unique items from cohort data\n",
    "2. Create patient-level transactions (lists of items per patient)\n",
    "3. Encode transactions into binary matrix\n",
    "4. Run FP-Growth algorithm to find frequent itemsets\n",
    "5. Generate association rules from frequent itemsets\n",
    "6. Create encoding map for ML models\n",
    "7. Save all outputs to S3\n",
    "\n",
    "## Expected Runtime (EC2: 32 cores, 1TB RAM)\n",
    "\n",
    "- **MIN_SUPPORT=0.01**: ~2-3 hours (12,783 drugs → 300-500 itemsets)\n",
    "- **MIN_SUPPORT=0.005**: ~4-6 hours (more itemsets, more rules)\n",
    "- **Total for all 3 types**: ~6-18 hours\n",
    "\n",
    "## Data Scale\n",
    "\n",
    "- **Total Events**: 947 million\n",
    "- **Patients**: 5.7 million\n",
    "- **Unique Drugs**: 12,783\n",
    "- **Unique ICD Codes**: ~10,000\n",
    "- **Unique CPT Codes**: ~5,000\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c526ba4",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a6f9a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import time\n",
    "import json\n",
    "import logging\n",
    "from pathlib import Path\n",
    "from typing import List, Dict\n",
    "import pandas as pd\n",
    "import boto3\n",
    "from mlxtend.frequent_patterns import fpgrowth, association_rules\n",
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "\n",
    "# Add project root to path\n",
    "project_root = Path.cwd().parent if Path.cwd().name == '3_fpgrowth_analysis' else Path.cwd()\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "from helpers_1997_13.duckdb_utils import get_duckdb_connection\n",
    "\n",
    "print(\"✓ All imports successful\")\n",
    "print(f\"✓ Project root: {project_root}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f19b589",
   "metadata": {},
   "source": [
    "## 2. Configuration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20f8f5db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# EC2 CONFIGURATION (32 cores, 1TB RAM)\n",
    "# =============================================================================\n",
    "\n",
    "# FP-Growth parameters\n",
    "MIN_SUPPORT = 0.01      # Items must appear in 1% of patients\n",
    "MIN_CONFIDENCE = 0.01   # Rules must have 1% confidence\n",
    "\n",
    "# Item types to process\n",
    "ITEM_TYPES = ['drug_name', 'icd_code', 'cpt_code']\n",
    "\n",
    "# Paths\n",
    "S3_OUTPUT_BASE = \"s3://pgxdatalake/gold/fpgrowth/global\"\n",
    "LOCAL_DATA_PATH = project_root / \"data\" / \"gold\" / \"cohorts_F1120\"\n",
    "\n",
    "# Setup logger\n",
    "logger = logging.getLogger('global_fpgrowth')\n",
    "logger.setLevel(logging.INFO)\n",
    "if not logger.handlers:\n",
    "    handler = logging.StreamHandler()\n",
    "    formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
    "    handler.setFormatter(formatter)\n",
    "    logger.addHandler(handler)\n",
    "\n",
    "print(f\"✓ Min Support: {MIN_SUPPORT}\")\n",
    "print(f\"✓ Min Confidence: {MIN_CONFIDENCE}\")\n",
    "print(f\"✓ Item Types: {ITEM_TYPES}\")\n",
    "print(f\"✓ S3 Output: {S3_OUTPUT_BASE}\")\n",
    "print(f\"✓ Local Data: {LOCAL_DATA_PATH}\")\n",
    "print(f\"✓ Local Data Exists: {LOCAL_DATA_PATH.exists()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38d439c9",
   "metadata": {},
   "source": [
    "## 3. Define Helper Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63384d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_global_items(local_data_path: Path, item_type: str, logger: logging.Logger) -> List[str]:\n",
    "    \"\"\"Extract all unique items of specified type from local cohort data.\"\"\"\n",
    "    logger.info(f\"Extracting global {item_type}s from local cohort data...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    con = get_duckdb_connection(logger=logger)\n",
    "    parquet_pattern = str(local_data_path / \"**\" / \"cohort.parquet\")\n",
    "    \n",
    "    if item_type == 'drug_name':\n",
    "        query = f\"\"\"\n",
    "        SELECT DISTINCT drug_name as item\n",
    "        FROM read_parquet('{parquet_pattern}', hive_partitioning=1)\n",
    "        WHERE drug_name IS NOT NULL AND drug_name != '' AND event_type = 'pharmacy'\n",
    "        ORDER BY item\n",
    "        \"\"\"\n",
    "    elif item_type == 'icd_code':\n",
    "        query = f\"\"\"\n",
    "        WITH all_icds AS (\n",
    "            SELECT primary_icd_diagnosis_code as icd FROM read_parquet('{parquet_pattern}', hive_partitioning=1) \n",
    "            WHERE primary_icd_diagnosis_code IS NOT NULL AND event_type = 'medical'\n",
    "            UNION ALL\n",
    "            SELECT two_icd_diagnosis_code as icd FROM read_parquet('{parquet_pattern}', hive_partitioning=1) \n",
    "            WHERE two_icd_diagnosis_code IS NOT NULL AND event_type = 'medical'\n",
    "            UNION ALL\n",
    "            SELECT three_icd_diagnosis_code as icd FROM read_parquet('{parquet_pattern}', hive_partitioning=1) \n",
    "            WHERE three_icd_diagnosis_code IS NOT NULL AND event_type = 'medical'\n",
    "            UNION ALL\n",
    "            SELECT four_icd_diagnosis_code as icd FROM read_parquet('{parquet_pattern}', hive_partitioning=1) \n",
    "            WHERE four_icd_diagnosis_code IS NOT NULL AND event_type = 'medical'\n",
    "            UNION ALL\n",
    "            SELECT five_icd_diagnosis_code as icd FROM read_parquet('{parquet_pattern}', hive_partitioning=1) \n",
    "            WHERE five_icd_diagnosis_code IS NOT NULL AND event_type = 'medical'\n",
    "        )\n",
    "        SELECT DISTINCT icd as item FROM all_icds WHERE icd != '' ORDER BY item\n",
    "        \"\"\"\n",
    "    elif item_type == 'cpt_code':\n",
    "        query = f\"\"\"\n",
    "        SELECT DISTINCT procedure_code as item\n",
    "        FROM read_parquet('{parquet_pattern}', hive_partitioning=1)\n",
    "        WHERE procedure_code IS NOT NULL AND procedure_code != '' AND event_type = 'medical'\n",
    "        ORDER BY item\n",
    "        \"\"\"\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown item_type: {item_type}\")\n",
    "    \n",
    "    logger.info(f\"Running query for {item_type}...\")\n",
    "    df = con.execute(query).df()\n",
    "    con.close()\n",
    "    items = df['item'].tolist()\n",
    "    \n",
    "    elapsed = time.time() - start_time\n",
    "    logger.info(f\"✓ Extracted {len(items):,} unique {item_type}s in {elapsed:.1f}s\")\n",
    "    return items\n",
    "\n",
    "\n",
    "def create_global_transactions(local_data_path: Path, item_type: str, logger: logging.Logger) -> List[List[str]]:\n",
    "    \"\"\"Create patient-level transactions from local cohort data.\"\"\"\n",
    "    logger.info(f\"Creating global {item_type} transactions...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    con = get_duckdb_connection(logger=logger)\n",
    "    parquet_pattern = str(local_data_path / \"**\" / \"cohort.parquet\")\n",
    "    \n",
    "    if item_type == 'drug_name':\n",
    "        query = f\"\"\"\n",
    "        SELECT mi_person_key, drug_name as item\n",
    "        FROM read_parquet('{parquet_pattern}', hive_partitioning=1)\n",
    "        WHERE drug_name IS NOT NULL AND drug_name != '' AND event_type = 'pharmacy'\n",
    "        \"\"\"\n",
    "    elif item_type == 'icd_code':\n",
    "        query = f\"\"\"\n",
    "        WITH all_icds AS (\n",
    "            SELECT mi_person_key, primary_icd_diagnosis_code as icd FROM read_parquet('{parquet_pattern}', hive_partitioning=1) \n",
    "            WHERE primary_icd_diagnosis_code IS NOT NULL AND event_type = 'medical'\n",
    "            UNION ALL\n",
    "            SELECT mi_person_key, two_icd_diagnosis_code as icd FROM read_parquet('{parquet_pattern}', hive_partitioning=1) \n",
    "            WHERE two_icd_diagnosis_code IS NOT NULL AND event_type = 'medical'\n",
    "            UNION ALL\n",
    "            SELECT mi_person_key, three_icd_diagnosis_code as icd FROM read_parquet('{parquet_pattern}', hive_partitioning=1) \n",
    "            WHERE three_icd_diagnosis_code IS NOT NULL AND event_type = 'medical'\n",
    "            UNION ALL\n",
    "            SELECT mi_person_key, four_icd_diagnosis_code as icd FROM read_parquet('{parquet_pattern}', hive_partitioning=1) \n",
    "            WHERE four_icd_diagnosis_code IS NOT NULL AND event_type = 'medical'\n",
    "            UNION ALL\n",
    "            SELECT mi_person_key, five_icd_diagnosis_code as icd FROM read_parquet('{parquet_pattern}', hive_partitioning=1) \n",
    "            WHERE five_icd_diagnosis_code IS NOT NULL AND event_type = 'medical'\n",
    "        )\n",
    "        SELECT mi_person_key, icd as item FROM all_icds WHERE icd != ''\n",
    "        \"\"\"\n",
    "    elif item_type == 'cpt_code':\n",
    "        query = f\"\"\"\n",
    "        SELECT mi_person_key, procedure_code as item\n",
    "        FROM read_parquet('{parquet_pattern}', hive_partitioning=1)\n",
    "        WHERE procedure_code IS NOT NULL AND procedure_code != '' AND event_type = 'medical'\n",
    "        \"\"\"\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown item_type: {item_type}\")\n",
    "    \n",
    "    logger.info(f\"Loading {item_type} events...\")\n",
    "    df = con.execute(query).df()\n",
    "    con.close()\n",
    "    \n",
    "    logger.info(f\"Grouping by patient...\")\n",
    "    transactions = (\n",
    "        df.groupby('mi_person_key')['item']\n",
    "        .apply(lambda x: sorted(set(x.tolist())))\n",
    "        .tolist()\n",
    "    )\n",
    "    \n",
    "    elapsed = time.time() - start_time\n",
    "    logger.info(f\"✓ Created {len(transactions):,} patient transactions in {elapsed:.1f}s\")\n",
    "    return transactions\n",
    "\n",
    "print(\"✓ Helper functions defined\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccc1346a",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5415ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_item_type(item_type, local_data_path, s3_output_base, min_support, min_confidence, logger):\n",
    "    \"\"\"Process a single item type: extract, encode, FP-Growth, save to S3.\"\"\"\n",
    "    logger.info(f\"\\n{'='*80}\")\n",
    "    logger.info(f\"Processing {item_type.upper()}\")\n",
    "    logger.info(f\"{'='*80}\")\n",
    "    overall_start = time.time()\n",
    "    \n",
    "    try:\n",
    "        # Extract items\n",
    "        items = extract_global_items(local_data_path, item_type, logger)\n",
    "        \n",
    "        # Create transactions\n",
    "        transactions = create_global_transactions(local_data_path, item_type, logger)\n",
    "        \n",
    "        # Encode transactions\n",
    "        logger.info(f\"Encoding {len(transactions):,} transactions...\")\n",
    "        encode_start = time.time()\n",
    "        te = TransactionEncoder()\n",
    "        te_ary = te.fit(transactions).transform(transactions)\n",
    "        df_encoded = pd.DataFrame(te_ary, columns=te.columns_)\n",
    "        encode_time = time.time() - encode_start\n",
    "        logger.info(f\"✓ Encoded to {df_encoded.shape} matrix in {encode_time:.1f}s\")\n",
    "        \n",
    "        # Run FP-Growth\n",
    "        logger.info(f\"Running FP-Growth (min_support={min_support})...\")\n",
    "        fpgrowth_start = time.time()\n",
    "        itemsets = fpgrowth(df_encoded, min_support=min_support, use_colnames=True)\n",
    "        itemsets = itemsets.sort_values('support', ascending=False).reset_index(drop=True)\n",
    "        fpgrowth_time = time.time() - fpgrowth_start\n",
    "        logger.info(f\"✓ Found {len(itemsets):,} frequent itemsets in {fpgrowth_time:.1f}s\")\n",
    "        \n",
    "        # Generate association rules\n",
    "        logger.info(f\"Generating association rules (min_confidence={min_confidence})...\")\n",
    "        rules_start = time.time()\n",
    "        rules = association_rules(itemsets, metric=\"confidence\", min_threshold=min_confidence)\n",
    "        rules = rules.sort_values('lift', ascending=False).reset_index(drop=True)\n",
    "        rules_time = time.time() - rules_start\n",
    "        logger.info(f\"✓ Generated {len(rules):,} association rules in {rules_time:.1f}s\")\n",
    "        \n",
    "        # Create encoding map\n",
    "        encoding_map = {}\n",
    "        for idx, row in itemsets.iterrows():\n",
    "            if len(row['itemsets']) == 1:\n",
    "                item = list(row['itemsets'])[0]\n",
    "                encoding_map[item] = {'support': float(row['support']), 'rank': int(idx)}\n",
    "        logger.info(f\"✓ Created encoding map with {len(encoding_map):,} items\")\n",
    "        \n",
    "        # Save to S3\n",
    "        logger.info(f\"Saving results to S3...\")\n",
    "        s3_client = boto3.client('s3')\n",
    "        prefix = f\"gold/fpgrowth/global/{item_type}\"\n",
    "        \n",
    "        # Convert frozensets to lists\n",
    "        itemsets_json = itemsets.copy()\n",
    "        itemsets_json['itemsets'] = itemsets_json['itemsets'].apply(list)\n",
    "        rules_json = rules.copy()\n",
    "        rules_json['antecedents'] = rules_json['antecedents'].apply(list)\n",
    "        rules_json['consequents'] = rules_json['consequents'].apply(list)\n",
    "        \n",
    "        # Upload files\n",
    "        s3_client.put_object(Bucket='pgxdatalake', Key=f\"{prefix}/encoding_map.json\", \n",
    "                            Body=json.dumps(encoding_map, indent=2))\n",
    "        s3_client.put_object(Bucket='pgxdatalake', Key=f\"{prefix}/itemsets.json\", \n",
    "                            Body=itemsets_json.to_json(orient='records', indent=2))\n",
    "        s3_client.put_object(Bucket='pgxdatalake', Key=f\"{prefix}/rules.json\", \n",
    "                            Body=rules_json.to_json(orient='records', indent=2))\n",
    "        \n",
    "        # Save metrics\n",
    "        metrics = {\n",
    "            'item_type': item_type, 'min_support': min_support, 'min_confidence': min_confidence,\n",
    "            'unique_items': len(items), 'total_transactions': len(transactions),\n",
    "            'frequent_itemsets': len(itemsets), 'association_rules': len(rules),\n",
    "            'encoding_map_size': len(encoding_map),\n",
    "            'processing_time_seconds': {'total': time.time() - overall_start}\n",
    "        }\n",
    "        s3_client.put_object(Bucket='pgxdatalake', Key=f\"{prefix}/metrics.json\", \n",
    "                            Body=json.dumps(metrics, indent=2))\n",
    "        \n",
    "        logger.info(f\"✓ {item_type.upper()} COMPLETE - {len(itemsets):,} itemsets, {len(rules):,} rules\")\n",
    "        return metrics\n",
    "    except Exception as e:\n",
    "        logger.error(f\"✗ Failed: {e}\", exc_info=True)\n",
    "        return {'item_type': item_type, 'error': str(e)}\n",
    "\n",
    "print(\"✓ process_item_type function defined\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc0d78d1",
   "metadata": {},
   "source": [
    "## 5. Execute Analysis\n",
    "\n",
    "Process all item types sequentially.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9beab471",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(f\"\\n{'='*80}\")\n",
    "logger.info(f\"GLOBAL FPGROWTH ANALYSIS - START\")\n",
    "logger.info(f\"{'='*80}\")\n",
    "logger.info(f\"Item types: {ITEM_TYPES}\")\n",
    "logger.info(f\"Min support: {MIN_SUPPORT}\")\n",
    "logger.info(f\"Min confidence: {MIN_CONFIDENCE}\")\n",
    "\n",
    "overall_start = time.time()\n",
    "all_metrics = []\n",
    "\n",
    "for item_type in ITEM_TYPES:\n",
    "    metrics = process_item_type(\n",
    "        item_type=item_type,\n",
    "        local_data_path=LOCAL_DATA_PATH,\n",
    "        s3_output_base=S3_OUTPUT_BASE,\n",
    "        min_support=MIN_SUPPORT,\n",
    "        min_confidence=MIN_CONFIDENCE,\n",
    "        logger=logger\n",
    "    )\n",
    "    all_metrics.append(metrics)\n",
    "\n",
    "total_elapsed = time.time() - overall_start\n",
    "\n",
    "logger.info(f\"\\n{'='*80}\")\n",
    "logger.info(f\"GLOBAL FPGROWTH ANALYSIS - COMPLETE\")\n",
    "logger.info(f\"{'='*80}\")\n",
    "logger.info(f\"Total processing time: {total_elapsed:.1f}s ({total_elapsed/60:.1f}min)\")\n",
    "logger.info(f\"\\nResults Summary:\")\n",
    "for m in all_metrics:\n",
    "    if 'error' not in m:\n",
    "        logger.info(f\"  {m['item_type']}: {m['frequent_itemsets']:,} itemsets, {m['association_rules']:,} rules\")\n",
    "    else:\n",
    "        logger.info(f\"  {m['item_type']}: ERROR - {m['error']}\")\n",
    "\n",
    "print(\"\\n✓ Analysis complete!\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
