{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Feature Importance with Monte Carlo Cross-Validation (Python)\n",
        "\n",
        "**Purpose:** Calculate scaled feature importance using multiple ML models  \n",
        "**Method:** Normalized feature importance scaled by MC-CV Recall scores  \n",
        "**Updated:** December 2024  \n",
        "**Hardware:** Optimized for EC2 (32 cores, 1TB RAM)  \n",
        "**Validation:** Proper evaluation on unseen test data\n",
        "\n",
        "## Key Features\n",
        "\n",
        "‚úÖ **Monte Carlo Cross-Validation** ‚Äì up to 1000 random train/test splits (100-split runs used for faster iteration)  \n",
        "‚úÖ **Stratified Sampling** - Maintains target distribution  \n",
        "‚úÖ **Parallel Processing** - Fast execution with joblib (‚âà30 workers)  \n",
        "‚úÖ **95% Confidence Intervals** - Narrow, precise estimates (tighter with more splits)  \n",
        "‚úÖ **Multiple Models** - Tree ensembles: CatBoost, Random Forest, XGBoost, XGBoost RF, LightGBM, ExtraTrees  \n",
        "‚úÖ **Linear Models** - LogisticRegression, LinearSVC, ElasticNet, LASSO  \n",
        "\n",
        "## Methodology\n",
        "\n",
        "This notebook implements the feature selection methodology:\n",
        "\n",
        "1. Load cohort data from parquet files (same as FP-Growth notebook)\n",
        "   - **Training data**: Years 2016-2018 (combined)\n",
        "   - **Test data**: Year 2019 (avoiding COVID year 2020)\n",
        "2. Create patient-level features (one-hot encoding of items)\n",
        "3. For each model type:\n",
        "   - Create 100‚Äì1000 MC-CV splits (each split samples from 2016-2018 training data)\n",
        "   - Train model on sampled training subset\n",
        "   - Evaluate Recall on 2019 test set (temporal validation)\n",
        "   - Extract feature importance\n",
        "   - Aggregate results across splits\n",
        "4. Normalize and scale feature importance by MC-CV Recall\n",
        "5. Aggregate across models\n",
        "6. Extract top features\n",
        "\n",
        "## Expected Runtime\n",
        "\n",
        "- **100 splits (current default):**\n",
        "  - Local (4 cores): ~2‚Äì4 hours\n",
        "  - Workstation (16 cores): ~1‚Äì2 hours\n",
        "  - EC2 (32 cores, 1TB RAM): ~1‚Äì2 hours ‚úÖ **RECOMMENDED FOR DEVELOPMENT**\n",
        "- **1000 splits (extended / publication-level):**\n",
        "  - Local (4 cores): 8‚Äì12+ hours\n",
        "  - Workstation (16 cores): ~8‚Äì16 hours\n",
        "  - EC2 (32 cores, 1TB RAM): ~10‚Äì20 hours ‚úÖ **RECOMMENDED FOR FINAL RESULTS**\n",
        "\n",
        "**üìñ Documentation:** See [Feature Importance README](README.md) for detailed documentation, usage examples, and troubleshooting.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup and Configuration\n",
        "\n",
        "Load required packages and configure parallel processing.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "from typing import Dict, List, Optional, Tuple\n",
        "from itertools import product\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Add project root to path\n",
        "project_root = Path.cwd().parent if Path.cwd().name == '3_feature_importance' else Path.cwd()\n",
        "if str(project_root) not in sys.path:\n",
        "    sys.path.insert(0, str(project_root))\n",
        "\n",
        "# Import constants\n",
        "from helpers_1997_13.constants import AGE_BANDS, COHORT_NAMES, EVENT_YEARS, S3_BUCKET\n",
        "\n",
        "# Import helper modules from helpers_1997_13\n",
        "from helpers_1997_13.logging_utils import setup_r_logging, save_logs_to_s3_r, check_memory_usage_r\n",
        "from helpers_1997_13.model_utils import calculate_recall, calculate_logloss\n",
        "from helpers_1997_13.mc_cv_utils import run_mc_cv_method\n",
        "from helpers_1997_13.feature_importance_utils import run_cohort_analysis\n",
        "from helpers_1997_13.s3_utils import check_feature_importance_results_exist, check_cohort_file_exists\n",
        "\n",
        "print(\"‚úì All packages loaded successfully\")\n",
        "print(f\"‚úì Age bands loaded from constants: {', '.join(AGE_BANDS)}\")\n",
        "print(f\"‚úì Cohorts loaded from constants: {', '.join(COHORT_NAMES)}\")\n",
        "print(f\"‚úì Event years loaded from constants: {', '.join(EVENT_YEARS)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Configuration\n",
        "\n",
        "Set debug mode, MC-CV parameters, and model configuration.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# DEBUG/TEST MODE - Quick testing before full run\n",
        "# ============================================================\n",
        "# Set DEBUG_MODE = True for quick testing (5 splits, ~2-5 min)\n",
        "# Set DEBUG_MODE = False for full analysis (100 splits, ~1-2 hours on EC2)\n",
        "\n",
        "DEBUG_MODE = False  # Change to True for quick test\n",
        "\n",
        "if DEBUG_MODE:\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"üîç DEBUG MODE ENABLED\")\n",
        "    print(\"=\"*80)\n",
        "    print(\"\\nQuick test configuration:\")\n",
        "    print(\"  ‚Ä¢ MC-CV Splits: 5 (instead of 100)\")\n",
        "    print(\"  ‚Ä¢ Expected time: 2-5 minutes\")\n",
        "    print(\"  ‚Ä¢ Purpose: Verify everything works before full run\")\n",
        "    print(\"\\nTo run full analysis, set DEBUG_MODE = False\\n\")\n",
        "\n",
        "# Configuration\n",
        "# Train on 2016-2018, test on 2019 (avoiding COVID year 2020)\n",
        "TRAIN_YEARS = [2016, 2017, 2018]  # Years to use for training\n",
        "TEST_YEAR = 2019  # Year to use for testing\n",
        "\n",
        "N_SPLITS = 5 if DEBUG_MODE else 200  # MC-CV splits (5 for debug, 100 for development, 1000 for production)\n",
        "TEST_SIZE = 0.2  # Test set proportion (20%) - used for sampling from training years\n",
        "TRAIN_PROP = 1 - TEST_SIZE  # Training proportion (80%) - used for sampling from training years\n",
        "\n",
        "# Scaling metric for feature importance\n",
        "# Options: \"recall\" (default) or \"logloss\"\n",
        "SCALING_METRIC = \"recall\"  # Change to \"logloss\" if preferred\n",
        "\n",
        "# Model parameters\n",
        "MODEL_PARAMS = {\n",
        "    # Tree Ensembles\n",
        "    'catboost': {\n",
        "        'iterations': 100 if DEBUG_MODE else 500,\n",
        "        'learning_rate': 0.1,\n",
        "        'depth': 6,\n",
        "        'verbose': False,\n",
        "        'random_seed': 42\n",
        "    },\n",
        "    'random_forest': {\n",
        "        'ntree': 100 if DEBUG_MODE else 500,\n",
        "        'mtry': None,  # Will be set to sqrt(n_features)\n",
        "        'nodesize': 1,\n",
        "        'maxnodes': None,\n",
        "        'random_seed': 42\n",
        "    },\n",
        "    'xgboost': {\n",
        "        'max_depth': 6,\n",
        "        'learning_rate': 0.1,\n",
        "        'n_estimators': 100 if DEBUG_MODE else 500,\n",
        "        'subsample': 1.0,\n",
        "        'colsample_bytree': 1.0,\n",
        "        'random_seed': 42\n",
        "    },\n",
        "    'xgboost_rf': {\n",
        "        'max_depth': 6,\n",
        "        'learning_rate': 0.1,\n",
        "        'n_estimators': 100 if DEBUG_MODE else 500,\n",
        "        'subsample': 0.8,\n",
        "        'max_features': None,  # Will be set to sqrt(n_features)\n",
        "        'random_seed': 42\n",
        "    },\n",
        "    'lightgbm': {\n",
        "        'n_estimators': 100 if DEBUG_MODE else 500,\n",
        "        'learning_rate': 0.1,\n",
        "        'num_leaves': 31,\n",
        "        'feature_fraction': 1.0,\n",
        "        'bagging_fraction': 1.0,\n",
        "        'bagging_freq': 0,\n",
        "        'random_seed': 42\n",
        "    },\n",
        "    'extratrees': {\n",
        "        'n_estimators': 100 if DEBUG_MODE else 500,\n",
        "        'max_features': None,  # Will be set to sqrt(n_features)\n",
        "        'min_samples_leaf': 1,\n",
        "        'max_depth': None,\n",
        "        'random_seed': 42\n",
        "    },\n",
        "    # Linear Models\n",
        "    'logistic_regression': {\n",
        "        'penalty': 'l2',\n",
        "        'C': 1.0,\n",
        "        'solver': 'lbfgs',\n",
        "        'max_iter': 1000,\n",
        "        'random_seed': 42\n",
        "    },\n",
        "    'linearsvc': {\n",
        "        'penalty': 'l2',\n",
        "        'C': 1.0,\n",
        "        'loss': 'squared_hinge',\n",
        "        'max_iter': 1000,\n",
        "        'dual': True,\n",
        "        'random_seed': 42\n",
        "    },\n",
        "    'elasticnet': {\n",
        "        'C': 1.0,\n",
        "        'l1_ratio': 0.5,\n",
        "        'max_iter': 1000,\n",
        "        'random_seed': 42\n",
        "    },\n",
        "    'lasso': {\n",
        "        'C': 1.0,\n",
        "        'max_iter': 1000,\n",
        "        'random_seed': 42\n",
        "    }\n",
        "}\n",
        "\n",
        "# Set up parallel processing\n",
        "# EC2 optimization: Use 30 out of 32 cores (leave 2 for system)\n",
        "import multiprocessing\n",
        "N_WORKERS = int(os.getenv(\"N_WORKERS\", \"0\"))\n",
        "if N_WORKERS < 1:\n",
        "    # Auto-detect: use all cores minus 2 for system\n",
        "    total_cores = multiprocessing.cpu_count()\n",
        "    N_WORKERS = max(1, total_cores - 2)\n",
        "    print(f\"Auto-detected {total_cores} cores, using {N_WORKERS} workers\")\n",
        "else:\n",
        "    print(f\"Using {N_WORKERS} workers from N_WORKERS environment variable\")\n",
        "\n",
        "# Output directory\n",
        "output_dir = Path(\"3_feature_importance/outputs\")\n",
        "output_dir.mkdir(exist_ok=True, parents=True)\n",
        "\n",
        "print(f\"\\nOutput directory: {output_dir}\")\n",
        "print(f\"MC-CV Configuration: {N_SPLITS} splits, {TRAIN_PROP*100:.0f}/{TEST_SIZE*100:.0f} train/test split\")\n",
        "print(f\"Cohorts to process: {', '.join(COHORT_NAMES)}\")\n",
        "print(f\"Age bands to process: {', '.join(AGE_BANDS)}\")\n",
        "print(f\"Running {len(COHORT_NAMES)} cohort(s) √ó {len(AGE_BANDS)} age-band(s) = {len(COHORT_NAMES) * len(AGE_BANDS)} combinations\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Parallel Execution\n",
        "\n",
        "Run feature importance analysis for all cohort √ó age-band combinations in parallel.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from joblib import Parallel, delayed\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Create all combinations of cohort and age-band\n",
        "combinations = list(product(COHORT_NAMES, AGE_BANDS))\n",
        "\n",
        "# Filter combinations: check if cohort files exist and if results already exist\n",
        "combinations_to_process = []\n",
        "for cohort_name, age_band in combinations:\n",
        "    # Check if training files exist (need all train years)\n",
        "    train_files_exist = all(\n",
        "        check_cohort_file_exists(cohort_name, age_band, year) \n",
        "        for year in TRAIN_YEARS\n",
        "    )\n",
        "    \n",
        "    if not train_files_exist:\n",
        "        missing_years = [y for y in TRAIN_YEARS if not check_cohort_file_exists(cohort_name, age_band, y)]\n",
        "        print(f\"‚ö† Skipping {cohort_name}/{age_band}: missing training files for years: {missing_years}\")\n",
        "        continue\n",
        "    \n",
        "    # Check if test file exists\n",
        "    if not check_cohort_file_exists(cohort_name, age_band, TEST_YEAR):\n",
        "        print(f\"‚ö† Skipping {cohort_name}/{age_band}: test file not found for year {TEST_YEAR}\")\n",
        "        continue\n",
        "    \n",
        "    # Check if results already exist (idempotency) - using test_year for S3 path\n",
        "    if check_feature_importance_results_exist(cohort_name, age_band, TEST_YEAR):\n",
        "        print(f\"‚úì Skipping {cohort_name}/{age_band} (train: {TRAIN_YEARS}, test: {TEST_YEAR}): results already exist in S3\")\n",
        "        continue\n",
        "    \n",
        "    combinations_to_process.append((cohort_name, age_band))\n",
        "\n",
        "print(f\"\\n{'='*80}\")\n",
        "print(f\"Processing {len(combinations_to_process)} combinations\")\n",
        "print(f\"{'='*80}\\n\")\n",
        "\n",
        "if len(combinations_to_process) == 0:\n",
        "    print(\"No combinations to process. All results already exist or cohort files missing.\")\n",
        "else:\n",
        "    # Run analysis in parallel\n",
        "    def run_single_combination(args):\n",
        "        cohort_name, age_band = args\n",
        "        try:\n",
        "            result = run_cohort_analysis(\n",
        "                cohort_name=cohort_name,\n",
        "                age_band=age_band,\n",
        "                train_years=TRAIN_YEARS,\n",
        "                test_year=TEST_YEAR,\n",
        "                n_splits=N_SPLITS,\n",
        "                train_prop=TRAIN_PROP,\n",
        "                n_workers=N_WORKERS,\n",
        "                scaling_metric=SCALING_METRIC,\n",
        "                model_params=MODEL_PARAMS,\n",
        "                debug_mode=DEBUG_MODE,\n",
        "                output_dir=str(output_dir)\n",
        "            )\n",
        "            return result\n",
        "        except Exception as e:\n",
        "            print(f\"‚úó Error processing {cohort_name}/{age_band} (train: {TRAIN_YEARS}, test: {TEST_YEAR}): {str(e)}\")\n",
        "            return {\n",
        "                'cohort': cohort_name,\n",
        "                'age_band': age_band,\n",
        "                'train_years': TRAIN_YEARS,\n",
        "                'test_year': TEST_YEAR,\n",
        "                'status': 'error',\n",
        "                'error': str(e)\n",
        "            }\n",
        "    \n",
        "    # Run in parallel with progress bar\n",
        "    results = Parallel(n_jobs=min(N_WORKERS, len(combinations_to_process)), verbose=0)(\n",
        "        delayed(run_single_combination)(combo) \n",
        "        for combo in tqdm(combinations_to_process, desc=\"Processing combinations\")\n",
        "    )\n",
        "    \n",
        "    # Print summary\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(\"Processing Summary\")\n",
        "    print(f\"{'='*80}\")\n",
        "    \n",
        "    successful = [r for r in results if r.get('status') == 'success']\n",
        "    failed = [r for r in results if r.get('status') == 'error']\n",
        "    \n",
        "    print(f\"‚úì Successful: {len(successful)}\")\n",
        "    print(f\"‚úó Failed: {len(failed)}\")\n",
        "    \n",
        "    if failed:\n",
        "        print(\"\\nFailed combinations:\")\n",
        "        for r in failed:\n",
        "            train_years_str = ', '.join(map(str, r.get('train_years', TRAIN_YEARS)))\n",
        "            print(f\"  - {r['cohort']}/{r['age_band']} (train: {train_years_str}, test: {r.get('test_year', TEST_YEAR)}): {r.get('error', 'Unknown error')}\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
