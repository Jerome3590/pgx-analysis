{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Importance Cohort Runner (EC2)\n",
    "\n",
    "This notebook is designed to run the **feature importance Monte Carlo CV pipeline** for all configured cohorts on an **EC2 instance**.\n",
    "\n",
    "- **Cohort scripts**: `3_feature_importance/run_cohort_*.py`\n",
    "- **Data location (EC2)**: `/mnt/nvme/cohorts` (synced from `s3://pgxdatalake/gold/cohorts_F1120/`)\n",
    "- **Environment**: Python environment with `xgboost`, `catboost`, `lightgbm`, `scikit-learn`, `pandas`, `numpy`, etc.\n",
    "\n",
    "**Purpose:** Calculate scaled feature importance across various ML algorithms  \n",
    "**Method:** Normalized feature importance scaled by MC-CV Recall scores  \n",
    "**Hardware:** Optimized for EC2 (32 cores, 1TB RAM)  \n",
    "\n",
    "## Performance Optimizations\n",
    "\n",
    "ðŸš€ **Parallel Processing** â€“ Leverages all available CPU cores for maximum performance:\n",
    "- **Feature Matrix Creation**: Parallel column-by-column construction using `joblib.Parallel` (uses `cpu_count() - 2` workers)\n",
    "- **MC-CV Splits**: Parallel execution of Monte Carlo cross-validation splits (uses `cpu_count() - 2` workers)\n",
    "- **Memory Efficient**: Replaced memory-intensive `pivot_table` with incremental column building to handle large cohorts (32,000+ features)\n",
    "\n",
    "ðŸ’¾ **Memory Optimization**:\n",
    "- Column-by-column feature matrix construction reduces peak memory usage\n",
    "- Optimized for large cohorts (e.g., age band 25-44 with 78,000+ patients and 32,000+ features)\n",
    "- Efficient handling of sparse categorical features for CatBoost\n",
    "\n",
    "## Key Features\n",
    "\n",
    "âœ… **Monte Carlo Cross-Validation** â€“ up to 1000 random train/test splits (200-split runs used for faster iteration)  \n",
    "âœ… **Stratified Sampling** - Maintains target distribution  \n",
    "âœ… **95% Confidence Intervals** - Narrow, precise estimates (tighter with more splits)  \n",
    "âœ… **Multiple Models** - CatBoost, Random Forest, XGBoost, LightGBM, Extra Trees, Logistic Regression, LinearSVC, ElasticNet, Lasso  \n",
    "âœ… **Permutation-Based Feature Importance** - Model-agnostic importance calculation for fair comparison across models  \n",
    "âœ… **Idempotent Workflow** - Automatically skips models with existing results (checks local files first, then S3)  \n",
    "âœ… **Parallel Processing** - Leverages all available CPU cores for feature matrix creation and MC-CV splits (uses `cpu_count() - 2` workers)  \n",
    "âœ… **Memory Optimized** - Column-by-column feature matrix construction handles large cohorts (32,000+ features, 78,000+ patients)  \n",
    "\n",
    "## Methodology\n",
    "\n",
    "This notebook implements the feature selection methodology:\n",
    "\n",
    "1. Load cohort data from parquet files (same as FP-Growth notebook)\n",
    "2. Create patient-level features (one-hot encoding of items)\n",
    "   - **Optimized**: Parallel feature matrix creation for CatBoost categorical features\n",
    "   - Filters constant features globally before MC-CV splits\n",
    "3. For each model type:\n",
    "   - Create 200 stratified train/test splits (parallelized across all cores)\n",
    "   - Train model on training set\n",
    "   - Evaluate Recall on unseen test set\n",
    "   - Extract permutation-based feature importance\n",
    "   - Aggregate results across splits\n",
    "4. Normalize and scale feature importance by MC-CV Recall\n",
    "5. Aggregate across models\n",
    "6. Extract top features\n",
    "\n",
    "## Output Files\n",
    "\n",
    "All results are saved locally and uploaded to S3:\n",
    "- **Individual model results**: `{cohort}_{age_band}_{method}_feature_importance.csv`\n",
    "- **Aggregated results**: `{cohort}_{age_band}_aggregated_feature_importance.csv`\n",
    "- **Constant features**: `{cohort}_{age_band}_constant_features.csv`\n",
    "- **S3 location**: `s3://pgxdatalake/gold/feature_importance/{cohort}/{age_band}/`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Using Python binary: /home/pgx3874/jupyter-env/bin/python3.11\n",
      "[INFO] Project root: /home/pgx3874/pgx-analysis\n",
      "Expected cohort data path: /mnt/nvme/cohorts\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "PYTHON_BIN = Path(\"/home/pgx3874/jupyter-env/bin/python3.11\")\n",
    "\n",
    "if not PYTHON_BIN.exists():\n",
    "    raise FileNotFoundError(\n",
    "        f\"Python binary not found at:\\n  {PYTHON_BIN}\\n\"\n",
    "        \"Ensure your EC2 environment path is correct.\"\n",
    "    )\n",
    "\n",
    "print(f\"[INFO] Using Python binary: {PYTHON_BIN}\")\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# Resolve project_root robustly for BOTH notebook + script mode\n",
    "# -------------------------------------------------------------\n",
    "def resolve_project_root():\n",
    "    # Case 1: running as a script â†’ __file__ exists\n",
    "    if '__file__' in globals():\n",
    "        return Path(__file__).resolve().parents[1]\n",
    "\n",
    "    # Case 2: running in Jupyter/Notebook â†’ no __file__\n",
    "    # Fallback = assume notebook is running inside project folder structure\n",
    "    notebook_path = Path(os.getcwd()).resolve()\n",
    "\n",
    "    # If running in .../pgx-analysis/3_feature_importance, go up 1 level\n",
    "    if notebook_path.name == \"3_feature_importance\":\n",
    "        return notebook_path.parent\n",
    "\n",
    "    # If running deeper inside scripts, go up until pgx-analysis appears\n",
    "    for parent in notebook_path.parents:\n",
    "        if parent.name == \"pgx-analysis\":\n",
    "            return parent\n",
    "\n",
    "    # Last fallback: use current working directory\n",
    "    return notebook_path\n",
    "\n",
    "\n",
    "PROJECT_ROOT = resolve_project_root()\n",
    "print(f\"[INFO] Project root: {PROJECT_ROOT}\")\n",
    "\n",
    "# Add to sys.path if needed\n",
    "if str(PROJECT_ROOT) not in sys.path:\n",
    "    sys.path.append(str(PROJECT_ROOT))\n",
    "\n",
    "# Expected EC2 data location (synced from S3) \n",
    "DATA_PATH = Path(\"/mnt/nvme/cohorts\") \n",
    "print(f\"Expected cohort data path: {DATA_PATH}\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Per-Cohort Runner Cells\n",
    "\n",
    "Each cell below runs a **single cohort script**. This makes it easy to:\n",
    "\n",
    "- Debug failures for a specific cohort/age-band\n",
    "- Modify a cohort script and immediately re-run just that cohort\n",
    "\n",
    "All scripts automatically leverage parallel processing and are idempotent (skip completed models).\n",
    "\n",
    "All cells assume this notebook is running from the `3_feature_importance/` directory (the default when opened from Jupyter in the project root).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cohort 1 â€“ Age 0â€“12\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running feature importance analysis:\n",
      "  Cohort: opioid_ed\n",
      "  Age Band: 0-12\n",
      "  Train Years: [2016, 2017, 2018]\n",
      "  Test Year: 2019\n",
      "  MC-CV Splits: 200\n",
      "  Workers: 30\n",
      "  Output Directory: 3_feature_importance/outputs\n",
      "\n",
      "Note: This script is idempotent - models with existing results in S3 will be skipped.\n",
      "\n",
      "2025-11-27 08:36:41,274 - INFO - ================================================================================\n",
      "2025-11-27 08:36:41,274 - INFO - FEATURE IMPORTANCE ANALYSIS - MONTE CARLO CROSS-VALIDATION\n",
      "2025-11-27 08:36:41,274 - INFO - ================================================================================\n",
      "2025-11-27 08:36:41,274 - INFO - Cohort: opioid_ed\n",
      "2025-11-27 08:36:41,274 - INFO - Age Band: 0-12\n",
      "2025-11-27 08:36:41,274 - INFO - Train Years: 2016, 2017, 2018\n",
      "2025-11-27 08:36:41,274 - INFO - Test Year: 2019\n",
      "2025-11-27 08:36:41,274 - INFO - MC-CV Splits: 200\n",
      "2025-11-27 08:36:41,274 - INFO - Scaling Metric: recall\n",
      "2025-11-27 08:36:41,274 - INFO - Debug Mode: Disabled\n",
      "2025-11-27 08:36:41,274 - INFO - ================================================================================\n",
      "2025-11-27 08:36:41,274 - INFO - Loading cohort data...\n",
      "2025-11-27 08:36:41,275 - INFO - Memory usage [Before Data Loading]: 336.3 MB\n",
      "2025-11-27 08:36:41,275 - INFO - Loading training data from years: 2016, 2017, 2018\n",
      "2025-11-27 08:36:41,289 - INFO - Loaded 438 records from year 2016\n",
      "2025-11-27 08:36:41,301 - INFO - Loaded 856 records from year 2017\n",
      "2025-11-27 08:36:41,312 - INFO - Loaded 892 records from year 2018\n",
      "2025-11-27 08:36:41,313 - INFO - Combined training data: 2186 event-level records, 78 unique patients\n",
      "2025-11-27 08:36:41,313 - INFO - Loading test data from year: 2019\n",
      "2025-11-27 08:36:41,325 - INFO - Test data: 1936 event-level records, 66 unique patients\n",
      "2025-11-27 08:36:41,325 - INFO - Memory usage [After Data Loading]: 345.7 MB\n",
      "2025-11-27 08:36:41,325 - INFO - Creating patient-level features for training data...\n",
      "2025-11-27 08:36:41,332 - INFO - Creating patient-level features for test data...\n",
      "2025-11-27 08:36:41,338 - INFO - Total unique items across train and test: 768\n",
      "2025-11-27 08:36:41,338 - INFO - Building CatBoost feature matrix: 78 patients Ã— 768 features\n",
      "2025-11-27 08:36:42,208 - INFO - CatBoost feature matrix created: 78 rows Ã— 768 features\n",
      "2025-11-27 08:36:42,254 - INFO - Building CatBoost feature matrix: 66 patients Ã— 768 features\n",
      "2025-11-27 08:36:42,638 - INFO - CatBoost feature matrix created: 66 rows Ã— 768 features\n",
      "2025-11-27 08:36:42,773 - INFO - Filtering constant features...\n",
      "2025-11-27 08:36:42,773 - INFO - Items in training data: 549\n",
      "2025-11-27 08:36:42,773 - INFO - Items in test data: 421\n",
      "2025-11-27 08:36:42,773 - INFO - Total items (train + test): 768\n",
      "2025-11-27 08:36:42,773 - INFO - Sample training items: ['J2405', '7197', '99203', 'J121', 'H53023']\n",
      "2025-11-27 08:36:42,781 - INFO - Item '7197': nunique=2, sample values: {'': 77, '7197': 1}\n",
      "2025-11-27 08:36:42,786 - INFO - Item 'J121': nunique=2, sample values: {'': 77, 'J121': 1}\n",
      "2025-11-27 08:36:42,788 - INFO - Item 'H53023': nunique=2, sample values: {'': 77, 'H53023': 1}\n",
      "2025-11-27 08:36:42,800 - INFO - Item '99203': nunique=2, sample values: {'': 69, '99203': 9}\n",
      "2025-11-27 08:36:42,802 - INFO - Item 'J2405': nunique=2, sample values: {'': 75, 'J2405': 3}\n",
      "2025-11-27 08:36:42,809 - INFO - Removing 219 constant features (out of 768 total)\n",
      "2025-11-27 08:36:42,811 - INFO - Saved constant features list: 3_feature_importance/outputs/opioid_ed_0_12_constant_features.csv\n",
      "2025-11-27 08:36:42,869 - INFO - Uploaded constant features to S3: s3://pgxdatalake/gold/feature_importance/opioid_ed/0-12/constant_features.csv\n",
      "2025-11-27 08:36:42,883 - INFO - Feature engineering complete:\n",
      "2025-11-27 08:36:42,883 - INFO -   Training: 78 patients, 549 features\n",
      "2025-11-27 08:36:42,883 - INFO -   Test: 66 patients, 549 features\n",
      "2025-11-27 08:36:42,883 - INFO - Memory usage [After Feature Engineering]: 356.6 MB\n",
      "2025-11-27 08:36:42,883 - INFO - Creating MC-CV splits (sampling from train years, testing on 2019)...\n",
      "2025-11-27 08:36:42,883 - INFO - Memory usage [Before MC-CV Split Creation]: 356.6 MB\n",
      "2025-11-27 08:36:42,904 - INFO - Created 200 MC-CV splits (train: sampled from 2016, 2017, 2018, test: 2019)\n",
      "2025-11-27 08:36:42,904 - INFO - Memory usage [After MC-CV Split Creation]: 356.6 MB\n",
      "2025-11-27 08:36:42,904 - INFO - Running MC-CV analysis...\n",
      "2025-11-27 08:36:42,904 - INFO - Memory usage [Before MC-CV Execution]: 356.6 MB\n",
      "2025-11-27 08:36:42,904 - INFO - Skipping catboost: results already exist locally (3_feature_importance/outputs/opioid_ed_0_12_catboost_feature_importance.csv)\n",
      "2025-11-27 08:36:42,906 - INFO - Loaded existing catboost results from local file: 549 features\n",
      "2025-11-27 08:36:42,921 - INFO - Skipping random_forest: results already exist locally (3_feature_importance/outputs/opioid_ed_0_12_random_forest_feature_importance.csv)\n",
      "2025-11-27 08:36:42,922 - INFO - Loaded existing random_forest results from local file: 549 features\n",
      "2025-11-27 08:36:42,932 - INFO - Skipping xgboost: results already exist locally (3_feature_importance/outputs/opioid_ed_0_12_xgboost_feature_importance.csv)\n",
      "2025-11-27 08:36:42,933 - INFO - Loaded existing xgboost results from local file: 549 features\n",
      "2025-11-27 08:36:42,945 - INFO - Skipping xgboost_rf: results already exist locally (3_feature_importance/outputs/opioid_ed_0_12_xgboost_rf_feature_importance.csv)\n",
      "2025-11-27 08:36:42,947 - INFO - Loaded existing xgboost_rf results from local file: 549 features\n",
      "2025-11-27 08:36:42,959 - INFO - Skipping lightgbm: results already exist locally (3_feature_importance/outputs/opioid_ed_0_12_lightgbm_feature_importance.csv)\n",
      "2025-11-27 08:36:42,961 - INFO - Loaded existing lightgbm results from local file: 549 features\n",
      "2025-11-27 08:36:42,972 - INFO - Skipping extratrees: results already exist locally (3_feature_importance/outputs/opioid_ed_0_12_extratrees_feature_importance.csv)\n",
      "2025-11-27 08:36:42,973 - INFO - Loaded existing extratrees results from local file: 549 features\n",
      "2025-11-27 08:36:42,984 - INFO - Skipping logistic_regression: results already exist locally (3_feature_importance/outputs/opioid_ed_0_12_logistic_regression_feature_importance.csv)\n",
      "2025-11-27 08:36:42,985 - INFO - Loaded existing logistic_regression results from local file: 549 features\n",
      "2025-11-27 08:36:42,996 - INFO - Skipping linearsvc: results already exist locally (3_feature_importance/outputs/opioid_ed_0_12_linearsvc_feature_importance.csv)\n",
      "2025-11-27 08:36:42,998 - INFO - Loaded existing linearsvc results from local file: 549 features\n",
      "2025-11-27 08:36:43,013 - INFO - Skipping elasticnet: results already exist locally (3_feature_importance/outputs/opioid_ed_0_12_elasticnet_feature_importance.csv)\n",
      "2025-11-27 08:36:43,014 - INFO - Loaded existing elasticnet results from local file: 549 features\n",
      "2025-11-27 08:36:43,029 - INFO - Skipping lasso: results already exist locally (3_feature_importance/outputs/opioid_ed_0_12_lasso_feature_importance.csv)\n",
      "2025-11-27 08:36:43,030 - INFO - Loaded existing lasso results from local file: 549 features\n",
      "2025-11-27 08:36:43,042 - INFO - Memory usage [After MC-CV Execution]: 357.6 MB\n",
      "2025-11-27 08:36:43,042 - INFO - Aggregating results...\n",
      "2025-11-27 08:36:43,042 - INFO - Best model: linearsvc with recall=0.2177\n",
      "2025-11-27 08:36:43,062 - INFO - Saved aggregated results locally: 3_feature_importance/outputs/opioid_ed_0_12_aggregated_feature_importance.csv\n",
      "2025-11-27 08:36:43,107 - INFO - Uploaded aggregated results to S3: s3://pgxdatalake/gold/feature_importance/opioid_ed/0-12/aggregated_feature_importance.csv\n",
      "2025-11-27 08:36:43,107 - INFO - Saving logs to S3...\n",
      "\n",
      "[SUCCESS] Analysis complete!\n",
      "  Aggregated output: 3_feature_importance/outputs/opioid_ed_0_12_aggregated_feature_importance.csv\n",
      "  Features analyzed: N/A\n",
      "\n",
      "  Individual model results saved to: 3_feature_importance/outputs/\n",
      "  All results uploaded to: s3://pgxdatalake/gold/feature_importance/opioid_ed/0-12/\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CompletedProcess(args=['/home/pgx3874/jupyter-env/bin/python3.11', '3_feature_importance/run_cohort_1_0_12.py'], returncode=0)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cohort 1, Age 0-12\n",
    "\n",
    "import subprocess\n",
    "\n",
    "subprocess.run(\n",
    "    [str(PYTHON_BIN), \"3_feature_importance/run_cohort_1_0_12.py\"],\n",
    "    cwd=PROJECT_ROOT,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cohort 1 â€“ Age 13â€“24\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running feature importance for opioid_ed / 13-24\n",
      "2025-11-27 08:36:55,989 - INFO - ================================================================================\n",
      "2025-11-27 08:36:55,989 - INFO - FEATURE IMPORTANCE ANALYSIS - MONTE CARLO CROSS-VALIDATION\n",
      "2025-11-27 08:36:55,989 - INFO - ================================================================================\n",
      "2025-11-27 08:36:55,989 - INFO - Cohort: opioid_ed\n",
      "2025-11-27 08:36:55,989 - INFO - Age Band: 13-24\n",
      "2025-11-27 08:36:55,989 - INFO - Train Years: 2016, 2017, 2018\n",
      "2025-11-27 08:36:55,989 - INFO - Test Year: 2019\n",
      "2025-11-27 08:36:55,989 - INFO - MC-CV Splits: 200\n",
      "2025-11-27 08:36:55,989 - INFO - Scaling Metric: recall\n",
      "2025-11-27 08:36:55,989 - INFO - Debug Mode: Disabled\n",
      "2025-11-27 08:36:55,989 - INFO - ================================================================================\n",
      "2025-11-27 08:36:55,989 - INFO - Loading cohort data...\n",
      "2025-11-27 08:36:55,990 - INFO - Memory usage [Before Data Loading]: 336.1 MB\n",
      "2025-11-27 08:36:55,990 - INFO - Loading training data from years: 2016, 2017, 2018\n",
      "2025-11-27 08:36:56,150 - INFO - Loaded 236568 records from year 2016\n",
      "2025-11-27 08:36:56,229 - INFO - Loaded 116367 records from year 2017\n",
      "2025-11-27 08:36:56,283 - INFO - Loaded 83047 records from year 2018\n",
      "2025-11-27 08:36:56,332 - INFO - Combined training data: 435982 event-level records, 9834 unique patients\n",
      "2025-11-27 08:36:56,332 - INFO - Loading test data from year: 2019\n",
      "2025-11-27 08:36:56,456 - INFO - Test data: 176151 event-level records, 3840 unique patients\n",
      "2025-11-27 08:36:56,456 - INFO - Memory usage [After Data Loading]: 563.0 MB\n",
      "2025-11-27 08:36:56,456 - INFO - Creating patient-level features for training data...\n",
      "2025-11-27 08:36:57,100 - INFO - Creating patient-level features for test data...\n",
      "2025-11-27 08:36:57,343 - INFO - Total unique items across train and test: 12557\n",
      "2025-11-27 08:36:57,345 - INFO - Building CatBoost feature matrix: 9834 patients Ã— 12557 features\n"
     ]
    }
   ],
   "source": [
    "# Cohort 1, Age 13-24\n",
    "# Medium cohort: ~9,800 patients Ã— 12,500+ features\n",
    "\n",
    "import subprocess\n",
    "\n",
    "subprocess.run(\n",
    "    [str(PYTHON_BIN), \"3_feature_importance/run_cohort_1_13_24.py\"],\n",
    "    cwd=PROJECT_ROOT,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cohort 1 â€“ Age 25â€“44\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cohort 1, Age 25-44\n",
    "# Large cohort: ~78,000 patients Ã— 32,000+ features\n",
    "\n",
    "import subprocess\n",
    "\n",
    "subprocess.run(\n",
    "    [str(PYTHON_BIN), \"3_feature_importance/run_cohort_1_25_44.py\"],\n",
    "    cwd=PROJECT_ROOT,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cohort 1 â€“ Age 45â€“54\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cohort 1, Age 45-54\n",
    "\n",
    "import subprocess\n",
    "\n",
    "subprocess.run(\n",
    "    [str(PYTHON_BIN), \"3_feature_importance/run_cohort_1_45_54.py\"],\n",
    "    cwd=PROJECT_ROOT,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cohort 1 â€“ Age 55â€“64\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cohort 1, Age 55-64\n",
    "\n",
    "import subprocess\n",
    "\n",
    "subprocess.run(\n",
    "    [str(PYTHON_BIN), \"3_feature_importance/run_cohort_1_55_64.py\"],\n",
    "    cwd=PROJECT_ROOT,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cohort 1 â€“ Age 65â€“74\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cohort 1, Age 65-74\n",
    "\n",
    "import subprocess\n",
    "\n",
    "subprocess.run(\n",
    "    [str(PYTHON_BIN), \"3_feature_importance/run_cohort_1_65_74.py\"],\n",
    "    cwd=PROJECT_ROOT,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cohort 1 â€“ Age 75â€“84\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cohort 1, Age 75-84\n",
    "# Note: Feature matrix creation and MC-CV splits run in parallel using all available cores\n",
    "\n",
    "import subprocess\n",
    "\n",
    "subprocess.run(\n",
    "    [str(PYTHON_BIN), \"3_feature_importance/run_cohort_1_75_84.py\"],\n",
    "    cwd=PROJECT_ROOT,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cohort 1 â€“ Age 85â€“94\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cohort 1, Age 85-94\n",
    "# Note: Feature matrix creation and MC-CV splits run in parallel using all available cores\n",
    "\n",
    "import subprocess\n",
    "\n",
    "subprocess.run(\n",
    "    [str(PYTHON_BIN), \"3_feature_importance/run_cohort_1_85_94.py\"],\n",
    "    cwd=PROJECT_ROOT,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cohort 1 â€“ Age 95â€“114\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cohort 1, Age 95-114\n",
    "# Note: Feature matrix creation and MC-CV splits run in parallel using all available cores\n",
    "\n",
    "import subprocess\n",
    "\n",
    "subprocess.run(\n",
    "    [str(PYTHON_BIN), \"3_feature_importance/run_cohort_1_95_114.py\"],\n",
    "    cwd=PROJECT_ROOT,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cohort 2 â€“ Age 0â€“12\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cohort 2, Age 0-12 (e.g., non-opioid_ed)\n",
    "\n",
    "import subprocess\n",
    "\n",
    "subprocess.run(\n",
    "    [str(PYTHON_BIN), \"3_feature_importance/run_cohort_2_0_12.py\"],\n",
    "    cwd=PROJECT_ROOT,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cohort 2 â€“ Age 13â€“24\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cohort 2, Age 13-24\n",
    "\n",
    "import subprocess\n",
    "\n",
    "subprocess.run(\n",
    "    [str(PYTHON_BIN), \"3_feature_importance/run_cohort_2_13_24.py\"],\n",
    "    cwd=PROJECT_ROOT,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cohort 2 â€“ Age 25â€“44\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cohort 2, Age 25-44\n",
    "# Large cohort: Similar size to Cohort 1, Age 25-44\n",
    "\n",
    "import subprocess\n",
    "\n",
    "subprocess.run(\n",
    "    [str(PYTHON_BIN), \"3_feature_importance/run_cohort_2_25_44.py\"],\n",
    "    cwd=PROJECT_ROOT,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cohort 2 â€“ Age 45â€“54\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cohort 2, Age 45-54\n",
    "\n",
    "import subprocess\n",
    "\n",
    "subprocess.run(\n",
    "    [str(PYTHON_BIN), \"3_feature_importance/run_cohort_2_45_54.py\"],\n",
    "    cwd=PROJECT_ROOT,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cohort 2 â€“ Age 55â€“64\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cohort 2, Age 55-64\n",
    "\n",
    "import subprocess\n",
    "\n",
    "subprocess.run(\n",
    "    [str(PYTHON_BIN), \"3_feature_importance/run_cohort_2_55_64.py\"],\n",
    "    cwd=PROJECT_ROOT,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cohort 2 â€“ Age 65â€“74\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cohort 2, Age 65-74\n",
    "\n",
    "import subprocess\n",
    "\n",
    "subprocess.run(\n",
    "    [str(PYTHON_BIN), \"3_feature_importance/run_cohort_2_65_74.py\"],\n",
    "    cwd=PROJECT_ROOT,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cohort 2 â€“ Age 75â€“84\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cohort 2, Age 75-84\n",
    "\n",
    "import subprocess\n",
    "\n",
    "subprocess.run(\n",
    "    [str(PYTHON_BIN), \"3_feature_importance/run_cohort_2_75_84.py\"],\n",
    "    cwd=PROJECT_ROOT,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cohort 2 â€“ Age 85â€“94\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cohort 2, Age 85-94\n",
    "\n",
    "import subprocess\n",
    "\n",
    "subprocess.run(\n",
    "    [str(PYTHON_BIN), \"3_feature_importance/run_cohort_2_85_94.py\"],\n",
    "    cwd=PROJECT_ROOT,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cohort 2 â€“ Age 95â€“114\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cohort 2, Age 95-114\n",
    "\n",
    "import subprocess\n",
    "\n",
    "subprocess.run(\n",
    "    [str(PYTHON_BIN), \"3_feature_importance/run_cohort_2_95_114.py\"],\n",
    "    cwd=PROJECT_ROOT,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run All Cohorts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run All Age Bands in Parallel (Per Cohort)\n",
    "\n",
    "Use this section to run all **9 age bands** for a given cohort in parallel on EC2.\n",
    "\n",
    "- Uses `ThreadPoolExecutor` to launch multiple `run_cohort_*` scripts concurrently\n",
    "- Each script remains idempotent (skips models with existing results in S3)\n",
    "- Adjust `MAX_PARALLEL_AGE_BANDS` based on available CPU/memory\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "# Configuration: which cohort to run in parallel\n",
    "# 1 => opioid_ed (run_cohort_1_*.py)\n",
    "# 2 => non_opioid_ed (run_cohort_2_*.py)\n",
    "COHORT_ID = 1\n",
    "\n",
    "# Age-band suffixes used in the script filenames\n",
    "AGE_BAND_SUFFIXES = [\n",
    "    \"0_12\",\n",
    "    \"13_24\",\n",
    "    \"25_44\",\n",
    "    \"45_54\",\n",
    "    \"55_64\",\n",
    "    \"65_74\",\n",
    "    \"75_84\",\n",
    "    \"85_94\",\n",
    "    \"95_114\",\n",
    "]\n",
    "\n",
    "MAX_PARALLEL_AGE_BANDS = 9  # Set lower (e.g., 3-4) if memory is tight\n",
    "FAIL_FAST = True\n",
    "\n",
    "\n",
    "def run_age_band_script(script_rel: str) -> int:\n",
    "    \"\"\"Run a single cohort age-band script and return its exit code.\"\"\"\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"[PARALLEL] Starting: {script_rel}\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    result = subprocess.run(\n",
    "        [str(PYTHON_BIN), script_rel],\n",
    "        cwd=PROJECT_ROOT,\n",
    "    )\n",
    "\n",
    "    if result.returncode == 0:\n",
    "        print(f\"[PARALLEL] COMPLETED: {script_rel}\")\n",
    "    else:\n",
    "        print(f\"[PARALLEL] FAILED ({result.returncode}): {script_rel}\")\n",
    "    return result.returncode\n",
    "\n",
    "\n",
    "scripts = [\n",
    "    f\"3_feature_importance/run_cohort_{COHORT_ID}_{suffix}.py\"\n",
    "    for suffix in AGE_BAND_SUFFIXES\n",
    "]\n",
    "\n",
    "print(f\"Running {len(scripts)} age bands in parallel for cohort ID {COHORT_ID}...\")\n",
    "\n",
    "errors = []\n",
    "with ThreadPoolExecutor(max_workers=MAX_PARALLEL_AGE_BANDS) as executor:\n",
    "    future_to_script = {\n",
    "        executor.submit(run_age_band_script, script): script\n",
    "        for script in scripts\n",
    "    }\n",
    "\n",
    "    for future in as_completed(future_to_script):\n",
    "        script = future_to_script[future]\n",
    "        try:\n",
    "            code = future.result()\n",
    "        except Exception as exc:\n",
    "            print(f\"[PARALLEL] EXCEPTION in {script}: {exc}\")\n",
    "            errors.append((script, str(exc)))\n",
    "            if FAIL_FAST:\n",
    "                break\n",
    "        else:\n",
    "            if code != 0:\n",
    "                errors.append((script, f\"exit code {code}\"))\n",
    "                if FAIL_FAST:\n",
    "                    break\n",
    "\n",
    "if errors:\n",
    "    print(\"\\nOne or more age-band runs failed:\")\n",
    "    for script, msg in errors:\n",
    "        print(f\"  - {script}: {msg}\")\n",
    "else:\n",
    "    print(\"\\nAll age bands completed successfully (or were skipped as already done).\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run all cohort scripts sequentially\n",
    "# Each script is idempotent and will skip work if results already exist in S3.\n",
    "\n",
    "FAIL_FAST = True  # Stop on first failure; set to False to continue on errors\n",
    "\n",
    "for script in COHORT_SCRIPTS:\n",
    "    rel_path = script.relative_to(PROJECT_ROOT)\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"Running cohort script: {rel_path}\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    result = subprocess.run(\n",
    "        [str(PYTHON_BIN), str(rel_path)],\n",
    "        cwd=PROJECT_ROOT,\n",
    "    )\n",
    "\n",
    "    if result.returncode != 0:\n",
    "        msg = f\"Script {rel_path} failed with exit code {result.returncode}\"\n",
    "        print(msg)\n",
    "        if FAIL_FAST:\n",
    "            raise RuntimeError(msg)\n",
    "\n",
    "print(\"\\nAll cohort scripts completed (or were skipped as already done).\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sync Results and Code to S3\n",
    "\n",
    "Sync output files and code (notebook + R script) to S3 bucket. \n",
    "- Outputs: CSV results files\n",
    "- Code: Notebook and R script for reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Sync outputs and code to S3\n",
    "# On EC2, we're in the feature_importance directory  \n",
    "s3_bucket <- \"s3://pgx-repository/pgx-analysis/3_feature_importance/\"\n",
    "\n",
    "# Find AWS CLI (check common locations - EC2 typically has it in /usr/local/bin or /usr/bin)\n",
    "aws_cmd <- Sys.which(\"aws\")\n",
    "if (aws_cmd == \"\") {\n",
    "  # Try common EC2 installation paths\n",
    "  aws_paths <- c(\n",
    "    \"/usr/local/bin/aws\",\n",
    "    \"/usr/bin/aws\",\n",
    "    \"/home/ec2-user/.local/bin/aws\"\n",
    "  )\n",
    "  aws_cmd <- NULL\n",
    "  for (path in aws_paths) {\n",
    "    if (file.exists(path)) {\n",
    "      aws_cmd <- path\n",
    "      break\n",
    "    }\n",
    "  }\n",
    "  if (is.null(aws_cmd)) {\n",
    "    stop(\"AWS CLI not found. Please install AWS CLI or ensure it's in your PATH.\")\n",
    "  }\n",
    "}\n",
    "\n",
    "cat(\"Syncing outputs and code to S3...\\n\")\n",
    "cat(\"Source: feature_importance/ directory\\n\")\n",
    "cat(\"Destination:\", s3_bucket, \"\\n\")\n",
    "cat(\"AWS CLI:\", aws_cmd, \"\\n\\n\")\n",
    "\n",
    "# Get current directory (should be feature_importance)\n",
    "current_dir <- getwd()\n",
    "if (!grepl(\"feature_importance\", current_dir)) {\n",
    "  warning(\"Current directory doesn't appear to be feature_importance. Double-check sync destination.\")\n",
    "}\n",
    "\n",
    "# Sync feature_importance directory (includes outputs/ and code files)\n",
    "# Explicitly include notebook, R scripts, README files, and outputs directory\n",
    "# Exclude temporary files, checkpoints, and unnecessary directories\n",
    "# Note: --delete flag removed for safety (won't delete files in S3 that don't exist locally)\n",
    "# Include patterns are processed before exclude patterns, then exclude everything else\n",
    "sync_cmd <- sprintf(\n",
    "  '\"%s\" s3 sync \"%s\" %s --include \"*.ipynb\" --include \"*.R\" --include \"README*.md\" --include \"outputs/**\" --exclude \"*checkpoint*\" --exclude \"*.tmp\" --exclude \"*.ipynb_checkpoints/*\" --exclude \"*.RData\" --exclude \"*.Rhistory\" --exclude \".Rproj.user/*\" --exclude \"catboost_info/*\" --exclude \"*.log\" --exclude \"*\"',\n",
    "  aws_cmd,\n",
    "  current_dir,\n",
    "  s3_bucket\n",
    ")\n",
    "\n",
    "cat(\"Running:\", sync_cmd, \"\\n\\n\")\n",
    "result <- system(sync_cmd)\n",
    "\n",
    "if (result == 0) {\n",
    "  cat(\"âœ“ Successfully synced outputs and code to S3\\n\")\n",
    "  cat(\"  - Outputs:\", file.path(output_dir), \"\\n\")\n",
    "  cat(\"  - Code: *.ipynb, *.R, README*.md\\n\")\n",
    "} else {\n",
    "  warning(sprintf(\"S3 sync returned exit code %d. Check AWS credentials and permissions.\", result))\n",
    "}\n",
    "\n",
    "# ============================================================\n",
    "# SAVE LOGS TO S3 (aligned with 2_create_cohort)\n",
    "# ============================================================\n",
    "cat(\"\\n========================================\\n\")\n",
    "cat(\"Saving logs to S3...\\n\")\n",
    "cat(\"========================================\\n\")\n",
    "\n",
    "# Close log file connection\n",
    "if (exists(\"log_setup\") && !is.null(log_setup$log_connection)) {\n",
    "  if (isOpen(log_setup$log_connection)) {\n",
    "    close(log_setup$log_connection)\n",
    "  }\n",
    "}\n",
    "\n",
    "# Save logs to S3\n",
    "if (exists(\"logger\") && exists(\"log_file_path\")) {\n",
    "  tryCatch({\n",
    "    s3_path <- save_logs_to_s3_r(log_file_path, COHORT_NAME, AGE_BAND, EVENT_YEAR, logger)\n",
    "    if (!is.null(s3_path)) {\n",
    "      logger$info(\"âœ“ Analysis completed successfully. Logs saved to S3.\")\n",
    "    }\n",
    "  }, error = function(e) {\n",
    "    cat(sprintf(\"Warning: Could not save logs to S3: %s\\n\", e$message))\n",
    "    cat(sprintf(\"Log file saved locally: %s\\n\", log_file_path))\n",
    "  })\n",
    "} else {\n",
    "  cat(\"Warning: Logger not initialized. Logs not saved to S3.\\n\")\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shutdown EC2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Shutdown EC2 instance after analysis completes\n",
    "# Set SHUTDOWN_EC2 = TRUE to enable, FALSE to disable\n",
    "SHUTDOWN_EC2 <- TRUE  # Change to TRUE to enable auto-shutdown\n",
    "\n",
    "if (SHUTDOWN_EC2) {\n",
    "  cat(\"\\n========================================\\n\")\n",
    "  cat(\"Shutting down EC2 instance...\\n\")\n",
    "  cat(\"========================================\\n\")\n",
    "  \n",
    "  # Get instance ID from EC2 metadata service\n",
    "  instance_id <- tryCatch({\n",
    "    system(\"curl -s http://169.254.169.254/latest/meta-data/instance-id\", intern = TRUE)\n",
    "  }, error = function(e) {\n",
    "    cat(\"Warning: Could not retrieve instance ID from metadata service.\\n\")\n",
    "    cat(\"If running on EC2, check that metadata service is accessible.\\n\")\n",
    "    return(NULL)\n",
    "  })\n",
    "  \n",
    "  if (!is.null(instance_id) && length(instance_id) > 0 && nchar(instance_id[1]) > 0) {\n",
    "    instance_id <- instance_id[1]\n",
    "    cat(sprintf(\"Instance ID: %s\\n\", instance_id))\n",
    "    \n",
    "    # Find AWS CLI\n",
    "    aws_cmd <- Sys.which(\"aws\")\n",
    "    if (aws_cmd == \"\") {\n",
    "      aws_paths <- c(\n",
    "        \"/usr/local/bin/aws\",\n",
    "        \"/usr/bin/aws\",\n",
    "        \"/home/ec2-user/.local/bin/aws\"\n",
    "      )\n",
    "      aws_cmd <- NULL\n",
    "      for (path in aws_paths) {\n",
    "        if (file.exists(path)) {\n",
    "          aws_cmd <- path\n",
    "          break\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "    \n",
    "    if (!is.null(aws_cmd) && aws_cmd != \"\") {\n",
    "      # Stop the instance (use terminate-instances for permanent deletion)\n",
    "      shutdown_cmd <- sprintf(\n",
    "        '\"%s\" ec2 stop-instances --instance-ids %s',\n",
    "        aws_cmd,\n",
    "        instance_id\n",
    "      )\n",
    "      \n",
    "      cat(\"Running:\", shutdown_cmd, \"\\n\")\n",
    "      result <- system(shutdown_cmd)\n",
    "      \n",
    "      if (result == 0) {\n",
    "        cat(\"âœ“ EC2 instance stop command sent successfully\\n\")\n",
    "        cat(\"Instance will stop in a few moments.\\n\")\n",
    "        cat(\"Note: This is a STOP (not terminate), so you can restart it later.\\n\")\n",
    "      } else {\n",
    "        warning(sprintf(\"EC2 stop command returned exit code %d. Check AWS credentials and permissions.\", result))\n",
    "      }\n",
    "    } else {\n",
    "      cat(\"Warning: AWS CLI not found. Cannot shutdown instance.\\n\")\n",
    "      cat(\"Install AWS CLI or ensure it's in your PATH.\\n\")\n",
    "    }\n",
    "  } else {\n",
    "    cat(\"Warning: Could not determine instance ID. Skipping shutdown.\\n\")\n",
    "    cat(\"If you want to shutdown manually, use:\\n\")\n",
    "    cat(\"  aws ec2 stop-instances --instance-ids <your-instance-id>\\n\")\n",
    "  }\n",
    "} else {\n",
    "  cat(\"\\n========================================\\n\")\n",
    "  cat(\"EC2 Auto-Shutdown: DISABLED\\n\")\n",
    "  cat(\"========================================\\n\")\n",
    "  cat(\"To enable auto-shutdown, set SHUTDOWN_EC2 = TRUE in this cell.\\n\")\n",
    "  cat(\"Instance will continue running.\\n\")\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
