{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "59bb209f",
   "metadata": {},
   "source": [
    "# Feature Importance with Monte Carlo Cross-Validation\n",
    "\n",
    "**Purpose:** Calculate scaled feature importance using CatBoost and Random Forest  \n",
    "**Method:** Normalized feature importance scaled by MC-CV Recall scores  \n",
    "**Based on:** [R Example](https://github.com/Jerome3590/phts/blob/main/graft-loss/feature_importance/replicate_20_features_MC_CV.R)  \n",
    "**Updated:** November 2025  \n",
    "**Hardware:** Optimized for EC2 (32 cores, 1TB RAM)  \n",
    "**Validation:** Proper evaluation on unseen test data\n",
    "\n",
    "## Key Features\n",
    "\n",
    "‚úÖ **Monte Carlo Cross-Validation** ‚Äì up to 1000 random train/test splits (100-split runs used for faster iteration)  \n",
    "‚úÖ **Stratified Sampling** - Maintains target distribution  \n",
    "‚úÖ **Parallel Processing** - Fast execution with furrr/future (‚âà30 workers)  \n",
    "‚úÖ **95% Confidence Intervals** - Narrow, precise estimates (tighter with more splits)  \n",
    "‚úÖ **Multiple Models** - CatBoost (R) and Random Forest (R)  \n",
    "\n",
    "## Methodology\n",
    "\n",
    "This notebook implements the feature selection methodology:\n",
    "\n",
    "1. Load cohort data from parquet files (same as FP-Growth notebook)\n",
    "2. Create patient-level features (one-hot encoding of items)\n",
    "3. For each model type:\n",
    "   - Create 100‚Äì1000 stratified train/test splits\n",
    "   - Train model on training set\n",
    "   - Evaluate Recall on unseen test set\n",
    "   - Extract feature importance\n",
    "   - Aggregate results across splits\n",
    "4. Normalize and scale feature importance by MC-CV Recall\n",
    "5. Aggregate across models\n",
    "6. Extract top features\n",
    "\n",
    "## Expected Runtime\n",
    "\n",
    "- **100 splits (current default):**\n",
    "  - Local (4 cores): ~2‚Äì4 hours\n",
    "  - Workstation (16 cores): ~1‚Äì2 hours\n",
    "  - EC2 (32 cores, 1TB RAM): ~1‚Äì2 hours ‚úÖ **RECOMMENDED FOR DEVELOPMENT**\n",
    "- **1000 splits (extended / publication-level):**\n",
    "  - Local (4 cores): 8‚Äì12+ hours\n",
    "  - Workstation (16 cores): ~8‚Äì16 hours\n",
    "  - EC2 (32 cores, 1TB RAM): ~10‚Äì20 hours ‚úÖ **RECOMMENDED FOR FINAL RESULTS**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26eb9c0a",
   "metadata": {},
   "source": [
    "## 1. Setup and Configuration\n",
    "\n",
    "Load required packages and configure parallel processing.\n",
    "\n",
    "**üìñ Documentation:** See [Feature Importance README](/docs/README_feature_importance.md) for detailed documentation, usage examples, and troubleshooting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0617449b",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "'R version 4.4.3 (2025-02-28)'"
      ],
      "text/latex": [
       "'R version 4.4.3 (2025-02-28)'"
      ],
      "text/markdown": [
       "'R version 4.4.3 (2025-02-28)'"
      ],
      "text/plain": [
       "[1] \"R version 4.4.3 (2025-02-28)\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì All packages loaded successfully\n",
      "‚úì All helper functions loaded from R scripts\n",
      "‚úì Age bands loaded from constants: 0-12, 13-24, 25-44, 45-54, 55-64, 65-74, 75-84, 85-94, 95-114\n",
      "‚úì Cohorts loaded from constants: opioid_ed, non_opioid_ed\n"
     ]
    }
   ],
   "source": [
    "# Check R version\n",
    "R.version.string\n",
    "\n",
    "# Load required packages\n",
    "suppressPackageStartupMessages({\n",
    "  library(here)\n",
    "  library(dplyr)\n",
    "  library(readr)\n",
    "  library(tidyr)\n",
    "  library(tibble)\n",
    "  library(purrr)\n",
    "  library(catboost)\n",
    "  library(randomForest)\n",
    "  library(rsample)    # For MC-CV\n",
    "  library(furrr)      # For parallel processing\n",
    "  library(future)     # For parallel backend\n",
    "  library(progressr)  # For progress bars\n",
    "  library(duckdb)     # For loading parquet files\n",
    "  library(DBI)        # Database interface for DuckDB\n",
    "})\n",
    "\n",
    "cat(\"‚úì All packages loaded successfully\\n\")\n",
    "\n",
    "# ============================================================\n",
    "# SOURCE HELPER FUNCTIONS (Modular R Scripts)\n",
    "# ============================================================\n",
    "helpers_dir <- here(\"helpers_13_1997\")\n",
    "if (!dir.exists(helpers_dir)) {\n",
    "  stop(sprintf(\"Helpers directory not found: %s\", helpers_dir))\n",
    "}\n",
    "\n",
    "# Source constants first (needed by other scripts)\n",
    "source(file.path(helpers_dir, \"constants.R\"))\n",
    "\n",
    "# Source all helper scripts\n",
    "source(file.path(helpers_dir, \"logging_utils.R\"))\n",
    "source(file.path(helpers_dir, \"metrics.R\"))\n",
    "source(file.path(helpers_dir, \"model_helpers.R\"))\n",
    "source(file.path(helpers_dir, \"mc_cv_helpers.R\"))\n",
    "source(file.path(helpers_dir, \"run_cohort_analysis.R\"))\n",
    "\n",
    "cat(\"‚úì All helper functions loaded from R scripts\\n\")\n",
    "cat(sprintf(\"‚úì Age bands loaded from constants: %s\\n\", paste(AGE_BANDS, collapse = \", \")))\n",
    "cat(sprintf(\"‚úì Cohorts loaded from constants: %s\\n\", paste(COHORT_NAMES, collapse = \", \")))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "785bdbc1",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Auto-detected 32 cores, using 30 workers\n",
      "Setting up parallel processing with 30 workers...\n",
      "Set future.globals.maxSize to 97 GB\n",
      "Output directory: /home/pgx3874/pgx-analysis/3_feature_importance/outputs \n",
      "MC-CV Configuration: 200 splits, 80/20 train/test split\n",
      "Cohorts to process: opioid_ed, non_opioid_ed\n",
      "Running 2 cohort(s) in parallel\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# DEBUG/TEST MODE - Quick testing before full run\n",
    "# ============================================================\n",
    "# Set DEBUG_MODE = TRUE for quick testing (5 splits, ~2-5 min)\n",
    "# Set DEBUG_MODE = FALSE for full analysis (100 splits, ~1-2 hours on EC2)\n",
    "\n",
    "DEBUG_MODE <- FALSE  # Change to TRUE for quick test\n",
    "\n",
    "if (DEBUG_MODE) {\n",
    "  cat(\"\\n\")\n",
    "  cat(\"‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó\\n\")\n",
    "  cat(\"‚ïë                    üîç DEBUG MODE ENABLED                       ‚ïë\\n\")\n",
    "  cat(\"‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù\\n\")\n",
    "  cat(\"\\n\")\n",
    "  cat(\"Quick test configuration:\\n\")\n",
    "  cat(\"  ‚Ä¢ MC-CV Splits: 5 (instead of 100)\\n\")\n",
    "  cat(\"  ‚Ä¢ Expected time: 2-5 minutes\\n\")\n",
    "  cat(\"  ‚Ä¢ Purpose: Verify everything works before full run\\n\")\n",
    "  cat(\"\\n\")\n",
    "  cat(\"To run full analysis, set DEBUG_MODE = FALSE\\n\")\n",
    "  cat(\"\\n\")\n",
    "}\n",
    "\n",
    "# Configuration\n",
    "# NOTE: Target definition differs by cohort (already baked into cohort data via is_target_case):\n",
    "#   - \"opioid_ed\": Target cases = patients with F1120/opioid ICD codes (any of 10 ICD columns)\n",
    "#   - \"non_opioid_ed\": Target cases = patients with HCG ED visits (P51/O11/P33) WITHOUT opioid codes\n",
    "# Controls (is_target_case=0) are sampled to maintain 5:1 ratio in both cohorts\n",
    "\n",
    "# Configuration\n",
    "# NOTE: AGE_BANDS and COHORT_NAMES are loaded from constants.R (sourced in Cell 2)\n",
    "# To override, uncomment and modify:\n",
    "# AGE_BANDS <- c(\"25-44\")  # Single age-band for testing\n",
    "# COHORT_NAMES <- c(\"opioid_ed\")  # Single cohort for testing\n",
    "\n",
    "EVENT_YEAR <- 2016          # Change as needed\n",
    "\n",
    "# For backward compatibility, set AGE_BAND to first value (used in single-cohort execution)\n",
    "AGE_BAND <- if (length(AGE_BANDS) == 1) AGE_BANDS else AGE_BANDS[1]\n",
    "\n",
    "# For backward compatibility, also set COHORT_NAME (will be overridden in function)\n",
    "COHORT_NAME <- COHORT_NAMES[1]\n",
    "\n",
    "N_SPLITS <- if (DEBUG_MODE) 5 else 200  # MC-CV splits (5 for debug, 100 for development, 1000 for production)\n",
    "TEST_SIZE <- 0.2             # Test set proportion (20%)\n",
    "TRAIN_PROP <- 1 - TEST_SIZE  # Training proportion (80%)\n",
    "\n",
    "# Scaling metric for feature importance\n",
    "# Options: \"recall\" (default) or \"logloss\"\n",
    "# - Recall: Higher is better (0-1), good for imbalanced classes, focuses on finding positives\n",
    "# - LogLoss: Lower is better, measures probability calibration, penalizes overconfident errors\n",
    "#   (will be inverted: 1/logloss for scaling, so higher = better)\n",
    "SCALING_METRIC <- \"recall\"  # Change to \"logloss\" if preferred\n",
    "\n",
    "# Model parameters\n",
    "MODEL_PARAMS <- list(\n",
    "  catboost = list(\n",
    "    iterations = 100,\n",
    "    learning_rate = 0.1,\n",
    "    depth = 6,\n",
    "    verbose = 0L,  # Turn off CatBoost logging (0L = integer 0)\n",
    "    random_seed = 42\n",
    "  ),\n",
    "  random_forest = list(\n",
    "    ntree = 100,\n",
    "    mtry = NULL,  # Will be set to sqrt(n_features)\n",
    "    nodesize = 1,\n",
    "    maxnodes = NULL\n",
    "  )\n",
    ")\n",
    "\n",
    "# Set up parallel processing\n",
    "# EC2 optimization: Use 30 out of 32 cores (leave 2 for system)\n",
    "N_WORKERS <- as.integer(Sys.getenv(\"N_WORKERS\", \"0\"))\n",
    "if (N_WORKERS < 1) {\n",
    "  # Auto-detect: use all cores minus 2 for system\n",
    "  total_cores <- parallel::detectCores()\n",
    "  N_WORKERS <- max(1, total_cores - 2)\n",
    "  cat(sprintf(\"Auto-detected %d cores, using %d workers\\n\", total_cores, N_WORKERS))\n",
    "}\n",
    "cat(sprintf(\"Setting up parallel processing with %d workers...\\n\", N_WORKERS))\n",
    "\n",
    "# Increase future.globals.maxSize for large data objects\n",
    "# With 1TB RAM on EC2, we can handle large transfers\n",
    "# Note: After refactoring, we use lightweight split_indices instead of large mc_splits\n",
    "# Still need space for X_all, y_all, and data_full matrices\n",
    "options(future.globals.maxSize = 97 * 1024^3)  # 97 GB limit\n",
    "cat(\"Set future.globals.maxSize to 97 GB\\n\")\n",
    "\n",
    "# Set up parallel processing plan\n",
    "# Use N_WORKERS for MC-CV within each cohort\n",
    "plan(multisession, workers = N_WORKERS)\n",
    "\n",
    "# Output directory\n",
    "output_dir <- here(\"outputs\")\n",
    "dir.create(output_dir, showWarnings = FALSE, recursive = TRUE)\n",
    "\n",
    "# Clean up any incomplete/checkpoint files from previous runs\n",
    "# Only keep files that match the expected pattern and are complete\n",
    "if (dir.exists(output_dir)) {\n",
    "  existing_files <- list.files(output_dir, pattern = \"feature_importance.*\\\\.csv$\", full.names = TRUE)\n",
    "  if (length(existing_files) > 0) {\n",
    "    cat(sprintf(\"Found %d existing output files in %s\\n\", length(existing_files), output_dir))\n",
    "    cat(\"Note: These will be overwritten if processing runs. Only S3 files are used for idempotency checks.\\n\")\n",
    "    # Optionally remove incomplete files (uncomment to enable):\n",
    "    # incomplete_patterns <- c(\"_partial\", \"_checkpoint\", \"_tmp\", \"_incomplete\")\n",
    "    # incomplete_files <- existing_files[grepl(paste(incomplete_patterns, collapse = \"|\"), existing_files)]\n",
    "    # if (length(incomplete_files) > 0) {\n",
    "    #   cat(sprintf(\"Removing %d incomplete/checkpoint files...\\n\", length(incomplete_files)))\n",
    "    #   file.remove(incomplete_files)\n",
    "    # }\n",
    "  }\n",
    "}\n",
    "\n",
    "cat(\"Output directory:\", output_dir, \"\\n\")\n",
    "cat(sprintf(\"MC-CV Configuration: %d splits, %.0f/%.0f train/test split\\n\", \n",
    "            N_SPLITS, TRAIN_PROP * 100, TEST_SIZE * 100))\n",
    "cat(sprintf(\"Cohorts to process: %s\\n\", paste(COHORT_NAMES, collapse = \", \")))\n",
    "cat(sprintf(\"Running %d cohort(s) in parallel\\n\", length(COHORT_NAMES)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f70cc30",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Checking for already processed combinations...\n",
      "Checking for cohort files...\n",
      "Total combinations: 18\n",
      "Already processed: 0\n",
      "To process: 18\n",
      "\n",
      "================================================================================\n",
      "Running feature importance analysis\n",
      "================================================================================\n",
      "Cohorts: opioid_ed, non_opioid_ed\n",
      "Age Bands: 0-12, 13-24, 25-44, 45-54, 55-64, 65-74, 75-84, 85-94, 95-114\n",
      "Event Year: 2016\n",
      "Total combinations: 18 (cohorts √ó age-bands)\n",
      "Parallel workers: 18\n",
      "MC-CV workers per task: 30\n",
      "================================================================================\n",
      "\n",
      "\n",
      "Nested Parallelism Configuration:\n",
      "  Available cores: 32\n",
      "  Usable cores (reserved 2 for system): 30\n",
      "  Task-level workers: 7 (running 7 tasks in parallel)\n",
      "  MC-CV workers per task: 3\n",
      "  Total worker allocation: 7 (task-level) + 7 √ó 3 (MC-CV) = 28 workers\n",
      "  (Note: Workers are shared/reused, so actual usage is ~30 concurrent)\n",
      "\n",
      "Setting task-level plan to 7 workers\n",
      "Each task will use 3 workers for MC-CV\n",
      "Current plan: tweaked with unknown workers\n",
      "\n",
      "Starting parallel execution of 18 tasks...\n",
      "Task-level plan: 7 workers\n",
      "MC-CV plan per task: 3 workers\n",
      "All tasks should start simultaneously...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message in serializedSize(x):\n",
      "‚Äú'package:rsample' may not be available when loading‚Äù\n",
      "Warning message in serializedSize(x):\n",
      "‚Äú'package:furrr' may not be available when loading‚Äù\n",
      "Warning message in serializedSize(x):\n",
      "‚Äú'package:progressr' may not be available when loading‚Äù\n",
      "Warning message in serializedSize(x):\n",
      "‚Äú'package:future' may not be available when loading‚Äù\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# CHECK FOR COHORT FILES EXISTENCE\n",
    "# ============================================================\n",
    "# Function to check if cohort file exists before processing\n",
    "check_cohort_file_exists <- function(cohort_name, age_band, event_year) {\n",
    "  # Determine local data path\n",
    "  local_data_path <- Sys.getenv(\"LOCAL_DATA_PATH\", \"/mnt/nvme/cohorts\")\n",
    "  if (!dir.exists(local_data_path)) {\n",
    "    local_data_path <- Sys.getenv(\"LOCAL_DATA_PATH\", \"C:/Projects/pgx-analysis/data/gold/cohorts_F1120\")\n",
    "  }\n",
    "  \n",
    "  parquet_file <- file.path(local_data_path, \n",
    "                            paste0(\"cohort_name=\", cohort_name),\n",
    "                            paste0(\"event_year=\", event_year),\n",
    "                            paste0(\"age_band=\", age_band),\n",
    "                            \"cohort.parquet\")\n",
    "  \n",
    "  return(file.exists(parquet_file))\n",
    "}\n",
    "\n",
    "# ============================================================\n",
    "# CHECK FOR ALREADY PROCESSED COMBINATIONS (IDEMPOTENCY)\n",
    "# ============================================================\n",
    "# Function to check if results already exist (S3 only - final destination)\n",
    "# Note: Only checks S3, not local output_dir which may contain checkpoints or incomplete files\n",
    "check_results_exist <- function(cohort_name, age_band, event_year) {\n",
    "  # Check S3 (final destination) - don't check local output_dir which may have checkpoints\n",
    "  s3_base <- \"s3://pgxdatalake/gold/feature_importance\"\n",
    "  s3_path <- sprintf(\"%s/cohort_name=%s/age_band=%s/event_year=%d/%s_%s_%d_feature_importance_aggregated.csv\",\n",
    "                     s3_base, cohort_name, age_band, event_year,\n",
    "                     cohort_name, age_band, event_year)\n",
    "  \n",
    "  # Use AWS CLI to check if file exists\n",
    "  aws_cmd <- Sys.which(\"aws\")\n",
    "  if (aws_cmd == \"\") {\n",
    "    # Try common AWS CLI paths\n",
    "    aws_paths <- c(\n",
    "      \"/usr/local/bin/aws\",\n",
    "      \"/usr/bin/aws\",\n",
    "      \"/home/ec2-user/.local/bin/aws\",\n",
    "      \"C:/Program Files/Amazon/AWSCLIV2/aws.exe\"\n",
    "    )\n",
    "    for (path in aws_paths) {\n",
    "      if (file.exists(path)) {\n",
    "        aws_cmd <- path\n",
    "        break\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "  \n",
    "  if (aws_cmd != \"\" && file.exists(aws_cmd)) {\n",
    "    # Extract bucket and key from s3:// path\n",
    "    s3_parts <- gsub(\"^s3://\", \"\", s3_path)\n",
    "    parts <- strsplit(s3_parts, \"/\", fixed = TRUE)[[1]]\n",
    "    bucket <- parts[1]\n",
    "    key <- paste(parts[-1], collapse = \"/\")\n",
    "    \n",
    "    # Check if object exists in S3 using s3api head-object (more reliable)\n",
    "    # Suppress warnings for \"file not found\" (status 1 is expected when file doesn't exist)\n",
    "    result <- tryCatch({\n",
    "      # Use head-object: returns 0 if file exists, non-zero if not found\n",
    "      # Suppress stderr to avoid warnings\n",
    "      exit_code <- suppressWarnings(\n",
    "        system2(aws_cmd, \n",
    "                c(\"s3api\", \"head-object\", \n",
    "                  \"--bucket\", bucket,\n",
    "                  \"--key\", key),\n",
    "                stdout = FALSE,  # Don't need stdout\n",
    "                stderr = FALSE,  # Suppress stderr to avoid warnings\n",
    "                wait = TRUE)\n",
    "      )\n",
    "      \n",
    "      # system2 returns exit code directly when wait=TRUE\n",
    "      # 0 = success (file exists), non-zero = file doesn't exist or error\n",
    "      exit_code == 0\n",
    "    }, error = function(e) {\n",
    "      # If head-object fails (e.g., not available), try ls as fallback\n",
    "      tryCatch({\n",
    "        # Suppress all warnings for ls command\n",
    "        result_ls <- suppressWarnings(\n",
    "          system2(aws_cmd, c(\"s3\", \"ls\", s3_path), \n",
    "                 stdout = TRUE, stderr = FALSE)  # Suppress stderr\n",
    "        )\n",
    "        # If we get output, file exists\n",
    "        !is.null(result_ls) && length(result_ls) > 0\n",
    "      }, error = function(e2) {\n",
    "        return(FALSE)\n",
    "      })\n",
    "    })\n",
    "    \n",
    "    if (is.logical(result) && result) {\n",
    "      return(TRUE)\n",
    "    }\n",
    "  }\n",
    "  \n",
    "  return(FALSE)\n",
    "}\n",
    "\n",
    "# ============================================================\n",
    "# RUN COHORTS AND AGE-BANDS IN PARALLEL\n",
    "# ============================================================\n",
    "# Create all combinations of cohort x age-band\n",
    "combinations <- expand.grid(\n",
    "  cohort = COHORT_NAMES,\n",
    "  age_band = AGE_BANDS,\n",
    "  stringsAsFactors = FALSE\n",
    ")\n",
    "\n",
    "# Check which combinations already exist\n",
    "cat(\"\\nChecking for already processed combinations...\\n\")\n",
    "combinations$already_exists <- mapply(check_results_exist, \n",
    "                                       combinations$cohort, \n",
    "                                       combinations$age_band, \n",
    "                                       EVENT_YEAR)\n",
    "\n",
    "# Check which cohort files exist\n",
    "cat(\"Checking for cohort files...\\n\")\n",
    "combinations$file_exists <- mapply(check_cohort_file_exists,\n",
    "                                    combinations$cohort,\n",
    "                                    combinations$age_band,\n",
    "                                    EVENT_YEAR)\n",
    "\n",
    "# Filter to only process new combinations that have cohort files\n",
    "combinations_to_process <- combinations[!combinations$already_exists & combinations$file_exists, ]\n",
    "combinations_skipped <- combinations[combinations$already_exists, ]\n",
    "combinations_missing_files <- combinations[!combinations$already_exists & !combinations$file_exists, ]\n",
    "\n",
    "cat(sprintf(\"Total combinations: %d\\n\", nrow(combinations)))\n",
    "cat(sprintf(\"Already processed: %d\\n\", sum(combinations$already_exists)))\n",
    "cat(sprintf(\"To process: %d\\n\", nrow(combinations_to_process)))\n",
    "\n",
    "if (nrow(combinations_skipped) > 0) {\n",
    "  cat(\"\\nSkipping (already processed):\\n\")\n",
    "  for (i in 1:nrow(combinations_skipped)) {\n",
    "    cat(sprintf(\"  - %s / %s\\n\", combinations_skipped$cohort[i], combinations_skipped$age_band[i]))\n",
    "  }\n",
    "}\n",
    "\n",
    "if (nrow(combinations_to_process) == 0) {\n",
    "  cat(\"\\n‚úì All combinations already processed! Nothing to do.\\n\")\n",
    "  stop(\"All combinations already processed. Exiting.\")\n",
    "}\n",
    "\n",
    "total_tasks <- nrow(combinations_to_process)\n",
    "cat(\"\\n\", paste(rep(\"=\", 80), collapse=\"\"), \"\\n\", sep=\"\")\n",
    "cat(\"Running feature importance analysis\\n\")\n",
    "cat(paste(rep(\"=\", 80), collapse=\"\"), \"\\n\", sep=\"\")\n",
    "cat(sprintf(\"Cohorts: %s\\n\", paste(COHORT_NAMES, collapse = \", \")))\n",
    "cat(sprintf(\"Age Bands: %s\\n\", paste(AGE_BANDS, collapse = \", \")))\n",
    "cat(sprintf(\"Event Year: %d\\n\", EVENT_YEAR))\n",
    "cat(sprintf(\"Total combinations: %d (cohorts √ó age-bands)\\n\", total_tasks))\n",
    "cat(sprintf(\"Parallel workers: %d\\n\", min(total_tasks, parallel::detectCores() - 2)))\n",
    "cat(sprintf(\"MC-CV workers per task: %d\\n\", N_WORKERS))\n",
    "cat(paste(rep(\"=\", 80), collapse=\"\"), \"\\n\\n\", sep=\"\")\n",
    "\n",
    "# Use a separate plan for task-level parallelism\n",
    "# Calculate worker allocation for nested parallelism:\n",
    "# - Task-level: Run multiple cohort/age-band combinations in parallel\n",
    "# - MC-CV level: Within each task, run MC-CV splits in parallel\n",
    "# We need to divide available cores between these two levels\n",
    "\n",
    "# Check available cores (respects mc.cores and other limits)\n",
    "available_cores <- tryCatch({\n",
    "  if (requireNamespace(\"parallelly\", quietly = TRUE)) {\n",
    "    parallelly::availableCores()\n",
    "  } else {\n",
    "    parallel::detectCores()\n",
    "  }\n",
    "}, error = function(e) {\n",
    "  parallel::detectCores()\n",
    "})\n",
    "\n",
    "# Reserve 2 cores for system\n",
    "usable_cores <- max(2, available_cores - 2)\n",
    "\n",
    "# Calculate task-level workers (how many tasks run in parallel)\n",
    "# Use min of: number of tasks, or reasonable limit based on cores\n",
    "# For many tasks, we want to run multiple in parallel but not all at once\n",
    "max_task_workers <- min(total_tasks, max(2, floor(usable_cores / 4)))  # Each task needs MC-CV workers too\n",
    "task_workers <- min(total_tasks, max_task_workers)\n",
    "\n",
    "# Calculate MC-CV workers per task\n",
    "# Divide remaining cores among tasks: (usable_cores - task_workers) / task_workers\n",
    "# But ensure each task gets at least 1 worker for MC-CV\n",
    "mc_cv_workers_per_task <- max(1, floor((usable_cores - task_workers) / task_workers))\n",
    "\n",
    "# Cap MC-CV workers to reasonable maximum (don't want too many per task)\n",
    "mc_cv_workers_per_task <- min(mc_cv_workers_per_task, 10)  # Max 10 workers per task for MC-CV\n",
    "\n",
    "cat(sprintf(\"\\nNested Parallelism Configuration:\\n\"))\n",
    "cat(sprintf(\"  Available cores: %d\\n\", available_cores))\n",
    "cat(sprintf(\"  Usable cores (reserved 2 for system): %d\\n\", usable_cores))\n",
    "cat(sprintf(\"  Task-level workers: %d (running %d tasks in parallel)\\n\", task_workers, min(total_tasks, task_workers)))\n",
    "cat(sprintf(\"  MC-CV workers per task: %d\\n\", mc_cv_workers_per_task))\n",
    "cat(sprintf(\"  Total worker allocation: %d (task-level) + %d √ó %d (MC-CV) = %d workers\\n\",\n",
    "            task_workers, task_workers, mc_cv_workers_per_task,\n",
    "            task_workers + (task_workers * mc_cv_workers_per_task)))\n",
    "cat(sprintf(\"  (Note: Workers are shared/reused, so actual usage is ~%d concurrent)\\n\", usable_cores))\n",
    "\n",
    "# Save current plan first\n",
    "current_plan <- plan(\"list\")\n",
    "# Set task-level plan\n",
    "cat(sprintf(\"\\nSetting task-level plan to %d workers\\n\", task_workers))\n",
    "plan(tweak(multisession, workers = task_workers))\n",
    "\n",
    "# Update N_WORKERS to be used by each task for MC-CV\n",
    "# This will be passed to run_cohort_analysis\n",
    "N_WORKERS_MC_CV <- mc_cv_workers_per_task\n",
    "cat(sprintf(\"Each task will use %d workers for MC-CV\\n\", N_WORKERS_MC_CV))\n",
    "# Verify plan is set correctly\n",
    "plan_info <- plan(\"list\")\n",
    "workers_info <- if (length(plan_info) > 0 && \"workers\" %in% names(plan_info[[1]])) {\n",
    "  as.character(plan_info[[1]]$workers)\n",
    "} else {\n",
    "  \"unknown\"\n",
    "}\n",
    "cat(sprintf(\"Current plan: %s with %s workers\\n\", \n",
    "            if (length(plan_info) > 0) class(plan_info[[1]])[1] else \"unknown\",\n",
    "            workers_info))\n",
    "cat(\"\\n\")\n",
    "\n",
    "# Run all combinations in parallel\n",
    "start_time <- Sys.time()\n",
    "cat(sprintf(\"Starting parallel execution of %d tasks...\\n\", total_tasks))\n",
    "cat(sprintf(\"Task-level plan: %d workers\\n\", task_workers))\n",
    "cat(sprintf(\"MC-CV plan per task: %d workers\\n\", if (exists(\"N_WORKERS_MC_CV\")) N_WORKERS_MC_CV else N_WORKERS))\n",
    "cat(\"All tasks should start simultaneously...\\n\\n\")\n",
    "\n",
    "task_results <- future_map(\n",
    "  1:total_tasks,\n",
    "  function(i) {\n",
    "    cohort_name <- combinations_to_process$cohort[i]\n",
    "    age_band <- combinations_to_process$age_band[i]\n",
    "    task_id <- sprintf(\"%s_%s\", cohort_name, age_band)\n",
    "    cat(sprintf(\"[%s] Starting analysis at %s\\n\", task_id, format(Sys.time(), \"%H:%M:%S\")))\n",
    "    \n",
    "    # Capture parameter values explicitly to avoid scoping issues in parallel workers\n",
    "    # This ensures values are captured from the calling environment, not looked up in worker environment\n",
    "    event_year_val <- EVENT_YEAR\n",
    "    n_splits_val <- N_SPLITS\n",
    "    train_prop_val <- TRAIN_PROP\n",
    "    # Use MC-CV specific worker count (calculated above for nested parallelism)\n",
    "    n_workers_val <- if (exists(\"N_WORKERS_MC_CV\")) N_WORKERS_MC_CV else N_WORKERS\n",
    "    scaling_metric_val <- SCALING_METRIC\n",
    "    model_params_val <- MODEL_PARAMS\n",
    "    debug_mode_val <- DEBUG_MODE\n",
    "    \n",
    "    # Validate critical parameters before calling\n",
    "    if (is.null(n_splits_val) || is.na(n_splits_val) || !is.numeric(n_splits_val) || n_splits_val <= 0) {\n",
    "      stop(sprintf(\"[%s] Invalid N_SPLITS value: %s (type: %s). Ensure Cell 3 was executed.\", \n",
    "                   task_id, \n",
    "                   if (is.null(n_splits_val)) \"NULL\" else if (is.na(n_splits_val)) \"NA\" else as.character(n_splits_val),\n",
    "                   typeof(n_splits_val)))\n",
    "    }\n",
    "    \n",
    "    # Pass parameters explicitly to ensure they're available in worker environment\n",
    "    result <- run_cohort_analysis(\n",
    "      cohort_name = cohort_name,\n",
    "      age_band = age_band,\n",
    "      event_year = event_year_val,\n",
    "      n_splits = n_splits_val,\n",
    "      train_prop = train_prop_val,\n",
    "      n_workers = n_workers_val,\n",
    "      scaling_metric = scaling_metric_val,\n",
    "      model_params = model_params_val,\n",
    "      debug_mode = debug_mode_val\n",
    "    )\n",
    "    cat(sprintf(\"[%s] Completed analysis at %s\\n\", task_id, format(Sys.time(), \"%H:%M:%S\")))\n",
    "    return(result)\n",
    "  },\n",
    "  .options = furrr_options(\n",
    "    seed = NULL,  # Don't set seed for task-level parallelism\n",
    "    globals = c(\"combinations_to_process\", \"EVENT_YEAR\", \"N_SPLITS\", \"TRAIN_PROP\", \n",
    "                \"TEST_SIZE\", \"SCALING_METRIC\", \"DEBUG_MODE\", \"N_WORKERS\", \"N_WORKERS_MC_CV\", \"MODEL_PARAMS\",\n",
    "                \"output_dir\", \"run_cohort_analysis\",\n",
    "                # Note: setup_r_logging creates file connections which can't be serialized\n",
    "                # The function will be called inside run_cohort_analysis, so it needs to be available\n",
    "                \"setup_r_logging\", \"save_logs_to_s3_r\", \"check_memory_usage_r\",\n",
    "                \"train_catboost_r\", \"train_random_forest_r\",\n",
    "                \"predict_catboost_r\", \"predict_random_forest_r\",\n",
    "                \"predict_proba_catboost_r\", \"predict_proba_random_forest_r\",\n",
    "                \"get_importance_catboost_r\", \"get_importance_random_forest_r\",\n",
    "                \"calculate_recall\", \"calculate_logloss\",\n",
    "                \"run_mc_cv_method\", \"mc_cv\", \"future_map\", \"furrr_options\", \"progressor\",\n",
    "                # Ensure future package functions are available\n",
    "                \"future\", \"plan\", \"multisession\"),\n",
    "    # Ensure required packages are available in worker environments\n",
    "    packages = c(\"rsample\", \"furrr\", \"progressr\", \"catboost\", \"randomForest\",\n",
    "                 \"dplyr\", \"tibble\", \"purrr\", \"tidyr\", \"readr\", \"here\", \"duckdb\", \"DBI\")\n",
    "  )\n",
    ")\n",
    "end_time <- Sys.time()\n",
    "\n",
    "# Restore original plan (use the MC-CV worker count, not task-level)\n",
    "plan(multisession, workers = if (exists(\"N_WORKERS_MC_CV\")) N_WORKERS_MC_CV else N_WORKERS)\n",
    "\n",
    "# Print summary\n",
    "cat(\"\\n\", paste(rep(\"=\", 80), collapse=\"\"), \"\\n\", sep=\"\")\n",
    "cat(\"TASK ANALYSIS SUMMARY\\n\")\n",
    "cat(paste(rep(\"=\", 80), collapse=\"\"), \"\\n\", sep=\"\")\n",
    "cat(sprintf(\"Total time: %.1f minutes\\n\", as.numeric(difftime(end_time, start_time, units = \"mins\"))))\n",
    "cat(sprintf(\"Total tasks completed: %d\\n\", length(task_results)))\n",
    "cat(sprintf(\"Total tasks skipped (already processed): %d\\n\", nrow(combinations_skipped)))\n",
    "\n",
    "# Group results by cohort (including skipped ones)\n",
    "cohort_summary <- list()\n",
    "for (result in task_results) {\n",
    "  cohort <- result$cohort\n",
    "  if (!cohort %in% names(cohort_summary)) {\n",
    "    cohort_summary[[cohort]] <- list()\n",
    "  }\n",
    "  cohort_summary[[cohort]][[length(cohort_summary[[cohort]]) + 1]] <- result\n",
    "}\n",
    "\n",
    "# Add skipped combinations to summary\n",
    "for (i in 1:nrow(combinations_skipped)) {\n",
    "  cohort <- combinations_skipped$cohort[i]\n",
    "  age_band <- combinations_skipped$age_band[i]\n",
    "  if (!cohort %in% names(cohort_summary)) {\n",
    "    cohort_summary[[cohort]] <- list()\n",
    "  }\n",
    "  cohort_summary[[cohort]][[length(cohort_summary[[cohort]]) + 1]] <- list(\n",
    "    cohort = cohort,\n",
    "    age_band = age_band,\n",
    "    status = \"skipped\",\n",
    "    note = \"Already processed\"\n",
    "  )\n",
    "}\n",
    "\n",
    "for (cohort_name in names(cohort_summary)) {\n",
    "  cat(sprintf(\"\\nCohort: %s\\n\", cohort_name))\n",
    "  cat(sprintf(\"  Age-bands processed: %d\\n\", length(cohort_summary[[cohort_name]])))\n",
    "  for (result in cohort_summary[[cohort_name]]) {\n",
    "    age_band <- if (\"age_band\" %in% names(result)) result$age_band else \"unknown\"\n",
    "    cat(sprintf(\"  - Age-band %s: \", age_band))\n",
    "    if (result$status == \"success\") {\n",
    "      cat(sprintf(\"‚úì Success (Features: %d)\\n\", nrow(result$aggregated)))\n",
    "    } else if (result$status == \"skipped\") {\n",
    "      cat(sprintf(\"‚äò Skipped (already processed)\\n\"))\n",
    "    } else {\n",
    "      cat(sprintf(\"‚úó Error: %s\\n\", result$error))\n",
    "    }\n",
    "  }\n",
    "}\n",
    "\n",
    "# ============================================================\n",
    "# CHECK IF ALL COMBINATIONS ARE COMPLETE FOR AGGREGATION\n",
    "# ============================================================\n",
    "cat(\"\\n\", paste(rep(\"=\", 80), collapse=\"\"), \"\\n\", sep=\"\")\n",
    "cat(\"CHECKING COMPLETENESS FOR AGGREGATION\\n\")\n",
    "cat(paste(rep(\"=\", 80), collapse=\"\"), \"\\n\", sep=\"\")\n",
    "\n",
    "# Function to check if all required combinations exist\n",
    "check_all_combinations_complete <- function(cohort_names, age_bands, event_year) {\n",
    "  all_combinations <- expand.grid(\n",
    "    cohort = cohort_names,\n",
    "    age_band = age_bands,\n",
    "    stringsAsFactors = FALSE\n",
    "  )\n",
    "  \n",
    "  complete_combinations <- 0\n",
    "  missing_combinations <- list()\n",
    "  \n",
    "  for (i in 1:nrow(all_combinations)) {\n",
    "    cohort <- all_combinations$cohort[i]\n",
    "    age_band <- all_combinations$age_band[i]\n",
    "    \n",
    "    if (check_results_exist(cohort, age_band, event_year)) {\n",
    "      complete_combinations <- complete_combinations + 1\n",
    "    } else {\n",
    "      missing_combinations[[length(missing_combinations) + 1]] <- \n",
    "        list(cohort = cohort, age_band = age_band)\n",
    "    }\n",
    "  }\n",
    "  \n",
    "  return(list(\n",
    "    total = nrow(all_combinations),\n",
    "    complete = complete_combinations,\n",
    "    missing = missing_combinations,\n",
    "    all_complete = (complete_combinations == nrow(all_combinations))\n",
    "  ))\n",
    "}\n",
    "\n",
    "# Check completeness for each cohort\n",
    "completeness_status <- list()\n",
    "for (cohort_name in COHORT_NAMES) {\n",
    "  status <- check_all_combinations_complete(c(cohort_name), AGE_BANDS, EVENT_YEAR)\n",
    "  completeness_status[[cohort_name]] <- status\n",
    "  \n",
    "  cat(sprintf(\"\\nCohort: %s\\n\", cohort_name))\n",
    "  cat(sprintf(\"  Required age-bands: %d\\n\", status$total))\n",
    "  cat(sprintf(\"  Complete: %d\\n\", status$complete))\n",
    "  cat(sprintf(\"  Missing: %d\\n\", length(status$missing)))\n",
    "  \n",
    "  if (length(status$missing) > 0) {\n",
    "    cat(\"  Missing combinations:\\n\")\n",
    "    for (missing in status$missing) {\n",
    "      cat(sprintf(\"    - %s / %s\\n\", missing$cohort, missing$age_band))\n",
    "    }\n",
    "  }\n",
    "  \n",
    "  if (status$all_complete) {\n",
    "    cat(sprintf(\"  ‚úì All age-bands complete - ready for aggregation\\n\"))\n",
    "  } else {\n",
    "    cat(sprintf(\"  ‚ö† Not all age-bands complete - aggregation will be skipped\\n\"))\n",
    "  }\n",
    "}\n",
    "\n",
    "# Determine if we can proceed with cross-age-band aggregation\n",
    "all_cohorts_complete <- all(sapply(completeness_status, function(x) x$all_complete))\n",
    "\n",
    "if (all_cohorts_complete) {\n",
    "  cat(\"\\n\", paste(rep(\"=\", 80), collapse=\"\"), \"\\n\", sep=\"\")\n",
    "  cat(\"‚úì ALL COMBINATIONS COMPLETE\\n\")\n",
    "  cat(\"  Ready for cross-age-band aggregation and visualizations\\n\")\n",
    "  cat(paste(rep(\"=\", 80), collapse=\"\"), \"\\n\", sep=\"\")\n",
    "  \n",
    "  # Set flag for downstream aggregation steps\n",
    "  AGGREGATION_READY <- TRUE\n",
    "} else {\n",
    "  cat(\"\\n\", paste(rep(\"=\", 80), collapse=\"\"), \"\\n\", sep=\"\")\n",
    "  cat(\"‚ö† NOT ALL COMBINATIONS COMPLETE\\n\")\n",
    "  cat(\"  Cross-age-band aggregation will be skipped\\n\")\n",
    "  cat(\"  Run again after all combinations are processed\\n\")\n",
    "  cat(paste(rep(\"=\", 80), collapse=\"\"), \"\\n\", sep=\"\")\n",
    "  \n",
    "  AGGREGATION_READY <- FALSE\n",
    "}\n",
    "\n",
    "cat(\"\\n\", paste(rep(\"=\", 80), collapse=\"\"), \"\\n\", sep=\"\")\n",
    "cat(\"‚úì All cohort analyses complete\\n\")\n",
    "cat(paste(rep(\"=\", 80), collapse=\"\"), \"\\n\\n\", sep=\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9146126-5837-4d3e-9ece-48f6053bf8e6",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "source": [
    "## 2. Parallel Execution\n",
    "\n",
    "**This notebook is configured for parallel execution** of multiple cohort √ó age-band combinations.\n",
    "\n",
    "**Parallel Processing**: Runs all combinations defined in `COHORT_NAMES` √ó `AGE_BANDS` in parallel. Each task processes one cohort/age-band combination and uses the `run_cohort_analysis()` function.\n",
    "\n",
    "**Single Cohort Execution**: If you need to run a single cohort/age-band combination instead of parallel execution, see the [Single Cohort Execution section](/docs/README_feature_importance.md#single-cohort-execution-optional) in the Feature Importance README for a complete example.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd63826",
   "metadata": {},
   "source": [
    "## 3. Cross-Age-Band Aggregation and Visualizations\n",
    "\n",
    "**Note:** This step only runs when all cohort √ó age-band combinations are complete.\n",
    "\n",
    "Creates:\n",
    "- Cross-age-band heatmaps showing feature importance across age bands\n",
    "- Aggregated visualizations comparing cohorts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea613ec0",
   "metadata": {},
   "source": [
    "# Cleanup\n",
    "\n",
    "Close parallel processing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc0fd1ef",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CROSS-AGE-BAND AGGREGATION AND VISUALIZATIONS\n",
    "# ============================================================\n",
    "# Only run if all combinations are complete\n",
    "if (exists(\"AGGREGATION_READY\") && AGGREGATION_READY) {\n",
    "  cat(\"\\n\", paste(rep(\"=\", 80), collapse=\"\"), \"\\n\", sep=\"\")\n",
    "  cat(\"CREATING CROSS-AGE-BAND VISUALIZATIONS\\n\")\n",
    "  cat(paste(rep(\"=\", 80), collapse=\"\"), \"\\n\", sep=\"\")\n",
    "  \n",
    "  # Source visualization scripts\n",
    "  if (!exists(\"helpers_dir\")) {\n",
    "    helpers_dir <- here(\"helpers_13_1997\")\n",
    "  }\n",
    "  source(file.path(helpers_dir, \"create_cross_ageband_heatmap.R\"))\n",
    "  \n",
    "  # Create cross-age-band heatmaps for each cohort\n",
    "  for (cohort_name in COHORT_NAMES) {\n",
    "    cat(sprintf(\"\\nCreating cross-age-band heatmap for: %s\\n\", cohort_name))\n",
    "    \n",
    "    tryCatch({\n",
    "      heatmap_file <- create_ageband_heatmap(\n",
    "        cohort_name = cohort_name,\n",
    "        event_year = EVENT_YEAR,\n",
    "        age_bands = AGE_BANDS,\n",
    "        output_dir = output_dir,\n",
    "        s3_upload = TRUE,\n",
    "        top_n = 50\n",
    "      )\n",
    "      cat(sprintf(\"‚úì Heatmap created: %s\\n\", heatmap_file))\n",
    "    }, error = function(e) {\n",
    "      cat(sprintf(\"‚úó Error creating heatmap for %s: %s\\n\", cohort_name, e$message))\n",
    "    })\n",
    "  }\n",
    "  \n",
    "  cat(\"\\n\", paste(rep(\"=\", 80), collapse=\"\"), \"\\n\", sep=\"\")\n",
    "  cat(\"‚úì Cross-age-band visualizations complete\\n\")\n",
    "  cat(paste(rep(\"=\", 80), collapse=\"\"), \"\\n\", sep=\"\")\n",
    "  \n",
    "} else {\n",
    "  cat(\"\\n\", paste(rep(\"=\", 80), collapse=\"\"), \"\\n\", sep=\"\")\n",
    "  cat(\"SKIPPING CROSS-AGE-BAND AGGREGATION\\n\")\n",
    "  cat(paste(rep(\"=\", 80), collapse=\"\"), \"\\n\", sep=\"\")\n",
    "  cat(\"Reason: Not all cohort √ó age-band combinations are complete\\n\")\n",
    "  cat(\"  - Complete all combinations first\\n\")\n",
    "  cat(\"  - Then re-run this cell to create cross-age-band visualizations\\n\")\n",
    "  cat(paste(rep(\"=\", 80), collapse=\"\"), \"\\n\", sep=\"\")\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "425a78ce",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Close parallel processing\n",
    "plan(sequential)\n",
    "\n",
    "cat(\"\\n========================================\\n\")\n",
    "cat(\"Analysis Complete!\\n\")\n",
    "cat(\"========================================\\n\")\n",
    "cat(sprintf(\"Local output directory: %s\\n\", output_dir))\n",
    "cat(sprintf(\"S3 output location: s3://pgxdatalake/gold/feature_importance/cohort_name=%s/age_band=%s/event_year=%d/\\n\",\n",
    "            COHORT_NAME, AGE_BAND, EVENT_YEAR))\n",
    "cat(sprintf(\"MC-CV splits: %d\\n\", N_SPLITS))\n",
    "cat(sprintf(\"Train/Test ratio: %.0f/%.0f\\n\", TRAIN_PROP * 100, TEST_SIZE * 100))\n",
    "cat(\"\\nResults show scaled feature importance with MC-CV Recall scores\\n\")\n",
    "cat(\"based on\", N_SPLITS, \"independent train/test splits.\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "065a50ea-25c9-4119-8ede-abc6a754e136",
   "metadata": {},
   "source": [
    "# Sync Results and Code to S3\n",
    "\n",
    "Sync output files and code (notebook + R script) to S3 bucket. \n",
    "- Outputs: CSV results files\n",
    "- Code: Notebook and R script for reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "861a2d43-3d45-4f58-9489-4380acfccd6f",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Sync outputs and code to S3\n",
    "# On EC2, we're in the feature_importance directory  \n",
    "s3_bucket <- \"s3://pgx-repository/pgx-analysis/3_feature_importance/\"\n",
    "\n",
    "# Find AWS CLI (check common locations - EC2 typically has it in /usr/local/bin or /usr/bin)\n",
    "aws_cmd <- Sys.which(\"aws\")\n",
    "if (aws_cmd == \"\") {\n",
    "  # Try common EC2 installation paths\n",
    "  aws_paths <- c(\n",
    "    \"/usr/local/bin/aws\",\n",
    "    \"/usr/bin/aws\",\n",
    "    \"/home/ec2-user/.local/bin/aws\"\n",
    "  )\n",
    "  aws_cmd <- NULL\n",
    "  for (path in aws_paths) {\n",
    "    if (file.exists(path)) {\n",
    "      aws_cmd <- path\n",
    "      break\n",
    "    }\n",
    "  }\n",
    "  if (is.null(aws_cmd)) {\n",
    "    stop(\"AWS CLI not found. Please install AWS CLI or ensure it's in your PATH.\")\n",
    "  }\n",
    "}\n",
    "\n",
    "cat(\"Syncing outputs and code to S3...\\n\")\n",
    "cat(\"Source: feature_importance/ directory\\n\")\n",
    "cat(\"Destination:\", s3_bucket, \"\\n\")\n",
    "cat(\"AWS CLI:\", aws_cmd, \"\\n\\n\")\n",
    "\n",
    "# Get current directory (should be feature_importance)\n",
    "current_dir <- getwd()\n",
    "if (!grepl(\"feature_importance\", current_dir)) {\n",
    "  warning(\"Current directory doesn't appear to be feature_importance. Double-check sync destination.\")\n",
    "}\n",
    "\n",
    "# Sync feature_importance directory (includes outputs/ and code files)\n",
    "# Explicitly include notebook, R scripts, README files, and outputs directory\n",
    "# Exclude temporary files, checkpoints, and unnecessary directories\n",
    "# Note: --delete flag removed for safety (won't delete files in S3 that don't exist locally)\n",
    "# Include patterns are processed before exclude patterns, then exclude everything else\n",
    "sync_cmd <- sprintf(\n",
    "  '\"%s\" s3 sync \"%s\" %s --include \"*.ipynb\" --include \"*.R\" --include \"README*.md\" --include \"outputs/**\" --exclude \"*checkpoint*\" --exclude \"*.tmp\" --exclude \"*.ipynb_checkpoints/*\" --exclude \"*.RData\" --exclude \"*.Rhistory\" --exclude \".Rproj.user/*\" --exclude \"catboost_info/*\" --exclude \"*.log\" --exclude \"*\"',\n",
    "  aws_cmd,\n",
    "  current_dir,\n",
    "  s3_bucket\n",
    ")\n",
    "\n",
    "cat(\"Running:\", sync_cmd, \"\\n\\n\")\n",
    "result <- system(sync_cmd)\n",
    "\n",
    "if (result == 0) {\n",
    "  cat(\"‚úì Successfully synced outputs and code to S3\\n\")\n",
    "  cat(\"  - Outputs:\", file.path(output_dir), \"\\n\")\n",
    "  cat(\"  - Code: *.ipynb, *.R, README*.md\\n\")\n",
    "} else {\n",
    "  warning(sprintf(\"S3 sync returned exit code %d. Check AWS credentials and permissions.\", result))\n",
    "}\n",
    "\n",
    "# ============================================================\n",
    "# SAVE LOGS TO S3 (aligned with 2_create_cohort)\n",
    "# ============================================================\n",
    "cat(\"\\n========================================\\n\")\n",
    "cat(\"Saving logs to S3...\\n\")\n",
    "cat(\"========================================\\n\")\n",
    "\n",
    "# Close log file connection\n",
    "if (exists(\"log_setup\") && !is.null(log_setup$log_connection)) {\n",
    "  if (isOpen(log_setup$log_connection)) {\n",
    "    close(log_setup$log_connection)\n",
    "  }\n",
    "}\n",
    "\n",
    "# Save logs to S3\n",
    "if (exists(\"logger\") && exists(\"log_file_path\")) {\n",
    "  tryCatch({\n",
    "    s3_path <- save_logs_to_s3_r(log_file_path, COHORT_NAME, AGE_BAND, EVENT_YEAR, logger)\n",
    "    if (!is.null(s3_path)) {\n",
    "      logger$info(\"‚úì Analysis completed successfully. Logs saved to S3.\")\n",
    "    }\n",
    "  }, error = function(e) {\n",
    "    cat(sprintf(\"Warning: Could not save logs to S3: %s\\n\", e$message))\n",
    "    cat(sprintf(\"Log file saved locally: %s\\n\", log_file_path))\n",
    "  })\n",
    "} else {\n",
    "  cat(\"Warning: Logger not initialized. Logs not saved to S3.\\n\")\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "832bf4fd-dc62-4c54-89a2-1b9d2d251a76",
   "metadata": {},
   "source": [
    "# Shutdown EC2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6315b0bf-daaa-4543-a9f0-a0a663f3daf5",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Shutdown EC2 instance after analysis completes\n",
    "# Set SHUTDOWN_EC2 = TRUE to enable, FALSE to disable\n",
    "SHUTDOWN_EC2 <- TRUE  # Change to TRUE to enable auto-shutdown\n",
    "\n",
    "if (SHUTDOWN_EC2) {\n",
    "  cat(\"\\n========================================\\n\")\n",
    "  cat(\"Shutting down EC2 instance...\\n\")\n",
    "  cat(\"========================================\\n\")\n",
    "  \n",
    "  # Get instance ID from EC2 metadata service\n",
    "  instance_id <- tryCatch({\n",
    "    system(\"curl -s http://169.254.169.254/latest/meta-data/instance-id\", intern = TRUE)\n",
    "  }, error = function(e) {\n",
    "    cat(\"Warning: Could not retrieve instance ID from metadata service.\\n\")\n",
    "    cat(\"If running on EC2, check that metadata service is accessible.\\n\")\n",
    "    return(NULL)\n",
    "  })\n",
    "  \n",
    "  if (!is.null(instance_id) && length(instance_id) > 0 && nchar(instance_id[1]) > 0) {\n",
    "    instance_id <- instance_id[1]\n",
    "    cat(sprintf(\"Instance ID: %s\\n\", instance_id))\n",
    "    \n",
    "    # Find AWS CLI\n",
    "    aws_cmd <- Sys.which(\"aws\")\n",
    "    if (aws_cmd == \"\") {\n",
    "      aws_paths <- c(\n",
    "        \"/usr/local/bin/aws\",\n",
    "        \"/usr/bin/aws\",\n",
    "        \"/home/ec2-user/.local/bin/aws\"\n",
    "      )\n",
    "      aws_cmd <- NULL\n",
    "      for (path in aws_paths) {\n",
    "        if (file.exists(path)) {\n",
    "          aws_cmd <- path\n",
    "          break\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "    \n",
    "    if (!is.null(aws_cmd) && aws_cmd != \"\") {\n",
    "      # Stop the instance (use terminate-instances for permanent deletion)\n",
    "      shutdown_cmd <- sprintf(\n",
    "        '\"%s\" ec2 stop-instances --instance-ids %s',\n",
    "        aws_cmd,\n",
    "        instance_id\n",
    "      )\n",
    "      \n",
    "      cat(\"Running:\", shutdown_cmd, \"\\n\")\n",
    "      result <- system(shutdown_cmd)\n",
    "      \n",
    "      if (result == 0) {\n",
    "        cat(\"‚úì EC2 instance stop command sent successfully\\n\")\n",
    "        cat(\"Instance will stop in a few moments.\\n\")\n",
    "        cat(\"Note: This is a STOP (not terminate), so you can restart it later.\\n\")\n",
    "      } else {\n",
    "        warning(sprintf(\"EC2 stop command returned exit code %d. Check AWS credentials and permissions.\", result))\n",
    "      }\n",
    "    } else {\n",
    "      cat(\"Warning: AWS CLI not found. Cannot shutdown instance.\\n\")\n",
    "      cat(\"Install AWS CLI or ensure it's in your PATH.\\n\")\n",
    "    }\n",
    "  } else {\n",
    "    cat(\"Warning: Could not determine instance ID. Skipping shutdown.\\n\")\n",
    "    cat(\"If you want to shutdown manually, use:\\n\")\n",
    "    cat(\"  aws ec2 stop-instances --instance-ids <your-instance-id>\\n\")\n",
    "  }\n",
    "} else {\n",
    "  cat(\"\\n========================================\\n\")\n",
    "  cat(\"EC2 Auto-Shutdown: DISABLED\\n\")\n",
    "  cat(\"========================================\\n\")\n",
    "  cat(\"To enable auto-shutdown, set SHUTDOWN_EC2 = TRUE in this cell.\\n\")\n",
    "  cat(\"Instance will continue running.\\n\")\n",
    "}\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R (PGX-Analysis)",
   "language": "R",
   "name": "pgx-analysis"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
