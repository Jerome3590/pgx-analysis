{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "59bb209f",
   "metadata": {},
   "source": [
    "# Feature Importance with Monte Carlo Cross-Validation\n",
    "\n",
    "**Purpose:** Calculate scaled feature importance using CatBoost and Random Forest  \n",
    "**Method:** Normalized feature importance scaled by MC-CV Recall scores  \n",
    "**Based on:** [R Example](https://github.com/Jerome3590/phts/blob/main/graft-loss/feature_importance/replicate_20_features_MC_CV.R)  \n",
    "**Updated:** November 2025  \n",
    "**Hardware:** Optimized for EC2 (32 cores, 1TB RAM)  \n",
    "**Validation:** Proper evaluation on unseen test data\n",
    "\n",
    "## Key Features\n",
    "\n",
    "‚úÖ **Monte Carlo Cross-Validation** ‚Äì up to 1000 random train/test splits (100-split runs used for faster iteration)  \n",
    "‚úÖ **Stratified Sampling** - Maintains target distribution  \n",
    "‚úÖ **Parallel Processing** - Fast execution with furrr/future (‚âà30 workers)  \n",
    "‚úÖ **95% Confidence Intervals** - Narrow, precise estimates (tighter with more splits)  \n",
    "‚úÖ **Multiple Models** - CatBoost (R) and Random Forest (R)  \n",
    "\n",
    "## Methodology\n",
    "\n",
    "This notebook implements the feature selection methodology:\n",
    "\n",
    "1. Load cohort data from parquet files (same as FP-Growth notebook)\n",
    "2. Create patient-level features (one-hot encoding of items)\n",
    "3. For each model type:\n",
    "   - Create 100‚Äì1000 stratified train/test splits\n",
    "   - Train model on training set\n",
    "   - Evaluate Recall on unseen test set\n",
    "   - Extract feature importance\n",
    "   - Aggregate results across splits\n",
    "4. Normalize and scale feature importance by MC-CV Recall\n",
    "5. Aggregate across models\n",
    "6. Extract top features\n",
    "\n",
    "## Expected Runtime\n",
    "\n",
    "- **100 splits (current default):**\n",
    "  - Local (4 cores): ~2‚Äì4 hours\n",
    "  - Workstation (16 cores): ~1‚Äì2 hours\n",
    "  - EC2 (32 cores, 1TB RAM): ~1‚Äì2 hours ‚úÖ **RECOMMENDED FOR DEVELOPMENT**\n",
    "- **1000 splits (extended / publication-level):**\n",
    "  - Local (4 cores): 8‚Äì12+ hours\n",
    "  - Workstation (16 cores): ~8‚Äì16 hours\n",
    "  - EC2 (32 cores, 1TB RAM): ~10‚Äì20 hours ‚úÖ **RECOMMENDED FOR FINAL RESULTS**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26eb9c0a",
   "metadata": {},
   "source": [
    "## 1. Setup and Configuration\n",
    "\n",
    "Load required packages and configure parallel processing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0617449b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "'R version 4.4.3 (2025-02-28)'"
      ],
      "text/latex": [
       "'R version 4.4.3 (2025-02-28)'"
      ],
      "text/markdown": [
       "'R version 4.4.3 (2025-02-28)'"
      ],
      "text/plain": [
       "[1] \"R version 4.4.3 (2025-02-28)\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì All packages loaded successfully\n"
     ]
    }
   ],
   "source": [
    "# Check R version\n",
    "R.version.string\n",
    "\n",
    "# Load required packages\n",
    "suppressPackageStartupMessages({\n",
    "  library(here)\n",
    "  library(dplyr)\n",
    "  library(readr)\n",
    "  library(tidyr)\n",
    "  library(tibble)\n",
    "  library(purrr)\n",
    "  library(catboost)\n",
    "  library(randomForest)\n",
    "  library(rsample)    # For MC-CV\n",
    "  library(furrr)      # For parallel processing\n",
    "  library(future)     # For parallel backend\n",
    "  library(progressr)  # For progress bars\n",
    "  library(duckdb)     # For loading parquet files\n",
    "  library(DBI)        # Database interface for DuckDB\n",
    "})\n",
    "\n",
    "cat(\"‚úì All packages loaded successfully\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "785bdbc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Auto-detected 32 cores, using 30 workers\n",
      "Setting up parallel processing with 30 workers...\n",
      "Set future.globals.maxSize to 800 GB\n",
      "Output directory: /home/pgx3874/pgx-analysis/3_feature_importance/outputs \n",
      "MC-CV Configuration: 200 splits, 80/20 train/test split\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# DEBUG/TEST MODE - Quick testing before full run\n",
    "# ============================================================\n",
    "# Set DEBUG_MODE = TRUE for quick testing (5 splits, ~2-5 min)\n",
    "# Set DEBUG_MODE = FALSE for full analysis (100 splits, ~1-2 hours on EC2)\n",
    "\n",
    "DEBUG_MODE <- FALSE  # Change to TRUE for quick test\n",
    "\n",
    "if (DEBUG_MODE) {\n",
    "  cat(\"\\n\")\n",
    "  cat(\"‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó\\n\")\n",
    "  cat(\"‚ïë                    üîç DEBUG MODE ENABLED                       ‚ïë\\n\")\n",
    "  cat(\"‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù\\n\")\n",
    "  cat(\"\\n\")\n",
    "  cat(\"Quick test configuration:\\n\")\n",
    "  cat(\"  ‚Ä¢ MC-CV Splits: 5 (instead of 100)\\n\")\n",
    "  cat(\"  ‚Ä¢ Expected time: 2-5 minutes\\n\")\n",
    "  cat(\"  ‚Ä¢ Purpose: Verify everything works before full run\\n\")\n",
    "  cat(\"\\n\")\n",
    "  cat(\"To run full analysis, set DEBUG_MODE = FALSE\\n\")\n",
    "  cat(\"\\n\")\n",
    "}\n",
    "\n",
    "# Configuration\n",
    "# NOTE: Target definition differs by cohort (already baked into cohort data via is_target_case):\n",
    "#   - \"opioid_ed\": Target cases = patients with F1120/opioid ICD codes (any of 10 ICD columns)\n",
    "#   - \"non_opioid_ed\": Target cases = patients with HCG ED visits (P51/O11/P33) WITHOUT opioid codes\n",
    "# Controls (is_target_case=0) are sampled to maintain 5:1 ratio in both cohorts\n",
    "COHORT_NAME <- \"opioid_ed\"  # Options: \"opioid_ed\" or \"non_opioid_ed\"\n",
    "AGE_BAND <- \"25-44\"         # Change as needed\n",
    "EVENT_YEAR <- 2016          # Change as needed\n",
    "\n",
    "N_SPLITS <- if (DEBUG_MODE) 5 else 200  # MC-CV splits (5 for debug, 100 for development, 1000 for production)\n",
    "TEST_SIZE <- 0.2             # Test set proportion (20%)\n",
    "TRAIN_PROP <- 1 - TEST_SIZE  # Training proportion (80%)\n",
    "\n",
    "# Scaling metric for feature importance\n",
    "# Options: \"recall\" (default) or \"logloss\"\n",
    "# - Recall: Higher is better (0-1), good for imbalanced classes, focuses on finding positives\n",
    "# - LogLoss: Lower is better, measures probability calibration, penalizes overconfident errors\n",
    "#   (will be inverted: 1/logloss for scaling, so higher = better)\n",
    "SCALING_METRIC <- \"recall\"  # Change to \"logloss\" if preferred\n",
    "\n",
    "# Model parameters\n",
    "MODEL_PARAMS <- list(\n",
    "  catboost = list(\n",
    "    iterations = 100,\n",
    "    learning_rate = 0.1,\n",
    "    depth = 6,\n",
    "    verbose = 0L,  # Turn off CatBoost logging (0L = integer 0)\n",
    "    random_seed = 42\n",
    "  ),\n",
    "  random_forest = list(\n",
    "    ntree = 100,\n",
    "    mtry = NULL,  # Will be set to sqrt(n_features)\n",
    "    nodesize = 1,\n",
    "    maxnodes = NULL\n",
    "  )\n",
    ")\n",
    "\n",
    "# Set up parallel processing\n",
    "# EC2 optimization: Use 30 out of 32 cores (leave 2 for system)\n",
    "N_WORKERS <- as.integer(Sys.getenv(\"N_WORKERS\", \"0\"))\n",
    "if (N_WORKERS < 1) {\n",
    "  # Auto-detect: use all cores minus 2 for system\n",
    "  total_cores <- parallel::detectCores()\n",
    "  N_WORKERS <- max(1, total_cores - 2)\n",
    "  cat(sprintf(\"Auto-detected %d cores, using %d workers\\n\", total_cores, N_WORKERS))\n",
    "}\n",
    "cat(sprintf(\"Setting up parallel processing with %d workers...\\n\", N_WORKERS))\n",
    "\n",
    "# Increase future.globals.maxSize for large MC-CV splits object\n",
    "# With 1TB RAM on EC2, we can handle large transfers\n",
    "# Note: MC-CV splits and data matrices can be very large (227+ GB)\n",
    "# Increase limit to accommodate large feature matrices (19,586 features √ó 31,146 patients)\n",
    "options(future.globals.maxSize = 800 * 1024^3)  # 800 GB limit\n",
    "cat(\"Set future.globals.maxSize to 800 GB\\n\")\n",
    "\n",
    "plan(multisession, workers = N_WORKERS)\n",
    "\n",
    "# Output directory\n",
    "output_dir <- here(\"outputs\")\n",
    "dir.create(output_dir, showWarnings = FALSE, recursive = TRUE)\n",
    "\n",
    "cat(\"Output directory:\", output_dir, \"\\n\")\n",
    "cat(sprintf(\"MC-CV Configuration: %d splits, %.0f/%.0f train/test split\\n\", \n",
    "            N_SPLITS, TRAIN_PROP * 100, TEST_SIZE * 100))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67cde83f",
   "metadata": {},
   "source": [
    "## 2. Data Loading\n",
    "\n",
    "Load cohort data from parquet files (same logic as FP-Growth notebook).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90f80186",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading from: /mnt/nvme/cohorts/cohort_name=opioid_ed/event_year=2016/age_band=25-44/cohort.parquet\n",
      "Loaded 1736972 event-level records\n",
      "Unique patients: 31146\n",
      "\n",
      "Event type distribution:\n",
      "  medical: 1242655 (71.5%)\n",
      "  pharmacy: 494317 (28.5%)\n",
      "\n",
      "Target distribution (event-level):\n",
      "  Target 0: 994025 events (57.2%)\n",
      "  Target 1: 742947 events (42.8%)\n",
      "\n",
      "Target distribution (patient-level):\n",
      "  Target 0: 25955 patients (83.3%)\n",
      "  Target 1: 5191 patients (16.7%)\n",
      "\n",
      "Checking for patients with inconsistent target values:\n",
      "  ‚úì No inconsistencies found\n"
     ]
    }
   ],
   "source": [
    "# Determine local data path (same as FP-Growth)\n",
    "LOCAL_DATA_PATH <- Sys.getenv(\"LOCAL_DATA_PATH\", \"/mnt/nvme/cohorts\")\n",
    "if (!dir.exists(LOCAL_DATA_PATH)) {\n",
    "  # Try Windows path\n",
    "  LOCAL_DATA_PATH <- Sys.getenv(\"LOCAL_DATA_PATH\", \"C:/Projects/pgx-analysis/data/gold/cohorts_F1120\")\n",
    "}\n",
    "\n",
    "parquet_file <- file.path(LOCAL_DATA_PATH, \n",
    "                          paste0(\"cohort_name=\", COHORT_NAME),\n",
    "                          paste0(\"event_year=\", EVENT_YEAR),\n",
    "                          paste0(\"age_band=\", AGE_BAND),\n",
    "                          \"cohort.parquet\")\n",
    "\n",
    "if (!file.exists(parquet_file)) {\n",
    "  stop(sprintf(\"Cohort file not found: %s\\nPlease check LOCAL_DATA_PATH and file structure.\", parquet_file))\n",
    "}\n",
    "\n",
    "cat(sprintf(\"Loading from: %s\\n\", parquet_file))\n",
    "\n",
    "# Load using DuckDB\n",
    "con <- dbConnect(duckdb::duckdb(), dbdir = \":memory:\")\n",
    "\n",
    "# Load cohort data (same columns as FP-Growth notebook)\n",
    "# NOTE: Use is_target_case as target variable (target column is hardcoded to 1 in cohort SQL)\n",
    "# is_target_case definition depends on COHORT_NAME:\n",
    "#   - opioid_ed: 1 = patients with opioid ICD codes (F1120, etc.), 0 = controls\n",
    "#   - non_opioid_ed: 1 = patients with HCG ED visits (no opioid codes), 0 = controls\n",
    "query <- sprintf(\"\n",
    "  SELECT \n",
    "    mi_person_key,\n",
    "    is_target_case as target,  -- Use is_target_case: 1=target case, 0=control\n",
    "    drug_name,\n",
    "    primary_icd_diagnosis_code,\n",
    "    two_icd_diagnosis_code,\n",
    "    three_icd_diagnosis_code,\n",
    "    four_icd_diagnosis_code,\n",
    "    five_icd_diagnosis_code,\n",
    "    procedure_code,\n",
    "    event_type\n",
    "  FROM read_parquet('%s')\n",
    "\", parquet_file)\n",
    "\n",
    "cohort_data <- dbGetQuery(con, query)\n",
    "dbDisconnect(con)\n",
    "\n",
    "cat(sprintf(\"Loaded %d event-level records\\n\", nrow(cohort_data)))\n",
    "cat(sprintf(\"Unique patients: %d\\n\", length(unique(cohort_data$mi_person_key))))\n",
    "\n",
    "# Print event type distribution\n",
    "cat(\"\\nEvent type distribution:\\n\")\n",
    "event_dist <- cohort_data %>%\n",
    "  count(event_type) %>%\n",
    "  mutate(pct = 100 * n / sum(n))\n",
    "for (i in 1:nrow(event_dist)) {\n",
    "  cat(sprintf(\"  %s: %d (%.1f%%)\\n\", event_dist$event_type[i], event_dist$n[i], event_dist$pct[i]))\n",
    "}\n",
    "\n",
    "# Print target distribution (at event level)\n",
    "cat(\"\\nTarget distribution (event-level):\\n\")\n",
    "target_dist <- cohort_data %>%\n",
    "  count(target) %>%\n",
    "  mutate(pct = 100 * n / sum(n))\n",
    "for (i in 1:nrow(target_dist)) {\n",
    "  cat(sprintf(\"  Target %d: %d events (%.1f%%)\\n\", target_dist$target[i], target_dist$n[i], target_dist$pct[i]))\n",
    "}\n",
    "\n",
    "# Print target distribution (patient-level)\n",
    "cat(\"\\nTarget distribution (patient-level):\\n\")\n",
    "patient_target_dist <- cohort_data %>%\n",
    "  select(mi_person_key, target) %>%\n",
    "  distinct() %>%\n",
    "  count(target) %>%\n",
    "  mutate(pct = 100 * n / sum(n))\n",
    "for (i in 1:nrow(patient_target_dist)) {\n",
    "  if (is.na(patient_target_dist$target[i])) {\n",
    "    cat(sprintf(\"  Target NA: %d patients (%.1f%%) ‚ö†Ô∏è  ISSUE: NULL is_target_case in source data\\n\", \n",
    "                patient_target_dist$n[i], patient_target_dist$pct[i]))\n",
    "  } else {\n",
    "    cat(sprintf(\"  Target %d: %d patients (%.1f%%)\\n\", \n",
    "                patient_target_dist$target[i], patient_target_dist$n[i], patient_target_dist$pct[i]))\n",
    "  }\n",
    "}\n",
    "\n",
    "# Check for patients with inconsistent target values across events\n",
    "cat(\"\\nChecking for patients with inconsistent target values:\\n\")\n",
    "inconsistent_patients <- cohort_data %>%\n",
    "  select(mi_person_key, target) %>%\n",
    "  distinct() %>%\n",
    "  group_by(mi_person_key) %>%\n",
    "  summarise(n_unique_targets = n_distinct(target, na.rm = FALSE), .groups = 'drop') %>%\n",
    "  filter(n_unique_targets > 1)\n",
    "\n",
    "if (nrow(inconsistent_patients) > 0) {\n",
    "  cat(sprintf(\"  ‚ö†Ô∏è  WARNING: %d patients have inconsistent target values across events!\\n\", \n",
    "              nrow(inconsistent_patients)))\n",
    "  cat(\"  This suggests is_target_case is calculated per-event instead of per-patient.\\n\")\n",
    "  cat(\"  Using first() value per patient as workaround.\\n\")\n",
    "} else {\n",
    "  cat(\"  ‚úì No inconsistencies found\\n\")\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "853059a9",
   "metadata": {},
   "source": [
    "## 3. Feature Engineering\n",
    "\n",
    "Create patient-level features with categorical factor columns for CatBoost.\n",
    "\n",
    "**Approach:**\n",
    "- Create feature columns where each column represents an item (drug, ICD code, CPT code)\n",
    "- Each column is a factor with levels based on the actual categorical values (item names)\n",
    "- Target remains binary (0/1)\n",
    "- For Random Forest: use numeric binary (0/1) format\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07a74527",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Creating patient-level features...\n",
      "Extracted 842791 unique patient-item pairs\n",
      "Unique items: 19586\n",
      "\n",
      "Creating feature matrix...\n",
      "Creating 19586 feature columns (one per unique item)\n",
      "\n",
      "Creating CatBoost format (item names as categorical values)...\n",
      "\n",
      "Validating feature matrix join:\n",
      "  feature_matrix_catboost rows: 31146\n",
      "  Target column present: TRUE\n"
     ]
    }
   ],
   "source": [
    "# Extract all unique items per patient\n",
    "cat(\"\\nCreating patient-level features...\\n\")\n",
    "\n",
    "patient_items <- cohort_data %>%\n",
    "  # Drug names (pharmacy events)\n",
    "  filter(!is.na(drug_name) & drug_name != \"\" & event_type == \"pharmacy\") %>%\n",
    "  select(mi_person_key, item = drug_name) %>%\n",
    "  # ICD codes (medical events) - all 5 columns\n",
    "  bind_rows(\n",
    "    cohort_data %>%\n",
    "      filter(event_type == \"medical\") %>%\n",
    "      select(mi_person_key, item = primary_icd_diagnosis_code) %>%\n",
    "      filter(!is.na(item) & item != \"\"),\n",
    "    cohort_data %>%\n",
    "      filter(event_type == \"medical\") %>%\n",
    "      select(mi_person_key, item = two_icd_diagnosis_code) %>%\n",
    "      filter(!is.na(item) & item != \"\"),\n",
    "    cohort_data %>%\n",
    "      filter(event_type == \"medical\") %>%\n",
    "      select(mi_person_key, item = three_icd_diagnosis_code) %>%\n",
    "      filter(!is.na(item) & item != \"\"),\n",
    "    cohort_data %>%\n",
    "      filter(event_type == \"medical\") %>%\n",
    "      select(mi_person_key, item = four_icd_diagnosis_code) %>%\n",
    "      filter(!is.na(item) & item != \"\"),\n",
    "    cohort_data %>%\n",
    "      filter(event_type == \"medical\") %>%\n",
    "      select(mi_person_key, item = five_icd_diagnosis_code) %>%\n",
    "      filter(!is.na(item) & item != \"\")\n",
    "  ) %>%\n",
    "  # CPT codes (medical events)\n",
    "  bind_rows(\n",
    "    cohort_data %>%\n",
    "      filter(!is.na(procedure_code) & procedure_code != \"\" & event_type == \"medical\") %>%\n",
    "      select(mi_person_key, item = procedure_code)\n",
    "  ) %>%\n",
    "  distinct() %>%\n",
    "  filter(!is.na(item) & item != \"\")\n",
    "\n",
    "cat(sprintf(\"Extracted %d unique patient-item pairs\\n\", nrow(patient_items)))\n",
    "cat(sprintf(\"Unique items: %d\\n\", length(unique(patient_items$item))))\n",
    "\n",
    "# Get target per patient\n",
    "patient_targets <- cohort_data %>%\n",
    "  select(mi_person_key, target) %>%\n",
    "  distinct() %>%\n",
    "  group_by(mi_person_key) %>%\n",
    "  summarise(target = first(target), .groups = 'drop')\n",
    "\n",
    "# Create feature matrix (one column per item)\n",
    "# For CatBoost: Use actual item names as categorical values\n",
    "# For Random Forest: Use binary 0/1\n",
    "\n",
    "cat(\"\\nCreating feature matrix...\\n\")\n",
    "\n",
    "# Get all unique items to create columns\n",
    "all_unique_items <- sort(unique(patient_items$item))\n",
    "cat(sprintf(\"Creating %d feature columns (one per unique item)\\n\", length(all_unique_items)))\n",
    "\n",
    "# FOR CATBOOST: Create feature matrix where each column represents an item\n",
    "# Value is the item name itself (categorical), or NA if patient doesn't have it\n",
    "cat(\"\\nCreating CatBoost format (item names as categorical values)...\\n\")\n",
    "feature_matrix_catboost <- patient_items %>%\n",
    "  pivot_wider(\n",
    "    id_cols = mi_person_key,\n",
    "    names_from = item,\n",
    "    values_from = item,  # Use item name itself as value (not 0/1)\n",
    "    values_fill = NA_character_,  # NA if patient doesn't have the item\n",
    "    names_prefix = \"item_\"\n",
    "  ) %>%\n",
    "  left_join(patient_targets, by = \"mi_person_key\")\n",
    "\n",
    "# Validate join - check if target column has values\n",
    "cat(\"\\nValidating feature matrix join:\\n\")\n",
    "cat(sprintf(\"  feature_matrix_catboost rows: %d\\n\", nrow(feature_matrix_catboost)))\n",
    "cat(sprintf(\"  Target column present: %s\\n\", \"target\" %in% names(feature_matrix_catboost)))\n",
    "if (\"target\" %in% names(feature_matrix_catboost)) {\n",
    "  cat(sprintf(\"  Target NAs: %d (%.1f%%)\\n\", \n",
    "              sum(is.na(feature_matrix_catboost$target)), \n",
    "              100 * mean(is.na(feature_matrix_catboost$target))))\n",
    "  cat(sprintf(\"  Target positives: %d (%.1f%%)\\n\", \n",
    "              sum(feature_matrix_catboost$target == 1, na.rm = TRUE),\n",
    "              100 * mean(feature_matrix_catboost$target == 1, na.rm = TRUE)))\n",
    "  cat(sprintf(\"  Target negatives: %d (%.1f%%)\\n\", \n",
    "              sum(feature_matrix_catboost$target == 0, na.rm = TRUE),\n",
    "              100 * mean(feature_matrix_catboost$target == 0, na.rm = TRUE)))\n",
    "  \n",
    "  # Check for patients missing from patient_targets\n",
    "  missing_targets <- feature_matrix_catboost %>%\n",
    "    filter(is.na(target)) %>%\n",
    "    select(mi_person_key) %>%\n",
    "    distinct()\n",
    "  if (nrow(missing_targets) > 0) {\n",
    "    cat(sprintf(\"  WARNING: %d patients missing target values\\n\", nrow(missing_targets)))\n",
    "  }\n",
    "}\n",
    "\n",
    "# FOR CATBOOST: Convert ALL feature columns to factors with actual item names as levels\n",
    "cat(\"\\nCreating CatBoost format (categorical factors with item names as levels)...\\n\")\n",
    "data_catboost <- feature_matrix_catboost %>%\n",
    "  select(-mi_person_key) %>%\n",
    "  # Convert ALL feature columns (except target) to factors\n",
    "  # Each column has levels: NA (patient doesn't have item) or the item name itself\n",
    "  mutate(across(-target, ~ as.factor(.x)))\n",
    "\n",
    "y_catboost <- data_catboost$target\n",
    "X_catboost <- data_catboost %>% select(-target)\n",
    "\n",
    "cat(sprintf(\"CatBoost format:\\n\"))\n",
    "cat(sprintf(\"  Patients: %d\\n\", nrow(X_catboost)))\n",
    "cat(sprintf(\"  Features: %d (categorical factors with item names as levels)\\n\", ncol(X_catboost)))\n",
    "cat(sprintf(\"  Target distribution: %d (%.1f%%) positive, %d (%.1f%%) negative\\n\",\n",
    "            sum(y_catboost == 1), 100 * mean(y_catboost == 1), \n",
    "            sum(y_catboost == 0), 100 * mean(y_catboost == 0)))\n",
    "\n",
    "# FOR RANDOM FOREST: Create binary feature matrix (0/1)\n",
    "cat(\"\\nCreating Random Forest format (numeric binary 0/1)...\\n\")\n",
    "feature_matrix_rf <- patient_items %>%\n",
    "  mutate(value = 1) %>%\n",
    "  pivot_wider(\n",
    "    id_cols = mi_person_key,\n",
    "    names_from = item,\n",
    "    values_from = value,\n",
    "    values_fill = 0,\n",
    "    names_prefix = \"item_\"\n",
    "  ) %>%\n",
    "  left_join(patient_targets, by = \"mi_person_key\")\n",
    "\n",
    "data_rf <- feature_matrix_rf %>%\n",
    "  select(-mi_person_key)\n",
    "# Keep as numeric (0/1) for Random Forest\n",
    "\n",
    "y_rf <- data_rf$target\n",
    "X_rf <- data_rf %>% select(-target)\n",
    "\n",
    "cat(sprintf(\"Random Forest format:\\n\"))\n",
    "cat(sprintf(\"  Patients: %d\\n\", nrow(X_rf)))\n",
    "cat(sprintf(\"  Features: %d (numeric binary)\\n\", ncol(X_rf)))\n",
    "\n",
    "# Use CatBoost format by default (categorical factors)\n",
    "data <- data_catboost\n",
    "y <- y_catboost\n",
    "X <- X_catboost\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f33981d5",
   "metadata": {},
   "source": [
    "## 4. Helper Functions\n",
    "\n",
    "Define functions for training models and calculating feature importance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15567823",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train CatBoost model (R) - uses categorical factor features\n",
    "train_catboost_r <- function(X_train, y_train, params) {\n",
    "  # X_train should have ALL columns as factors (categorical features)\n",
    "  # R's CatBoost automatically detects factor columns as categorical - no need to specify\n",
    "  # Verify all columns are factors\n",
    "  factor_cols <- names(X_train)[sapply(X_train, is.factor)]\n",
    "  if (length(factor_cols) != ncol(X_train)) {\n",
    "    warning(sprintf(\"Not all columns are factors! Factors: %d, Total: %d\", \n",
    "                    length(factor_cols), ncol(X_train)))\n",
    "  }\n",
    "  \n",
    "  # Create pool - R CatBoost automatically handles factor columns as categorical\n",
    "  train_pool <- catboost.load_pool(\n",
    "    data = X_train, \n",
    "    label = y_train\n",
    "  )\n",
    "  \n",
    "  catboost_params <- list(\n",
    "    iterations = params$iterations,\n",
    "    learning_rate = params$learning_rate,\n",
    "    depth = params$depth,\n",
    "    loss_function = 'Logloss',\n",
    "    eval_metric = 'Recall',\n",
    "    verbose = params$verbose,\n",
    "    logging_level = 'Silent',  # Suppress CatBoost logging to avoid thread safety warnings\n",
    "    random_seed = params$random_seed\n",
    "  )\n",
    "  \n",
    "  model <- catboost.train(train_pool, NULL, catboost_params)\n",
    "  return(model)\n",
    "}\n",
    "\n",
    "# Train Random Forest model (R)\n",
    "train_random_forest_r <- function(X_train, y_train, params) {\n",
    "  if (is.null(params$mtry)) {\n",
    "    params$mtry <- floor(sqrt(ncol(X_train)))\n",
    "  }\n",
    "  \n",
    "  y_train_factor <- as.factor(y_train)\n",
    "  \n",
    "  model <- randomForest(\n",
    "    x = X_train,\n",
    "    y = y_train_factor,\n",
    "    ntree = params$ntree,\n",
    "    mtry = params$mtry,\n",
    "    nodesize = params$nodesize,\n",
    "    maxnodes = params$maxnodes,\n",
    "    importance = TRUE\n",
    "  )\n",
    "  \n",
    "  return(model)\n",
    "}\n",
    "\n",
    "# Get feature importance from CatBoost (R)\n",
    "get_importance_catboost_r <- function(model, X_test) {\n",
    "  # R's CatBoost automatically detects factor columns as categorical - no need to specify\n",
    "  # Verify all columns are factors (should match training)\n",
    "  factor_cols <- names(X_test)[sapply(X_test, is.factor)]\n",
    "  if (length(factor_cols) != ncol(X_test)) {\n",
    "    warning(sprintf(\"Test data: Not all columns are factors! Factors: %d, Total: %d\", \n",
    "                    length(factor_cols), ncol(X_test)))\n",
    "  }\n",
    "  \n",
    "  # Create test pool - R CatBoost automatically handles factor columns as categorical\n",
    "  test_pool <- catboost.load_pool(data = X_test)\n",
    "  \n",
    "  # Get feature importance - ensure it returns a vector\n",
    "  importance <- catboost.get_feature_importance(model, pool = test_pool, type = 'PredictionValuesChange')\n",
    "  \n",
    "  # Ensure importance is a named vector with correct length\n",
    "  if (length(importance) == 1 && ncol(X_test) > 1) {\n",
    "    warning(sprintf(\"Feature importance returned single value instead of vector (expected %d features)\", ncol(X_test)))\n",
    "    # Fallback: return zeros with feature names\n",
    "    importance <- setNames(rep(0, ncol(X_test)), names(X_test))\n",
    "  } else if (length(importance) != ncol(X_test)) {\n",
    "    warning(sprintf(\"Feature importance length mismatch: got %d, expected %d\", length(importance), ncol(X_test)))\n",
    "    # Try to pad or truncate to match\n",
    "    if (length(importance) < ncol(X_test)) {\n",
    "      importance <- c(importance, rep(0, ncol(X_test) - length(importance)))\n",
    "    } else {\n",
    "      importance <- importance[1:ncol(X_test)]\n",
    "    }\n",
    "    names(importance) <- names(X_test)\n",
    "  } else if (is.null(names(importance))) {\n",
    "    # Ensure names are set\n",
    "    names(importance) <- names(X_test)\n",
    "  }\n",
    "  \n",
    "  return(importance)\n",
    "}\n",
    "\n",
    "# Get feature importance from Random Forest (R)\n",
    "get_importance_random_forest_r <- function(model) {\n",
    "  importance <- importance(model)[, \"MeanDecreaseGini\"]\n",
    "  return(importance)\n",
    "}\n",
    "\n",
    "# Calculate metrics for scaling\n",
    "# Handle NA values: remove them before calculation\n",
    "\n",
    "# Calculate Recall\n",
    "calculate_recall <- function(y_true, y_pred) {\n",
    "  # Check inputs\n",
    "  if (length(y_true) != length(y_pred)) {\n",
    "    warning(sprintf(\"Length mismatch: y_true=%d, y_pred=%d\", length(y_true), length(y_pred)))\n",
    "    return(0)\n",
    "  }\n",
    "  \n",
    "  # Remove NA values from both vectors\n",
    "  valid_idx <- !is.na(y_true) & !is.na(y_pred)\n",
    "  \n",
    "  if (sum(valid_idx) == 0) {\n",
    "    warning(sprintf(\"No valid predictions for recall calculation (y_true: %d NAs/%d, y_pred: %d NAs/%d)\", \n",
    "                    sum(is.na(y_true)), length(y_true), sum(is.na(y_pred)), length(y_pred)))\n",
    "    return(0)\n",
    "  }\n",
    "  \n",
    "  y_true_clean <- y_true[valid_idx]\n",
    "  y_pred_clean <- y_pred[valid_idx]\n",
    "  \n",
    "  # Check if we have any valid data\n",
    "  if (length(y_true_clean) == 0) {\n",
    "    warning(\"No valid data after filtering NAs\")\n",
    "    return(0)\n",
    "  }\n",
    "  \n",
    "  tp <- sum((y_true_clean == 1) & (y_pred_clean == 1))\n",
    "  fn <- sum((y_true_clean == 1) & (y_pred_clean == 0))\n",
    "  \n",
    "  # Handle edge case: no positive cases in true labels\n",
    "  if (tp + fn == 0) {\n",
    "    warning(sprintf(\"No positive cases in y_true for recall calculation (total valid: %d, positives: %d)\", \n",
    "                    length(y_true_clean), sum(y_true_clean == 1)))\n",
    "    return(0)\n",
    "  }\n",
    "  \n",
    "  return(tp / (tp + fn))\n",
    "}\n",
    "\n",
    "# Calculate LogLoss (logarithmic loss)\n",
    "# Lower is better, so we'll invert it for scaling (1/logloss or use negative)\n",
    "calculate_logloss <- function(y_true, y_pred_proba) {\n",
    "  # Remove NA values\n",
    "  valid_idx <- !is.na(y_true) & !is.na(y_pred_proba)\n",
    "  if (sum(valid_idx) == 0) {\n",
    "    warning(\"No valid predictions for logloss calculation\")\n",
    "    return(Inf)  # Return Inf so 1/logloss = 0\n",
    "  }\n",
    "  \n",
    "  y_true_clean <- y_true[valid_idx]\n",
    "  y_pred_proba_clean <- y_pred_proba[valid_idx]\n",
    "  \n",
    "  # Clip probabilities to avoid log(0) or log(1)\n",
    "  y_pred_proba_clean <- pmax(pmin(y_pred_proba_clean, 1 - 1e-15), 1e-15)\n",
    "  \n",
    "  # Calculate logloss\n",
    "  logloss <- -mean(y_true_clean * log(y_pred_proba_clean) + (1 - y_true_clean) * log(1 - y_pred_proba_clean))\n",
    "  \n",
    "  return(logloss)\n",
    "}\n",
    "\n",
    "# Predict with CatBoost (R) - returns binary predictions\n",
    "predict_catboost_r <- function(model, X_test) {\n",
    "  # R's CatBoost automatically detects factor columns as categorical - no need to specify\n",
    "  # Create test pool - R CatBoost automatically handles factor columns as categorical\n",
    "  test_pool <- catboost.load_pool(data = X_test)\n",
    "  pred_proba <- catboost.predict(model, test_pool, prediction_type = 'Probability')\n",
    "  \n",
    "  # Handle NA values: if pred_proba is NA, default to 0 (negative class)\n",
    "  pred <- ifelse(is.na(pred_proba), 0, ifelse(pred_proba > 0.5, 1, 0))\n",
    "  \n",
    "  # Ensure no NA values remain\n",
    "  if (any(is.na(pred))) {\n",
    "    warning(\"NA values in CatBoost predictions, replacing with 0\")\n",
    "    pred[is.na(pred)] <- 0\n",
    "  }\n",
    "  \n",
    "  return(pred)\n",
    "}\n",
    "\n",
    "# Predict probabilities with CatBoost (R) - returns probability values\n",
    "predict_proba_catboost_r <- function(model, X_test) {\n",
    "  test_pool <- catboost.load_pool(data = X_test)\n",
    "  pred_proba <- catboost.predict(model, test_pool, prediction_type = 'Probability')\n",
    "  \n",
    "  # Handle NA values\n",
    "  if (any(is.na(pred_proba))) {\n",
    "    warning(\"NA values in CatBoost probability predictions, replacing with 0.5\")\n",
    "    pred_proba[is.na(pred_proba)] <- 0.5\n",
    "  }\n",
    "  \n",
    "  return(pred_proba)\n",
    "}\n",
    "\n",
    "# Predict with Random Forest (R) - returns binary predictions\n",
    "predict_random_forest_r <- function(model, X_test) {\n",
    "  pred <- predict(model, X_test, type = 'response')\n",
    "  pred <- as.integer(pred) - 1  # Convert factor to 0/1\n",
    "  \n",
    "  # Handle NA values: if prediction is NA, default to 0 (negative class)\n",
    "  if (any(is.na(pred))) {\n",
    "    warning(\"NA values in Random Forest predictions, replacing with 0\")\n",
    "    pred[is.na(pred)] <- 0\n",
    "  }\n",
    "  \n",
    "  return(pred)\n",
    "}\n",
    "\n",
    "# Predict probabilities with Random Forest (R) - returns probability values\n",
    "predict_proba_random_forest_r <- function(model, X_test) {\n",
    "  pred_proba <- predict(model, X_test, type = 'prob')[, 2]  # Get probability of class 1\n",
    "  \n",
    "  # Handle NA values\n",
    "  if (any(is.na(pred_proba))) {\n",
    "    warning(\"NA values in Random Forest probability predictions, replacing with 0.5\")\n",
    "    pred_proba[is.na(pred_proba)] <- 0.5\n",
    "  }\n",
    "  \n",
    "  return(pred_proba)\n",
    "}\n",
    "\n",
    "cat(\"‚úì Helper functions defined\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2de9768",
   "metadata": {},
   "source": [
    "## 5. Monte Carlo Cross-Validation\n",
    "\n",
    "Run MC-CV for each model type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c739951",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create MC-CV splits (stratified by target)\n",
    "# Note: We use the CatBoost format (categorical) for splits, but Random Forest will use its own format\n",
    "cat(\"\\n========================================\\n\")\n",
    "cat(\"Creating MC-CV Splits\\n\")\n",
    "cat(\"========================================\\n\")\n",
    "\n",
    "# Validate data before creating splits\n",
    "if (!\"target\" %in% names(data)) {\n",
    "  stop(\"Target column not found in data. Cannot create MC-CV splits.\")\n",
    "}\n",
    "if (all(is.na(data$target))) {\n",
    "  stop(\"Target column is all NA. Cannot create MC-CV splits. Check data loading.\")\n",
    "}\n",
    "cat(sprintf(\"Data for splits (before cleaning): nrow=%d, target NAs=%d, target positives=%d\\n\",\n",
    "            nrow(data), sum(is.na(data$target)), sum(data$target == 1, na.rm = TRUE)))\n",
    "\n",
    "# CRITICAL FIX: Remove any rows with NA target BEFORE creating splits\n",
    "# mc_cv() with strata cannot handle NA values in the strata variable\n",
    "data_clean <- data %>% filter(!is.na(target))\n",
    "if (nrow(data_clean) < nrow(data)) {\n",
    "  warning(sprintf(\"Removed %d rows with NA target values before MC-CV\", nrow(data) - nrow(data_clean)))\n",
    "}\n",
    "data <- data_clean\n",
    "\n",
    "# Also update data_rf to match (remove same rows if they exist)\n",
    "if (exists(\"data_rf\")) {\n",
    "  data_rf_clean <- data_rf %>% filter(!is.na(target))\n",
    "  if (nrow(data_rf_clean) < nrow(data_rf)) {\n",
    "    warning(sprintf(\"Removed %d rows from data_rf with NA target values\", nrow(data_rf) - nrow(data_rf_clean)))\n",
    "  }\n",
    "  data_rf <- data_rf_clean\n",
    "}\n",
    "\n",
    "# Validate minimum sample sizes for stratification\n",
    "target_counts <- table(data$target)\n",
    "min_samples_needed <- ceiling(N_SPLITS * TEST_SIZE * 2)  # Need at least 2 samples per class per split\n",
    "cat(sprintf(\"\\nAfter cleaning:\\n\"))\n",
    "cat(sprintf(\"  Total samples: %d\\n\", nrow(data)))\n",
    "cat(sprintf(\"  Target distribution: 0=%d (%.1f%%), 1=%d (%.1f%%)\\n\", \n",
    "            target_counts[1], 100*target_counts[1]/sum(target_counts),\n",
    "            target_counts[2], 100*target_counts[2]/sum(target_counts)))\n",
    "cat(sprintf(\"  Min samples needed per class: %d (for %d splits with %.0f%% test)\\n\",\n",
    "            min_samples_needed, N_SPLITS, TEST_SIZE*100))\n",
    "\n",
    "if (any(target_counts < min_samples_needed)) {\n",
    "  warning(sprintf(\"‚ö†Ô∏è  Small sample size detected for stratification!\\n\"))\n",
    "  warning(sprintf(\"   Consider: reducing N_SPLITS, increasing TEST_SIZE, or using more data.\\n\"))\n",
    "}\n",
    "\n",
    "mc_splits <- mc_cv(\n",
    "  data = data,\n",
    "  prop = TRAIN_PROP,\n",
    "  times = N_SPLITS,\n",
    "  strata = target  # Stratified by target (bare name - rsample uses non-standard evaluation)\n",
    ")\n",
    "\n",
    "cat(sprintf(\"\\n‚úì Created %d MC-CV splits (stratified)\\n\", N_SPLITS))\n",
    "\n",
    "# DIAGNOSTIC: Check first split indices\n",
    "cat(\"\\nValidating split indices...\\n\")\n",
    "first_split <- mc_splits$splits[[1]]\n",
    "\n",
    "# Training indices (stored by rsample)\n",
    "train_idx <- first_split$in_id\n",
    "\n",
    "# Test indices = complement of train indices\n",
    "test_idx  <- setdiff(seq_len(nrow(data)), train_idx)\n",
    "\n",
    "cat(sprintf(\n",
    "  \"  Split 1 train_idx: length=%d, NAs=%d, range=[%d, %d]\\n\",\n",
    "  length(train_idx), sum(is.na(train_idx)),\n",
    "  as.integer(min(train_idx, na.rm = TRUE)),\n",
    "  as.integer(max(train_idx, na.rm = TRUE))\n",
    "))\n",
    "\n",
    "cat(sprintf(\n",
    "  \"  Split 1 test_idx: length=%d, NAs=%d, range=[%d, %d]\\n\",\n",
    "  length(test_idx), sum(is.na(test_idx)),\n",
    "  as.integer(min(test_idx, na.rm = TRUE)),\n",
    "  as.integer(max(test_idx, na.rm = TRUE))\n",
    "))\n",
    "\n",
    "cat(sprintf(\n",
    "  \"  Data nrow=%d, target length=%d\\n\",\n",
    "  nrow(data), length(data$target)\n",
    "))\n",
    "\n",
    "if (sum(is.na(train_idx)) > 0 || sum(is.na(test_idx)) > 0) {\n",
    "  stop(sprintf(\n",
    "    \"‚ùå SPLIT CONTAINS NA INDICES! This suggests:\\n  1. rsample package bug\\n  2. Data structure issue\\n  3. Target column type mismatch\\n\\nDebugging info:\\n  - data class: %s\\n  - target class: %s\\n  - data has row.names: %s\",\n",
    "    paste(class(data), collapse = \", \"),\n",
    "    paste(class(data$target), collapse = \", \"),\n",
    "    !is.null(row.names(data))\n",
    "  ))\n",
    "}\n",
    "\n",
    "cat(\"  ‚úì No NA indices found in splits\\n\")\n",
    "cat(\"Note: CatBoost uses categorical features, Random Forest uses one-hot encoded features\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65ee8b83-5639-4e1f-b346-105534448702",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_mc_cv_method <- function(data, method, mc_splits, data_rf = NULL) {\n",
    "  cat(sprintf(\"\\n--- Running MC-CV for %s ---\\n\", method))\n",
    "\n",
    "  # Select features and target based on model type ----------------------------\n",
    "  if (method == \"catboost\") {\n",
    "    X_all <- data %>% dplyr::select(-target)\n",
    "    y_all <- data$target\n",
    "  } else if (method == \"random_forest\") {\n",
    "    if (is.null(data_rf)) stop(\"Random Forest requires data_rf\")\n",
    "    X_all <- data_rf %>% dplyr::select(-target)\n",
    "    y_all <- data_rf$target\n",
    "  } else {\n",
    "    stop(sprintf(\"Unknown method: %s\", method))\n",
    "  }\n",
    "\n",
    "  feature_names <- colnames(X_all)\n",
    "  n_obs <- length(y_all)\n",
    "\n",
    "  # Progress bar --------------------------------------------------------------\n",
    "  p <- progressor(steps = N_SPLITS)\n",
    "\n",
    "  # Core MC-CV loop -----------------------------------------------------------\n",
    "  results <- future_map(\n",
    "    1:N_SPLITS,\n",
    "    function(i) {\n",
    "      p()\n",
    "\n",
    "      split <- mc_splits$splits[[i]]\n",
    "\n",
    "      # Slice train/test: CatBoost uses rsample accessor, RF uses indices\n",
    "      if (method == \"catboost\") {\n",
    "        train_data <- rsample::analysis(split)\n",
    "        test_data  <- rsample::assessment(split)\n",
    "\n",
    "        X_train <- train_data %>% dplyr::select(-target)\n",
    "        y_train <- train_data$target\n",
    "\n",
    "        X_test  <- test_data %>% dplyr::select(-target)\n",
    "        y_test  <- test_data$target\n",
    "\n",
    "      } else {  # random forest\n",
    "        train_idx <- split$in_id\n",
    "        test_idx  <- setdiff(seq_len(n_obs), train_idx)\n",
    "\n",
    "        X_train <- X_all[train_idx, , drop = FALSE]\n",
    "        X_test  <- X_all[test_idx,  , drop = FALSE]\n",
    "        y_train <- y_all[train_idx]\n",
    "        y_test  <- y_all[test_idx]\n",
    "      }\n",
    "\n",
    "      # Train --------------------------------------------------------------\n",
    "      if (method == \"catboost\") {\n",
    "        model <- train_catboost_r(X_train, y_train, MODEL_PARAMS$catboost)\n",
    "      } else {\n",
    "        model <- train_random_forest_r(X_train, y_train, MODEL_PARAMS$random_forest)\n",
    "      }\n",
    "\n",
    "      # Predict ------------------------------------------------------------\n",
    "      if (method == \"catboost\") {\n",
    "        y_pred       <- predict_catboost_r(model, X_test)\n",
    "        y_pred_proba <- predict_proba_catboost_r(model, X_test)\n",
    "      } else {\n",
    "        y_pred       <- predict_random_forest_r(model, X_test)\n",
    "        y_pred_proba <- predict_proba_random_forest_r(model, X_test)\n",
    "      }\n",
    "\n",
    "      # Metric -------------------------------------------------------------\n",
    "      if (SCALING_METRIC == \"recall\") {\n",
    "        metric_value <- calculate_recall(y_test, y_pred)\n",
    "      } else {\n",
    "        logloss <- calculate_logloss(y_test, y_pred_proba)\n",
    "        metric_value <- if (is.infinite(logloss) || logloss == 0) 0 else 1 / logloss\n",
    "      }\n",
    "\n",
    "      # Feature importance -------------------------------------------------\n",
    "      if (method == \"catboost\") {\n",
    "        imp <- get_importance_catboost_r(model, X_test)\n",
    "      } else {\n",
    "        imp <- get_importance_random_forest_r(model)\n",
    "      }\n",
    "\n",
    "      # Fix importance length if needed\n",
    "      if (length(imp) != length(feature_names)) {\n",
    "        imp <- rep(0, length(feature_names))\n",
    "      }\n",
    "      names(imp) <- feature_names\n",
    "\n",
    "      list(metric = metric_value, imp = imp)\n",
    "    },\n",
    "    .options = furrr_options(seed = 42)\n",
    "  )\n",
    "\n",
    "  # Aggregate ---------------------------------------------------------------\n",
    "  metric_values <- purrr::map_dbl(results, \"metric\")\n",
    "  imp_matrix    <- do.call(rbind, purrr::map(results, \"imp\"))\n",
    "\n",
    "  avg_imp <- colMeans(imp_matrix)\n",
    "\n",
    "  # Normalize 0‚Äì1\n",
    "  if (max(avg_imp) > min(avg_imp)) {\n",
    "    norm_imp <- (avg_imp - min(avg_imp)) / (max(avg_imp) - min(avg_imp))\n",
    "  } else {\n",
    "    norm_imp <- rep(1 / length(avg_imp), length(avg_imp))\n",
    "  }\n",
    "\n",
    "  scaled_imp <- norm_imp * mean(metric_values)\n",
    "\n",
    "  results_df <- tibble::tibble(\n",
    "    feature               = feature_names,\n",
    "    importance_raw        = avg_imp,\n",
    "    importance_normalized = norm_imp,\n",
    "    importance_scaled     = scaled_imp,\n",
    "    model_type            = method,\n",
    "    mc_cv_mean            = mean(metric_values),\n",
    "    mc_cv_sd              = sd(metric_values)\n",
    "  ) %>%\n",
    "    dplyr::arrange(dplyr::desc(importance_scaled)) %>%\n",
    "    dplyr::mutate(rank = dplyr::row_number())\n",
    "\n",
    "  cat(sprintf(\"  Mean metric: %.4f ¬± %.4f\\n\",\n",
    "              mean(metric_values), sd(metric_values)))\n",
    "  cat(\"  Top features:\\n\")\n",
    "  print(head(results_df, 50))\n",
    "\n",
    "  results_df\n",
    "}\n",
    "\n",
    "cat(\"‚úì MC-CV (simplified) function defined\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a3b2699",
   "metadata": {},
   "source": [
    "## 6. Run Analysis\n",
    "\n",
    "Run MC-CV for each model type.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d67d8d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run each method\n",
    "methods <- c(\"catboost\", \"random_forest\")\n",
    "all_results <- list()\n",
    "\n",
    "# Note: data_rf was created in the feature engineering step above\n",
    "# Both data and data_rf should have the same number of rows and same patient order\n",
    "\n",
    "for (method in methods) {\n",
    "  if (method == \"random_forest\") {\n",
    "    result <- run_mc_cv_method(data, method, mc_splits, data_rf = data_rf)\n",
    "  } else {\n",
    "    result <- run_mc_cv_method(data, method, mc_splits)\n",
    "  }\n",
    "  all_results[[method]] <- result\n",
    "  \n",
    "  # Save individual results\n",
    "  output_file <- file.path(output_dir, sprintf(\"%s_%s_%s_%s_feature_importance.csv\",\n",
    "                                                COHORT_NAME, \n",
    "                                                AGE_BAND, \n",
    "                                                EVENT_YEAR, \n",
    "                                                method))\n",
    "  write_csv(result, output_file)\n",
    "  cat(sprintf(\"Saved: %s\\n\", output_file))\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33bb106e",
   "metadata": {},
   "source": [
    "## 7. Aggregate Results\n",
    "\n",
    "Combine results across models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9df12cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate across models using UNION of Top 50 from each model\n",
    "cat(\"\\n========================================\\n\")\n",
    "cat(\"Aggregating Results: Union of Top 50 Features\\n\")\n",
    "cat(\"========================================\\n\")\n",
    "\n",
    "# Get metric column names dynamically\n",
    "metric_mean_col <- sprintf(\"mc_cv_%s_mean\", if (SCALING_METRIC == \"recall\") \"recall\" else \"logloss_inverted\")\n",
    "metric_std_col <- sprintf(\"mc_cv_%s_std\", if (SCALING_METRIC == \"recall\") \"recall\" else \"logloss_inverted\")\n",
    "\n",
    "# Step 1: Get top 50 from each model WITH performance scaling\n",
    "top50_per_model <- list()\n",
    "model_recalls <- list()\n",
    "\n",
    "for (method in names(all_results)) {\n",
    "  # Get average recall for this model across all features\n",
    "  model_recall <- mean(all_results[[method]][[metric_mean_col]], na.rm = TRUE)\n",
    "  model_recalls[[method]] <- model_recall\n",
    "  \n",
    "  top50 <- all_results[[method]] %>%\n",
    "    arrange(desc(importance_normalized)) %>%  # Use normalized permutation importance\n",
    "    head(50) %>%\n",
    "    mutate(\n",
    "      importance_scaled = importance_normalized * .data[[metric_mean_col]],  # Scale by feature's recall\n",
    "      model = method,\n",
    "      model_recall = model_recall\n",
    "    ) %>%\n",
    "    select(feature, importance_normalized, importance_scaled, model, model_recall, \n",
    "           all_of(c(metric_mean_col, metric_std_col)))\n",
    "  \n",
    "  top50_per_model[[method]] <- top50\n",
    "  cat(sprintf(\"‚úì %s: Top 50 features | Mean Recall = %.4f\\n\", method, model_recall))\n",
    "}\n",
    "\n",
    "# Step 2: Union of features from both models\n",
    "all_top_features <- bind_rows(top50_per_model)\n",
    "cat(sprintf(\"\\nTotal feature-model combinations: %d\\n\", nrow(all_top_features)))\n",
    "\n",
    "# Step 3: Aggregate by feature - SUM scaled importances where overlap\n",
    "aggregated <- all_top_features %>%\n",
    "  group_by(feature) %>%\n",
    "  summarise(\n",
    "    importance_normalized = sum(importance_normalized),  # SUM normalized importances\n",
    "    importance_scaled = sum(importance_scaled),  # SUM Recall-scaled importances\n",
    "    n_models = n(),  # How many models included this feature\n",
    "    models = paste(model, collapse = \", \"),  # Which models\n",
    "    recall_mean = mean(.data[[metric_mean_col]]),  # Average feature recall across models\n",
    "    recall_std = mean(.data[[metric_std_col]]),\n",
    "    .groups = 'drop'\n",
    "  ) %>%\n",
    "  arrange(desc(importance_scaled)) %>%  # Rank by scaled importance (quality-weighted)\n",
    "  mutate(rank = row_number())\n",
    "\n",
    "# Rename recall columns for clarity\n",
    "names(aggregated)[names(aggregated) == \"recall_mean\"] <- metric_mean_col\n",
    "names(aggregated)[names(aggregated) == \"recall_std\"] <- metric_std_col\n",
    "\n",
    "# Print summary\n",
    "cat(sprintf(\"\\nUnion Summary:\\n\"))\n",
    "cat(sprintf(\"  Total unique features: %d\\n\", nrow(aggregated)))\n",
    "cat(sprintf(\"  Features in both models: %d (%.1f%%)\\n\", \n",
    "            sum(aggregated$n_models == 2),\n",
    "            100 * sum(aggregated$n_models == 2) / nrow(aggregated)))\n",
    "cat(sprintf(\"  Features in CatBoost only: %d\\n\", \n",
    "            sum(aggregated$n_models == 1 & grepl(\"catboost\", aggregated$models, ignore.case = TRUE))))\n",
    "cat(sprintf(\"  Features in Random Forest only: %d\\n\", \n",
    "            sum(aggregated$n_models == 1 & grepl(\"random\", aggregated$models, ignore.case = TRUE))))\n",
    "cat(sprintf(\"\\nRanking Method: Sum of Recall-scaled importances (quality-weighted)\\n\"))\n",
    "cat(sprintf(\"  - Each feature's importance is scaled by its MC-CV Recall\\n\"))\n",
    "cat(sprintf(\"  - When a feature appears in both models, scaled importances are summed\\n\"))\n",
    "\n",
    "# Save aggregated results locally\n",
    "output_file <- file.path(output_dir, sprintf(\"%s_%s_%d_feature_importance_aggregated.csv\",\n",
    "                                              COHORT_NAME, AGE_BAND, EVENT_YEAR))\n",
    "write_csv(aggregated, output_file)\n",
    "cat(sprintf(\"Saved locally: %s\\n\", output_file))\n",
    "\n",
    "# Upload to S3 (final results repository)\n",
    "cat(\"\\nUploading final results to S3...\\n\")\n",
    "s3_base <- \"s3://pgxdatalake/gold/feature_importance\"\n",
    "s3_path <- sprintf(\"%s/cohort_name=%s/age_band=%s/event_year=%d/%s_%s_%d_feature_importance_aggregated.csv\",\n",
    "                   s3_base, COHORT_NAME, AGE_BAND, EVENT_YEAR,\n",
    "                   COHORT_NAME, AGE_BAND, EVENT_YEAR)\n",
    "\n",
    "# Find AWS CLI\n",
    "aws_cmd <- Sys.which(\"aws\")\n",
    "if (aws_cmd == \"\") {\n",
    "  aws_paths <- c(\"/usr/local/bin/aws\", \"/usr/bin/aws\", \"/home/ec2-user/.local/bin/aws\")\n",
    "  for (path in aws_paths) {\n",
    "    if (file.exists(path)) {\n",
    "      aws_cmd <- path\n",
    "      break\n",
    "    }\n",
    "  }\n",
    "}\n",
    "\n",
    "if (!is.null(aws_cmd) && aws_cmd != \"\") {\n",
    "  upload_cmd <- sprintf('\"%s\" s3 cp \"%s\" \"%s\"', aws_cmd, output_file, s3_path)\n",
    "  result <- system(upload_cmd)\n",
    "  \n",
    "  if (result == 0) {\n",
    "    cat(sprintf(\"‚úì Uploaded to S3: %s\\n\", s3_path))\n",
    "  } else {\n",
    "    warning(sprintf(\"S3 upload failed (exit code %d). File saved locally.\", result))\n",
    "  }\n",
    "} else {\n",
    "  cat(\"‚ö† AWS CLI not found. File saved locally only.\\n\")\n",
    "  cat(\"To upload manually: aws s3 cp\", output_file, s3_path, \"\\n\")\n",
    "}\n",
    "\n",
    "# Print summary\n",
    "cat(\"\\n========================================\\n\")\n",
    "cat(\"Summary\\n\")\n",
    "cat(\"========================================\\n\")\n",
    "cat(sprintf(\"Total features: %d\\n\", nrow(aggregated)))\n",
    "cat(sprintf(\"Models used: %s\\n\", paste(methods, collapse = \", \")))\n",
    "cat(sprintf(\"Mean MC-CV %s: %.4f\\n\", \n",
    "            if (SCALING_METRIC == \"recall\") \"Recall\" else \"LogLoss (inverted)\",\n",
    "            mean(aggregated[[metric_mean_col]])))\n",
    "cat(\"\\nTop 50 features:\\n\")\n",
    "top50 <- head(aggregated, 50)\n",
    "for (i in 1:nrow(top50)) {\n",
    "  cat(sprintf(\"  %2d. %-40s | scaled=%.6f | %s=%.4f\\n\",\n",
    "              top50$rank[i], top50$feature[i], top50$importance_scaled[i], \n",
    "              if (SCALING_METRIC == \"recall\") \"recall\" else \"logloss_inv\",\n",
    "              top50[[metric_mean_col]][i]))\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "viz-section",
   "metadata": {},
   "source": [
    "## 7. Create Visualizations\n",
    "\n",
    "Generate plots showing:\n",
    "- **Top 50 features** by scaled importance (bar chart)\n",
    "- **Top 50 features** with Recall confidence (color-coded by quality, 95% CI)\n",
    "  - Color gradient: Orange (lower Recall) ‚Üí Dark Blue (higher Recall)\n",
    "  - Bar height represents scaled importance\n",
    "- **Normalized vs Recall-scaled** comparison (top 50, side-by-side)\n",
    "  - Shows impact of model quality weighting\n",
    "- **Feature category distribution** (Drug/ICD/CPT breakdown)\n",
    "\n",
    "Plots are saved locally to `outputs/plots/` and uploaded to S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "create-viz",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source visualization script\n",
    "source(\"create_visualizations.R\")\n",
    "\n",
    "# Create visualizations\n",
    "cat(\"\\n\", rep(\"=\", 80), \"\\n\", sep=\"\")\n",
    "cat(\"Creating Feature Importance Visualizations\\n\")\n",
    "cat(rep(\"=\", 80), \"\\n\", sep=\"\")\n",
    "\n",
    "plot_files <- create_feature_importance_plots(\n",
    "  aggregated_file = output_file,\n",
    "  output_dir = output_dir,\n",
    "  s3_upload = TRUE,\n",
    "  cohort_name = COHORT_NAME,\n",
    "  age_band = AGE_BAND,\n",
    "  event_year = EVENT_YEAR\n",
    ")\n",
    "\n",
    "cat(\"\\n‚úì All visualizations complete\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea613ec0",
   "metadata": {},
   "source": [
    "# Cleanup\n",
    "\n",
    "Close parallel processing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "425a78ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close parallel processing\n",
    "plan(sequential)\n",
    "\n",
    "cat(\"\\n========================================\\n\")\n",
    "cat(\"Analysis Complete!\\n\")\n",
    "cat(\"========================================\\n\")\n",
    "cat(sprintf(\"Local output directory: %s\\n\", output_dir))\n",
    "cat(sprintf(\"S3 output location: s3://pgxdatalake/gold/feature_importance/cohort_name=%s/age_band=%s/event_year=%d/\\n\",\n",
    "            COHORT_NAME, AGE_BAND, EVENT_YEAR))\n",
    "cat(sprintf(\"MC-CV splits: %d\\n\", N_SPLITS))\n",
    "cat(sprintf(\"Train/Test ratio: %.0f/%.0f\\n\", TRAIN_PROP * 100, TEST_SIZE * 100))\n",
    "cat(\"\\nResults show scaled feature importance with MC-CV Recall scores\\n\")\n",
    "cat(\"based on\", N_SPLITS, \"independent train/test splits.\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "065a50ea-25c9-4119-8ede-abc6a754e136",
   "metadata": {},
   "source": [
    "# Sync Results and Code to S3\n",
    "\n",
    "Sync output files and code (notebook + R script) to S3 bucket. \n",
    "- Outputs: CSV results files\n",
    "- Code: Notebook and R script for reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "861a2d43-3d45-4f58-9489-4380acfccd6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sync outputs and code to S3\n",
    "# On EC2, we're in the feature_importance directory  \n",
    "s3_bucket <- \"s3://pgx-repository/pgx-analysis/3_feature_importance/\"\n",
    "\n",
    "# Find AWS CLI (check common locations - EC2 typically has it in /usr/local/bin or /usr/bin)\n",
    "aws_cmd <- Sys.which(\"aws\")\n",
    "if (aws_cmd == \"\") {\n",
    "  # Try common EC2 installation paths\n",
    "  aws_paths <- c(\n",
    "    \"/usr/local/bin/aws\",\n",
    "    \"/usr/bin/aws\",\n",
    "    \"/home/ec2-user/.local/bin/aws\"\n",
    "  )\n",
    "  aws_cmd <- NULL\n",
    "  for (path in aws_paths) {\n",
    "    if (file.exists(path)) {\n",
    "      aws_cmd <- path\n",
    "      break\n",
    "    }\n",
    "  }\n",
    "  if (is.null(aws_cmd)) {\n",
    "    stop(\"AWS CLI not found. Please install AWS CLI or ensure it's in your PATH.\")\n",
    "  }\n",
    "}\n",
    "\n",
    "cat(\"Syncing outputs and code to S3...\\n\")\n",
    "cat(\"Source: feature_importance/ directory\\n\")\n",
    "cat(\"Destination:\", s3_bucket, \"\\n\")\n",
    "cat(\"AWS CLI:\", aws_cmd, \"\\n\\n\")\n",
    "\n",
    "# Get current directory (should be feature_importance)\n",
    "current_dir <- getwd()\n",
    "if (!grepl(\"feature_importance\", current_dir)) {\n",
    "  warning(\"Current directory doesn't appear to be feature_importance. Double-check sync destination.\")\n",
    "}\n",
    "\n",
    "# Sync feature_importance directory (includes outputs/ and code files)\n",
    "# Explicitly include notebook, R scripts, README files, and outputs directory\n",
    "# Exclude temporary files, checkpoints, and unnecessary directories\n",
    "# Note: --delete flag removed for safety (won't delete files in S3 that don't exist locally)\n",
    "# Include patterns are processed before exclude patterns, then exclude everything else\n",
    "sync_cmd <- sprintf(\n",
    "  '\"%s\" s3 sync \"%s\" %s --include \"*.ipynb\" --include \"*.R\" --include \"README*.md\" --include \"outputs/**\" --exclude \"*checkpoint*\" --exclude \"*.tmp\" --exclude \"*.ipynb_checkpoints/*\" --exclude \"*.RData\" --exclude \"*.Rhistory\" --exclude \".Rproj.user/*\" --exclude \"catboost_info/*\" --exclude \"*.log\" --exclude \"*\"',\n",
    "  aws_cmd,\n",
    "  current_dir,\n",
    "  s3_bucket\n",
    ")\n",
    "\n",
    "cat(\"Running:\", sync_cmd, \"\\n\\n\")\n",
    "result <- system(sync_cmd)\n",
    "\n",
    "if (result == 0) {\n",
    "  cat(\"‚úì Successfully synced outputs and code to S3\\n\")\n",
    "  cat(\"  - Outputs:\", file.path(output_dir), \"\\n\")\n",
    "  cat(\"  - Code: *.ipynb, *.R, README*.md\\n\")\n",
    "} else {\n",
    "  warning(sprintf(\"S3 sync returned exit code %d. Check AWS credentials and permissions.\", result))\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "832bf4fd-dc62-4c54-89a2-1b9d2d251a76",
   "metadata": {},
   "source": [
    "# Shutdown EC2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6315b0bf-daaa-4543-a9f0-a0a663f3daf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Shutdown EC2 instance after analysis completes\n",
    "# Set SHUTDOWN_EC2 = TRUE to enable, FALSE to disable\n",
    "SHUTDOWN_EC2 <- TRUE  # Change to TRUE to enable auto-shutdown\n",
    "\n",
    "if (SHUTDOWN_EC2) {\n",
    "  cat(\"\\n========================================\\n\")\n",
    "  cat(\"Shutting down EC2 instance...\\n\")\n",
    "  cat(\"========================================\\n\")\n",
    "  \n",
    "  # Get instance ID from EC2 metadata service\n",
    "  instance_id <- tryCatch({\n",
    "    system(\"curl -s http://169.254.169.254/latest/meta-data/instance-id\", intern = TRUE)\n",
    "  }, error = function(e) {\n",
    "    cat(\"Warning: Could not retrieve instance ID from metadata service.\\n\")\n",
    "    cat(\"If running on EC2, check that metadata service is accessible.\\n\")\n",
    "    return(NULL)\n",
    "  })\n",
    "  \n",
    "  if (!is.null(instance_id) && length(instance_id) > 0 && nchar(instance_id[1]) > 0) {\n",
    "    instance_id <- instance_id[1]\n",
    "    cat(sprintf(\"Instance ID: %s\\n\", instance_id))\n",
    "    \n",
    "    # Find AWS CLI\n",
    "    aws_cmd <- Sys.which(\"aws\")\n",
    "    if (aws_cmd == \"\") {\n",
    "      aws_paths <- c(\n",
    "        \"/usr/local/bin/aws\",\n",
    "        \"/usr/bin/aws\",\n",
    "        \"/home/ec2-user/.local/bin/aws\"\n",
    "      )\n",
    "      aws_cmd <- NULL\n",
    "      for (path in aws_paths) {\n",
    "        if (file.exists(path)) {\n",
    "          aws_cmd <- path\n",
    "          break\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "    \n",
    "    if (!is.null(aws_cmd) && aws_cmd != \"\") {\n",
    "      # Stop the instance (use terminate-instances for permanent deletion)\n",
    "      shutdown_cmd <- sprintf(\n",
    "        '\"%s\" ec2 stop-instances --instance-ids %s',\n",
    "        aws_cmd,\n",
    "        instance_id\n",
    "      )\n",
    "      \n",
    "      cat(\"Running:\", shutdown_cmd, \"\\n\")\n",
    "      result <- system(shutdown_cmd)\n",
    "      \n",
    "      if (result == 0) {\n",
    "        cat(\"‚úì EC2 instance stop command sent successfully\\n\")\n",
    "        cat(\"Instance will stop in a few moments.\\n\")\n",
    "        cat(\"Note: This is a STOP (not terminate), so you can restart it later.\\n\")\n",
    "      } else {\n",
    "        warning(sprintf(\"EC2 stop command returned exit code %d. Check AWS credentials and permissions.\", result))\n",
    "      }\n",
    "    } else {\n",
    "      cat(\"Warning: AWS CLI not found. Cannot shutdown instance.\\n\")\n",
    "      cat(\"Install AWS CLI or ensure it's in your PATH.\\n\")\n",
    "    }\n",
    "  } else {\n",
    "    cat(\"Warning: Could not determine instance ID. Skipping shutdown.\\n\")\n",
    "    cat(\"If you want to shutdown manually, use:\\n\")\n",
    "    cat(\"  aws ec2 stop-instances --instance-ids <your-instance-id>\\n\")\n",
    "  }\n",
    "} else {\n",
    "  cat(\"\\n========================================\\n\")\n",
    "  cat(\"EC2 Auto-Shutdown: DISABLED\\n\")\n",
    "  cat(\"========================================\\n\")\n",
    "  cat(\"To enable auto-shutdown, set SHUTDOWN_EC2 = TRUE in this cell.\\n\")\n",
    "  cat(\"Instance will continue running.\\n\")\n",
    "}\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "python",
   "pygments_lexer": "r",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
