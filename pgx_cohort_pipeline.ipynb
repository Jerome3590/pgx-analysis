{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e22d451-f32c-4d62-955c-38707123a539",
   "metadata": {},
   "source": [
    "## PGx Cohort Pipeline (Driver Notebook)\n",
    "\n",
    "This notebook serves as the **top-level driver** for running the PGx analysis pipeline for a given `(cohort_name, age_band)` pair.\n",
    "\n",
    "### High-Level Phases\n",
    "\n",
    "1. **Data Ingestion & Cleaning** (`1_apcd_input_data/`) ‚Äì raw TXT ‚Üí parquet, global imputation, QA (detailed cells below in this notebook).\n",
    "2. **Cohort Construction** (`2_create_cohort/`) ‚Äì builds `cohorts_F1120` from medical + pharmacy.\n",
    "3. **Feature Importance** (`3_feature_importance/`) ‚Äì Monte Carlo CV feature importance with **CatBoost, XGBoost, XGBoost RF**.\n",
    "4. **FP-Growth** (`4_fpgrowth_analysis/`) ‚Äì frequent pattern mining and drug encoding.\n",
    "5. **Process Mining (bupaR)** (`5_bupaR_analysis/`) ‚Äì event logs, time-to-event, and time-windowed patterns.\n",
    "6. **DTW Trajectories** (`6_dtw_analysis/`) ‚Äì trajectory prototypes and DTW distances.\n",
    "7. **Final Model Ensemble** (`7_final_model/`) ‚Äì final feature table + CatBoost / XGBoost / XGBoost RF ensemble.\n",
    "8. **Formal Feature Analysis** (`8_ffa_analysis/`) ‚Äì calibration, symbolic rules, and causal analysis.\n",
    "\n",
    "### Notebook Integration\n",
    "\n",
    "For deeper analysis in each phase, open the cohort-specific notebooks:\n",
    "\n",
    "- **Feature Importance (EC2 driver)**: `3_feature_importance/feature_importance_cohort_runner.ipynb`\n",
    "- **bupaR ‚Äì opioid_ed**: `5_bupaR_analysis/bupaR_pipeline_opioid_ed.ipynb`\n",
    "- **bupaR ‚Äì non_opioid_ed**: `5_bupaR_analysis/bupaR_pipeline_non_opioid_ed.ipynb`\n",
    "- **DTW ‚Äì opioid_ed**: `6_dtw_analysis/dtw_pipeline_opioid_ed.ipynb`\n",
    "- **DTW ‚Äì non_opioid_ed**: `6_dtw_analysis/dtw_pipeline_non_opioid_ed.ipynb`\n",
    "- **Final Model Ensemble**: `7_final_model/final_model.ipynb`\n",
    "- **Formal Feature Analysis (FFA)**: `8_ffa_analysis/catboost_feature_attribution_analysis.ipynb`\n",
    "\n",
    "The sections below retain the **detailed ingestion, QA, FP-Growth, CatBoost, and FFA code**. Use this notebook as:\n",
    "\n",
    "- A **driver** (top cells) to orchestrate key phases, and\n",
    "- A **reference** (lower cells) for full data pipeline details and one-off investigations.\n",
    "\n",
    "### Final Analysis Workflow (from `README_analysis_workflow.md`)\n",
    "\n",
    "For the **final analysis**, we organize the downstream methods into three phases:\n",
    "\n",
    "1. **Phase 1 ‚Äì Monte Carlo CV + Feature Importance** (`3_feature_importance/`)\n",
    "   - Three core models: **CatBoost, XGBoost (boosted trees), XGBoost RF mode**.\n",
    "   - Monte Carlo CV on temporally separated train/test (2016‚Äì2018 ‚Üí 2019) with permutation-based importance.\n",
    "   - Outputs: ranked feature lists and filtered `model_data` event tables for target (`opioid_ed`) and control (`non_opioid_ed`) cohorts.\n",
    "\n",
    "2. **Phase 2 ‚Äì Pattern & Process Mining + DTW** (`4_fpgrowth_analysis/`, `5_bupaR_analysis/`, `6_dtw_analysis/`)\n",
    "   - **FP-Growth**: frequent itemsets and target-focused rules on selected codes.\n",
    "   - **bupaR**: event logs, pre/post-target process maps, time-windowed and time-to-event features saved under `5_bupaR_analysis/outputs/.../features/`.\n",
    "   - **DTW**: cohort- and age-band‚Äìspecific trajectory features (distances to prototypes) under `6_dtw_analysis/outputs/.../features/`.\n",
    "\n",
    "3. **Phase 3 ‚Äì Final Model Development & Attribution** (`7_final_model/`, `8_ffa_analysis/`)\n",
    "   - Integrate feature-importance‚Äìfiltered `model_data`, FP-Growth, bupaR, and DTW features into a single patient-level table following `final_feature_schema.json`.\n",
    "   - Train the final three-model ensemble (CatBoost, XGBoost, XGBoost RF) with temporal validation.\n",
    "   - Run FFA/attribution analysis to export symbolic rules, SHAP-style summaries, and causal-effect diagnostics.\n",
    "\n",
    "See `README_analysis_workflow.md` for full details and the end-to-end Mermaid diagram of these phases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06be424d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "import sys\n",
    "\n",
    "\n",
    "def resolve_project_root() -> Path:\n",
    "    \"\"\"Resolve the pgx-analysis project root for both notebook and script modes.\"\"\"\n",
    "    # When run as a script, __file__ is defined\n",
    "    if \"__file__\" in globals():\n",
    "        return Path(__file__).resolve().parents[0]\n",
    "\n",
    "    # When run inside Jupyter, fall back to current working directory and parents\n",
    "    cwd = Path(os.getcwd()).resolve()\n",
    "    # If we're already in the repo root, keep it\n",
    "    if (cwd / \"pgx_cohort_pipeline.ipynb\").exists():\n",
    "        return cwd\n",
    "\n",
    "    for parent in cwd.parents:\n",
    "        if (parent / \"pgx_cohort_pipeline.ipynb\").exists():\n",
    "            return parent\n",
    "\n",
    "    return cwd\n",
    "\n",
    "\n",
    "PROJECT_ROOT = resolve_project_root()\n",
    "print(f\"[INFO] Project root: {PROJECT_ROOT}\")\n",
    "\n",
    "if str(PROJECT_ROOT) not in sys.path:\n",
    "    sys.path.append(str(PROJECT_ROOT))\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# Cohort configuration for downstream phases\n",
    "# ------------------------------------------------------------------\n",
    "COHORT_NAME = \"opioid_ed\"       # or \"non_opioid_ed\"\n",
    "AGE_BAND = \"25-44\"              # e.g., \"0-12\", \"13-24\", ..., \"85-94\"\n",
    "TRAIN_YEARS = [2016, 2017, 2018]\n",
    "TEST_YEAR = 2019\n",
    "\n",
    "print(\n",
    "    f\"[CONFIG] cohort={COHORT_NAME}, age_band={AGE_BAND}, \"\n",
    "    f\"train_years={TRAIN_YEARS}, test_year={TEST_YEAR}\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a008c18f-74ff-48d3-906a-1d7dd566504b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "# Delete all partitions to start fresh\n",
    "aws s3 rm s3://pgxdatalake/bronze/ --recursive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69ac5e0d-b1f0-4692-b27b-970cbf8c323c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "# Delete all partitions to start fresh\n",
    "aws s3 rm s3://pgxdatalake/silver/ --recursive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50d4b503-0ef4-4935-ba77-49708161dcd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "# Delete all partitions to start fresh\n",
    "aws s3 rm s3://pgxdatalake/gold/pharmacy/ --recursive\n",
    "aws s3 rm s3://pgxdatalake/gold/medical/ --recursive"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ee724f5-d99c-4c1b-83cc-192dbeada9cd",
   "metadata": {},
   "source": [
    "## A. Input Datasets - Initial Clean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7161964d-fbe9-4c70-a1f2-7386dee3e501",
   "metadata": {},
   "source": [
    "### 1. Text to Parquet Format\n",
    "\n",
    "- run this from command line on EC2:  \n",
    "sudo mkdir -p /mnt/nvme/duckdb_tmp  \n",
    "sudo chown -R \"$USER\":\"$USER\" /mnt/nvme/duckdb_tmp  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca9187a9-4af1-4ca2-ae7c-89c5688fffd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "set -euo pipefail\n",
    "\n",
    "echo \"üöÄ TXT ‚Üí Parquet (bronze) starting...\"\n",
    "echo \"Started at: $(date)\"\n",
    "echo \"\"\n",
    "\n",
    "mkdir -p /home/pgx3874/pgx-analysis/1_apcd_input_data/logs\n",
    "\n",
    "/home/pgx3874/jupyter-env/bin/python3.11 /home/pgx3874/pgx-analysis/1_apcd_input_data/0_txt_to_parquet.py \\\n",
    "  --dataset both \\\n",
    "  --workers 18 \\\n",
    "  --duckdb-threads 1 \\\n",
    "  --split-rejects \\\n",
    "  --bronze-root s3://pgxdatalake/bronze/ \\\n",
    "  --overwrite \\\n",
    "  --tmp-dir /mnt/nvme/duckdb_tmp 2>&1 | tee \"/home/pgx3874/pgx-analysis/1_apcd_input_data/logs/0_txt_to_parquet_$(date +%Y%m%d_%H%M%S).log\"\n",
    "\n",
    "echo \"\"\n",
    "echo \"‚úÖ TXT ‚Üí Parquet (bronze) completed at: $(date)\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f982ccc-1302-4446-b29f-487fa6a2dffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "set -euo pipefail\n",
    "\n",
    "echo \"üöÄ Reprocessing corrected rejects...\"\n",
    "echo \"Started at: $(date)\"\n",
    "echo \"\"\n",
    "\n",
    "mkdir -p /home/pgx3874/pgx-analysis/1_apcd_input_data/logs\n",
    "\n",
    "/home/pgx3874/jupyter-env/bin/python3.11 /home/pgx3874/pgx-analysis/1_apcd_input_data/1_reprocess_txt_to_parquet.py \\\n",
    "  --dataset both \\\n",
    "  --workers 18 \\\n",
    "  --duckdb-threads 1 \\\n",
    "  --bronze-root s3://pgxdatalake/bronze/ \\\n",
    "  --tmp-dir /mnt/nvme/duckdb_tmp 2>&1 | tee \"/home/pgx3874/pgx-analysis/1_apcd_input_data/logs/1_reprocess_txt_to_parquet_$(date +%Y%m%d_%H%M%S).log\"\n",
    "\n",
    "echo \"\"\n",
    "echo \"‚úÖ Reprocess completed at: $(date)\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12dfd2c3-0171-4d5d-b00f-b09d5e97e905",
   "metadata": {},
   "source": [
    "### 2. Clean Pharmacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2ba12da-ad54-43a9-835e-1f67f93e3d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import duckdb\n",
    "\n",
    "# Enable S3 and HTTPFS support\n",
    "duckdb.sql(\"INSTALL httpfs; LOAD httpfs;\")\n",
    "duckdb.sql(\"CALL load_aws_credentials();\")\n",
    "\n",
    "# Define your input path\n",
    "pharmacy_input_path = 's3://pgxdatalake/bronze/pharmacy/**/*.parquet'\n",
    "\n",
    "# Check the schema (grab 0 rows to inspect column names and types only)\n",
    "schema_df = duckdb.sql(f\"\"\"\n",
    "    DESCRIBE SELECT * FROM read_parquet('{pharmacy_input_path}') LIMIT 0\n",
    "\"\"\").df()\n",
    "\n",
    "print(schema_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e60384b4-32c5-4f48-8908-4bf631c577b7",
   "metadata": {},
   "source": [
    "#### a. Drug Lookup Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5617369d-3eb8-4647-b0f3-7b7fd70f922c",
   "metadata": {},
   "outputs": [],
   "source": [
    "! aws s3 sync s3://pgx-repository/drug_mappings/ /home/pgx3874/pgx-analysis/1_apcd_input_data/drug_mappings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfe5cf0f-6383-4c51-8a88-525613da2aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import duckdb\n",
    "\n",
    "mapping_dir = \"/home/pgx3874/pgx-analysis/1_apcd_input_data/drug_mappings\"\n",
    "\n",
    "# Access struct fields from MAP_ENTRIES\n",
    "mapping_files_query = f\"\"\"\n",
    "SELECT \n",
    "  LOWER(key.key) AS key,\n",
    "  LOWER(key.value) AS value\n",
    "FROM read_json_auto('{mapping_dir}/*_mappings.json'),\n",
    "UNNEST(MAP_ENTRIES(json)) AS kv(key)\n",
    "\"\"\"\n",
    "\n",
    "drug_map = duckdb.sql(mapping_files_query)\n",
    "drug_map.create(\"drug_map\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccd5982e-12c7-47f8-8429-ef3401ddd1b2",
   "metadata": {},
   "source": [
    "#### b. Impute Missing Data / Create Silver Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0430a329-8abb-4f41-9a29-fd69ca9b0e90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Phase 1: Starting Global Demographic Imputation...\n",
      "Input: Bronze tier pharmacy and medical data\n",
      "Output: Imputed partitioned data\n",
      "Started at: Wed Nov 26 14:56:51 UTC 2025\n",
      "\n",
      "2025-11-26 14:56:55,702 - INFO - üöÄ Starting Global Demographic Imputation\n",
      "2025-11-26 14:56:55,702 - INFO - üìä Pharmacy input: s3://pgxdatalake/bronze/pharmacy/*.parquet\n",
      "2025-11-26 14:56:55,702 - INFO - üìä Medical input: s3://pgxdatalake/bronze/medical/*.parquet\n",
      "2025-11-26 14:56:55,702 - INFO - üìä Output root: s3://pgxdatalake/silver/imputed\n",
      "2025-11-26 14:56:55,702 - INFO - üìä Lookahead years: 5\n",
      "2025-11-26 14:56:55,702 - INFO - üìä Create raw silver: True\n",
      "2025-11-26 14:56:55,702 - INFO - üìä DuckDB will auto-detect optimal memory and thread settings\n",
      "2025-11-26 14:56:55,702 - INFO - üöÄ Starting Global Demographic Imputation\n",
      "2025-11-26 14:56:55,702 - INFO - üîß Using Version 1997 + 12 - Global Imputation (DuckDB Lessons Learned Applied)\n",
      "2025-11-26 14:56:55,702 - INFO - ================================================================================\n",
      "2025-11-26 14:56:55,702 - INFO - üìÅ Silver imputed paths:\n",
      "2025-11-26 14:56:55,702 - INFO -    ‚Ä¢ Base path: s3://pgxdatalake/silver/imputed\n",
      "2025-11-26 14:56:55,702 - INFO -    ‚Ä¢ mi_person_key demographics lookup: s3://pgxdatalake/silver/imputed/mi_person_key_demographics_lookup.parquet\n",
      "2025-11-26 14:56:55,702 - INFO - ================================================================================\n",
      "2025-11-26 14:56:55,702 - INFO - STEP 0: Creating Raw Silver Datasets\n",
      "2025-11-26 14:56:55,702 - INFO - ================================================================================\n",
      "2025-11-26 14:56:55,702 - INFO - Note: Raw silver creation is independent of partitioned data existence\n",
      "2025-11-26 14:56:55,843 - INFO - ‚úÖ Simple DuckDB connection created - 1 thread per worker (for multiprocessing)\n",
      "2025-11-26 14:56:55,843 - INFO - ‚úÖ Simple DuckDB connection created - auto memory/threads\n",
      "2025-11-26 14:56:55,905 - INFO - ‚úì Saved text file to s3://pgx-repository/build_logs/apcd_input_data/global_imputation/global/all/log_20251126_145655_step0_a_raw_silver_started.txt\n",
      "2025-11-26 14:56:55,905 - INFO - ‚úì Checkpoint logs saved: s3://pgx-repository/build_logs/apcd_input_data/global_imputation/global/all/log_20251126_145655_step0_a_raw_silver_started.txt\n",
      "2025-11-26 14:56:55,905 - INFO - ================================================================================\n",
      "2025-11-26 14:56:55,905 - INFO - CREATING RAW SILVER DATASETS (All Original Columns)\n",
      "2025-11-26 14:56:55,905 - INFO - ================================================================================\n",
      "2025-11-26 14:56:55,905 - INFO - Pharmacy raw output: s3://pgxdatalake/silver/pharmacy_raw\n",
      "2025-11-26 14:56:55,905 - INFO - Medical raw output: s3://pgxdatalake/silver/medical_raw\n",
      "2025-11-26 14:56:57,270 - INFO - üìä Will create: s3://pgxdatalake/silver/pharmacy_raw\n",
      "2025-11-26 14:56:57,270 - INFO - üìä Will create: s3://pgxdatalake/silver/medical_raw\n",
      "2025-11-26 14:56:57,270 - INFO - Creating raw pharmacy dataset with all original columns...\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "set -euo pipefail\n",
    "\n",
    "mkdir -p /home/pgx3874/pgx-analysis/1_apcd_input_data/logs\n",
    "\n",
    "# Phase 1: Global Imputation (Optimized - No Demographics Lookup)\n",
    "echo \"üöÄ Phase 1: Starting Global Demographic Imputation...\"\n",
    "echo \"Input: Bronze tier pharmacy and medical data\"\n",
    "echo \"Output: Imputed partitioned data\"\n",
    "echo \"Started at: $(date)\"\n",
    "echo \"\"\n",
    "\n",
    "/home/pgx3874/jupyter-env/bin/python3.11 \\\n",
    "  /home/pgx3874/pgx-analysis/1_apcd_input_data/2_global_imputation.py \\\n",
    "  --pharmacy-input s3://pgxdatalake/bronze/pharmacy/*.parquet \\\n",
    "  --medical-input s3://pgxdatalake/bronze/medical/*.parquet \\\n",
    "  --output-root s3://pgxdatalake/silver/imputed \\\n",
    "  --create-raw-silver \\\n",
    "  --lookahead-years 5 \\\n",
    "  --no-demographics-lookup \\\n",
    "  --log-level INFO 2>&1 | tee \"/home/pgx3874/pgx-analysis/1_apcd_input_data/logs/2_global_imputation_$(date +%Y%m%d_%H%M%S).log\"\n",
    "\n",
    "echo \"\"\n",
    "echo \"‚úÖ Global Imputation completed successfully at: $(date)\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4394957b-da3b-435a-a1f3-69cf1894d61b",
   "metadata": {},
   "source": [
    "#### c. Normalize Drug Names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "651e0455-6351-45e4-9fec-02168f7b6e5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "set -euo pipefail\n",
    "\n",
    "# Phase 2: Optimized Partition Processing using Pre-Imputed Data\n",
    "echo \"üöÄ Phase 2: Running Optimized Partition Processing with Pre-Imputed Data...\"\n",
    "echo \"Input: Silver tier imputed partitioned data (no demographics lookup needed)\"\n",
    "echo \"üìÅ Output: Gold tier final partitions\"\n",
    "echo \" Started at: $(date)\"\n",
    "echo \"\"\n",
    "\n",
    "# Create logs directory\n",
    "mkdir -p /home/pgx3874/pgx-analysis/1_apcd_input_data/logs\n",
    "\n",
    "job=\"pharmacy\"\n",
    "PHARMACY_SCRIPT=\"/home/pgx3874/pgx-analysis/1_apcd_input_data/3a_clean_pharmacy.py\"\n",
    "\n",
    "# Use the imputed partitioned data directly (no demographics lookup needed)\n",
    "/home/pgx3874/jupyter-env/bin/python3.11 /home/pgx3874/pgx-analysis/1_apcd_input_data/3_apcd_clean.py \\\n",
    "  --job \"$job\" \\\n",
    "  --pharmacy-input s3://pgxdatalake/silver/imputed/pharmacy_partitioned/**/*.parquet \\\n",
    "  --output-root s3://pgxdatalake/gold/pharmacy \\\n",
    "  --min-year 2016 --max-year 2020 \\\n",
    "  --workers 48 \\\n",
    "  --retries 1 \\\n",
    "  --run-mode subprocess \\\n",
    "  --pharmacy-script \"$PHARMACY_SCRIPT\" \\\n",
    "  --log-level INFO 2>&1 | tee \"/home/pgx3874/pgx-analysis/1_apcd_input_data/logs/${job}_clean_output_$(date +%Y%m%d_%H%M%S).log\"\n",
    "\n",
    "echo \"\"\n",
    "echo \"‚úÖ Optimized partition processing completed at: $(date)\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bf189a7-3317-4a03-8e29-4441bbc0f27f",
   "metadata": {},
   "source": [
    "### 3. Drug Frequency Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b64af9e-fcc3-47f4-85a3-9dad8cd9fdd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "/home/pgx3874/jupyter-env/bin/python3.11 \\\n",
    "   /home/pgx3874/pgx-analysis/1_apcd_input_data/4_drug_frequency_analysis.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a8b0537-e8c0-46b5-b008-e8d4c5391fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Load data from script\n",
    "pickle_path = '/home/pgx3874/pgx-analysis/1_apcd_input_data/outputs/drug_analysis_data.pkl'\n",
    "with open(pickle_path, 'rb') as f:\n",
    "    drug_data = pickle.load(f)\n",
    "\n",
    "print(\"‚úÖ Data loaded successfully!\")\n",
    "\n",
    "drug_df = drug_data['df']\n",
    "drug_high_freq_df = drug_data['high_freq_df']\n",
    "drug_low_freq_df = drug_data['low_freq_df']\n",
    "drug_summary_df = drug_data['summary_df']\n",
    "drug_trends_df = drug_data['trends_df']\n",
    "\n",
    "\n",
    "print(f\"üìä Main data: {len(drug_df):,} records\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17529999-8fe6-4a7f-9c57-19003b61b9bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pickle\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set up plotting style\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 10\n",
    "\n",
    "# Reusable visuals\n",
    "from helpers_1997_13.visualization_utils import (\n",
    "    plot_stacked_by_year,\n",
    "    plot_top_bars,\n",
    "    save_current_chart,\n",
    "    create_plotly_frequency_dashboard\n",
    ")\n",
    "\n",
    "\n",
    "# High frequency drugs stacked by year using reusable function\n",
    "ordered_high = drug_high_freq_df['drug_name'].tolist()\n",
    "plot_stacked_by_year(\n",
    "    drug_df[drug_df['drug_name'].isin(ordered_high)],\n",
    "    target_col='drug_name',\n",
    "    year_col='event_year',\n",
    "    freq_col='frequency',\n",
    "    ordered_targets=ordered_high,\n",
    "    title_suffix='High Frequency (>=1000)'\n",
    ")\n",
    "\n",
    "save_current_chart('high_freq_by_year_drug', 'drug_frequency')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aa59765-6230-4bd1-985a-1021a67934a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Low frequency drugs stacked by year (limit to top 25 for readability)\n",
    "ordered_low = drug_low_freq_df.head(25)['drug_name'].tolist()\n",
    "plot_stacked_by_year(\n",
    "    drug_df[drug_df['drug_name'].isin(ordered_low)],\n",
    "    target_col='drug_name',\n",
    "    year_col='event_year',\n",
    "    freq_col='frequency',\n",
    "    ordered_targets=ordered_low,\n",
    "    title_suffix='Low Frequency (<1000) top25'\n",
    ")\n",
    "\n",
    "save_current_chart('low_freq_by_year_drug', 'drug_frequency')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "844d710e-2a42-47f8-81ef-20d23426e02b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top-N bars using reusable function\n",
    "plot_top_bars(drug_high_freq_df, target_col='drug_name', value_col='total_frequency', top_n=10, title='Top 10 High Frequency Drugs')\n",
    "save_current_chart('top10_high_drug', 'drug_frequency')\n",
    "\n",
    "plot_top_bars(drug_low_freq_df, target_col='drug_name', value_col='total_frequency', top_n=10, title='Top 10 Low Frequency Drugs')\n",
    "save_current_chart('top10_low_drug', 'drug_frequency')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c4d6fd-6bb4-45fe-ade9-0896f9495091",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drug Dashboard\n",
    "create_plotly_frequency_dashboard(\n",
    "    drug_df,\n",
    "    title='Drug Frequency Explorer',\n",
    "    s3_output_path='s3://pgxdatalake/visualizations/drug_name/drug_frequency_dashboard.html',\n",
    "    target_col='drug_name', year_col='event_year', freq_col='frequency', system_col=None, top_n=999999\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "716c951a-2532-480e-ab8c-27e44d16fb75",
   "metadata": {},
   "source": [
    "### 4. Clean Medical"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c93908d-78c0-4bb1-8505-9e6f3dd162dc",
   "metadata": {},
   "source": [
    "#### a. Clean Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ca1f026-e8a0-4c91-9e64-cf3b9c32a50d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "set -euo pipefail\n",
    "\n",
    "# Phase 2b: Optimized Medical Processing\n",
    "echo \"üöÄ Phase 2b: Starting Optimized Medical Processing...\"\n",
    "echo \"Input: Silver tier imputed partitioned medical data\"\n",
    "echo \"Output: Gold tier final medical partitions\"\n",
    "echo \"Started at: $(date)\"\n",
    "echo \"\"\n",
    "\n",
    "# Create logs directory\n",
    "mkdir -p logs\n",
    "\n",
    "/home/pgx3874/jupyter-env/bin/python3.11 \\\n",
    "    /home/pgx3874/pgx-analysis/1_apcd_input_data/3_apcd_clean.py \\\n",
    "    --job medical \\\n",
    "    --raw-medical s3://pgxdatalake/silver/medical/*.parquet \\\n",
    "    --output-root s3://pgxdatalake/gold/medical \\\n",
    "    --min-year 2016 --max-year 2020 \\\n",
    "    --workers 9 \\\n",
    "    --retries 1 \\\n",
    "    --run-mode subprocess \\\n",
    "    --medical-script /home/pgx3874/pgx-analysis/1_apcd_input_data/3b_clean_medical.py \\\n",
    "    --log-level INFO 2>&1 | tee \"logs/medical_clean_output_$(date +%Y%m%d_%H%M%S).log\"\n",
    "\n",
    "echo \"\"\n",
    "echo \"‚úÖ Phase 2b completed successfully at: $(date)\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9776dc36-6070-4166-940b-546128d204af",
   "metadata": {},
   "source": [
    "### 5. QA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e70de1b6-f406-405c-bfe0-03c357a9b3c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "# Run QA validation on both datasets\n",
    "/home/pgx3874/jupyter-env/bin/python3.11 \\\n",
    "  1_apcd_input_data/5_step1_data_quality_qa.py \\\n",
    "  --type both \\\n",
    "  --all-partitions \\\n",
    "  --workers 16 \\\n",
    "  --save-results \\\n",
    "  --verbose \\\n",
    "  2>&1 | tee 1_apcd_input_data/logs/qa_results_$(date +%Y%m%d_%H%M%S).log"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a419d538-49a8-4326-86db-4b887a5dae69",
   "metadata": {},
   "source": [
    "### 6. Glue Crawler for Athena"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e7d912-16f6-4fe2-8de5-119dda78b2ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import time\n",
    "from botocore.exceptions import ClientError\n",
    "\n",
    "# --- Configuration ---\n",
    "CRAWLER_NAME = \"medical\"\n",
    "REGION = \"us-east-1\"  # Change this if your crawler is in another region\n",
    "\n",
    "# --- Initialize the Glue client ---\n",
    "glue = boto3.client(\"glue\", region_name=REGION)\n",
    "\n",
    "# --- Start the crawler (safely) ---\n",
    "try:\n",
    "    crawler = glue.get_crawler(Name=CRAWLER_NAME)\n",
    "    state = crawler[\"Crawler\"][\"State\"]\n",
    "\n",
    "    if state == \"READY\":\n",
    "        print(f\"Starting crawler: {CRAWLER_NAME} ...\")\n",
    "        glue.start_crawler(Name=CRAWLER_NAME)\n",
    "    else:\n",
    "        print(f\"Crawler '{CRAWLER_NAME}' is currently {state}. Waiting...\")\n",
    "\n",
    "except ClientError as e:\n",
    "    if e.response[\"Error\"][\"Code\"] == \"CrawlerRunningException\":\n",
    "        print(f\"Crawler '{CRAWLER_NAME}' is already running.\")\n",
    "    else:\n",
    "        raise\n",
    "\n",
    "# --- Monitor progress until completion ---\n",
    "while True:\n",
    "    crawler = glue.get_crawler(Name=CRAWLER_NAME)\n",
    "    state = crawler[\"Crawler\"][\"State\"]\n",
    "    last_run = crawler[\"Crawler\"].get(\"LastCrawl\", {}).get(\"Status\")\n",
    "\n",
    "    print(f\"[{time.strftime('%X')}] Crawler state: {state} | Last run: {last_run}\")\n",
    "    if state == \"READY\":\n",
    "        print(f\"Crawler '{CRAWLER_NAME}' has completed.\")\n",
    "        break\n",
    "    time.sleep(30)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9977a3a4-fa0d-45cf-92a4-e80e197b6bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "CRAWLER_NAME2 = \"pharmacy\"\n",
    "\n",
    "try:\n",
    "    crawler = glue.get_crawler(Name=CRAWLER_NAME2)\n",
    "    state = crawler[\"Crawler\"][\"State\"]\n",
    "\n",
    "    if state == \"READY\":\n",
    "        print(f\"Starting crawler: {CRAWLER_NAME2} ...\")\n",
    "        glue.start_crawler(Name=CRAWLER_NAME2)\n",
    "    else:\n",
    "        print(f\"Crawler '{CRAWLER_NAME2}' is currently {state}. Waiting...\")\n",
    "\n",
    "except ClientError as e:\n",
    "    if e.response[\"Error\"][\"Code\"] == \"CrawlerRunningException\":\n",
    "        print(f\"Crawler '{CRAWLER_NAME2}' is already running.\")\n",
    "    else:\n",
    "        raise\n",
    "\n",
    "while True:\n",
    "    crawler = glue.get_crawler(Name=CRAWLER_NAME2)\n",
    "    state = crawler[\"Crawler\"][\"State\"]\n",
    "    last_run = crawler[\"Crawler\"].get(\"LastCrawl\", {}).get(\"Status\")\n",
    "\n",
    "    print(f\"[{time.strftime('%X')}] Crawler state: {state} | Last run: {last_run}\")\n",
    "    if state == \"READY\":\n",
    "        print(f\"Crawler '{CRAWLER_NAME2}' has completed.\")\n",
    "        break\n",
    "    time.sleep(30)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10afddfe-a68a-4462-a52c-d226db7355ae",
   "metadata": {},
   "source": [
    "### 7. Target Variable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c59911df-2661-482a-aa9f-3976b4772313",
   "metadata": {},
   "source": [
    "#### a. Target Variable Frequency Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b9cdcb-3675-4298-b0bc-bee06a4f5529",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "export PGX_WORKERS_MEDICAL=16\n",
    "export PGX_THREADS_PER_WORKER=1\n",
    "export PGX_S3_MAX_CONNECTIONS=64  \n",
    "\n",
    "/home/pgx3874/jupyter-env/bin/python3.11 \\\n",
    "  /home/pgx3874/pgx-analysis/1_apcd_input_data/6_target_frequency_analysis.py \\\n",
    "  --codes-of-interest \"F11.20\" \\\n",
    "  --workers ${PGX_WORKERS_MEDICAL} \\\n",
    "  --min-year 2016 --max-year 2020 \\\n",
    "  --log-cpu --log-s3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce7dd966-b5f2-4a25-a197-395ef0ba08c3",
   "metadata": {},
   "source": [
    "#### b. Update Target Codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1da8fdb1-620b-4fa8-bf2c-b086ed632d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "set -euo pipefail\n",
    "\n",
    "# ========================================\n",
    "# LOCAL STAGING: Maximum Performance\n",
    "# ========================================\n",
    "export PGX_USE_LOCAL_STAGING=1  # Enable local staging (default: on)\n",
    "export PGX_LOCAL_STAGING_DIR=\"/mnt/nvme/duckdb_tmp\"  # Use fast NVMe\n",
    "\n",
    "# ========================================\n",
    "# WORKER CONFIGURATION: 24 workers\n",
    "# ========================================\n",
    "export PGX_WORKERS_MEDICAL=28  # High parallelism (safe with local staging)\n",
    "export PGX_THREADS_PER_WORKER=1\n",
    "export PGX_TOTAL_WORKERS=28\n",
    "export PGX_DUCKDB_MEMORY_LIMIT=8GB\n",
    "export PGX_TARGET_FILE_SIZE_MB=1024\n",
    "export PGX_MAX_CHUNKS_PER_BATCH=8\n",
    "export PGX_NO_MERGE=0\n",
    "export PGX_PERSIST_MAPPINGS=1\n",
    "export PGX_SKIP_SAMPLE_CHECK=1\n",
    "export PGX_USE_TEMP_DB=1\n",
    "export PGX_MAX_UPLOAD_CONCURRENCY=10\n",
    "export PGX_S3_MAX_CONNECTIONS=256\n",
    "export PGX_MP_START_METHOD=fork\n",
    "\n",
    "\n",
    "# ========================================\n",
    "# CHUNKING: Balanced for performance\n",
    "# ========================================\n",
    "CHUNK_ROWS=1000000  # 1M rows per chunk (good balance)\n",
    "CHECKPOINT_SUFFIX=\".codes_updated.v2.ok\"\n",
    "STAGING_SUFFIX=\".codes_updated.staging/\"\n",
    "\n",
    "# ========================================\n",
    "# PATHS\n",
    "# ========================================\n",
    "ICD_MAP=\"/home/pgx3874/pgx-analysis/1_apcd_input_data/target_mapping/target_icd_mapping.json\"\n",
    "LOG_FILE=\"logs/medical_codes_$(date +%Y%m%d_%H%M%S).log\"\n",
    "\n",
    "# ========================================\n",
    "# SETUP\n",
    "# ========================================\n",
    "mkdir -p logs\n",
    "\n",
    "echo \"üöÄ Starting with LOCAL STAGING for maximum performance\"\n",
    "echo \"‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\"\n",
    "echo \"üìÇ Staging dir: $PGX_LOCAL_STAGING_DIR\"\n",
    "echo \"üë∑ Workers: $PGX_WORKERS_MEDICAL\"\n",
    "echo \"üß† Memory per worker: $PGX_DUCKDB_MEMORY_LIMIT\"\n",
    "echo \"üì¶ Chunk size: $CHUNK_ROWS rows\"\n",
    "echo \"üìã Log: $LOG_FILE\"\n",
    "echo \"‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\"\n",
    "echo \"Started at: $(date)\"\n",
    "echo \"\"\n",
    "\n",
    "# ========================================\n",
    "# RUN\n",
    "# ========================================\n",
    "nohup /home/pgx3874/jupyter-env/bin/python3.11 \\\n",
    "  /home/pgx3874/pgx-analysis/1_apcd_input_data/7_update_codes.py \\\n",
    "  --icd-target-map \"$ICD_MAP\" \\\n",
    "  --years \"2016,2017,2018,2019,2020\" \\\n",
    "  --workers-medical $PGX_WORKERS_MEDICAL \\\n",
    "  --threads $PGX_THREADS_PER_WORKER \\\n",
    "  --resume \\\n",
    "  --checkpoint-suffix \"$CHECKPOINT_SUFFIX\" \\\n",
    "  --chunked \\\n",
    "  --chunk-rows $CHUNK_ROWS \\\n",
    "  --staging-suffix \"$STAGING_SUFFIX\" \\\n",
    "  --duckdb-mem-limit $PGX_DUCKDB_MEMORY_LIMIT \\\n",
    "  --no-merge \\\n",
    "  > \"$LOG_FILE\" 2>&1 &\n",
    "\n",
    "echo $! > logs/medical_codes.pid\n",
    "echo \"‚úÖ Job started with PID: $(cat logs/medical_codes.pid)\"\n",
    "echo \"\"\n",
    "echo \"‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\"\n",
    "echo \"MONITOR PROGRESS:\"\n",
    "echo \"  !tail -f $LOG_FILE\"\n",
    "echo \"  !grep -c '‚úì Updated' $LOG_FILE\"\n",
    "echo \"  !du -sh /mnt/nvme/duckdb_tmp\"\n",
    "echo \"‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ed091f5-f8cd-4c2c-8793-0639265ae250",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import subprocess\n",
    "\n",
    "log_files = glob.glob(\"logs/medical_codes_*.log\")\n",
    "if log_files:\n",
    "    latest_log = max(log_files, key=os.path.getmtime)\n",
    "    \n",
    "    result = subprocess.run(\n",
    "        ['grep', '-c', '‚úì Updated', latest_log],\n",
    "        capture_output=True,\n",
    "        text=True\n",
    "    )\n",
    "    completed = result.stdout.strip() if result.returncode == 0 else \"0\"\n",
    "    \n",
    "    # Disk usage\n",
    "    staging_size = subprocess.check_output(\n",
    "        ['du', '-sh', '/mnt/nvme/duckdb_tmp'],\n",
    "        stderr=subprocess.DEVNULL\n",
    "    ).decode().strip().split()[0]\n",
    "    \n",
    "    duckdb_tmp_size = subprocess.check_output(\n",
    "        ['du', '-sh', '/mnt/nvme/duckdb_tmp'],\n",
    "        stderr=subprocess.DEVNULL\n",
    "    ).decode().strip().split()[0]\n",
    "    \n",
    "    print(f\"‚úì Completed: {completed} partitions\")\n",
    "    print(f\"üíæ Staging: {staging_size}\")\n",
    "    print(f\"üíæ DuckDB tmp: {duckdb_tmp_size}\")\n",
    "    print(f\"üìã Log: {os.path.basename(latest_log)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "967daaf3-9a99-4a85-a749-62673d4d441b",
   "metadata": {},
   "source": [
    "### QA/Validate Updates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25ba848c-2eff-49e2-9696-ff9504737eed",
   "metadata": {},
   "source": [
    "#### 1. Re-Run Frequency Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32747d82-721d-47cb-aff1-9b3073ab0046",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "export PGX_WORKERS_MEDICAL=16\n",
    "export PGX_THREADS_PER_WORKER=1\n",
    "export PGX_S3_MAX_CONNECTIONS=64  # optional\n",
    "                             \n",
    "/home/pgx3874/jupyter-env/bin/python3.11 \\\n",
    "  /home/pgx3874/pgx-analysis/1_apcd_input_data/6_target_frequency_analysis.py \\\n",
    "  --codes-of-interest \"F11.20\" \\\n",
    "  --workers ${PGX_WORKERS_MEDICAL} \\\n",
    "  --min-year 2016 --max-year 2020 \\\n",
    "  --log-cpu --log-s3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cdce1c0-7507-4b06-8a42-a5a610da5337",
   "metadata": {},
   "source": [
    "#### 2. Updated Pickle Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ea8fbcd-ddf5-47cf-b730-5272517071f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from helpers_1997_13.data_utils import (\n",
    "    load_target_artifacts,\n",
    "    find_variants,\n",
    "    totals_for_codes,\n",
    "    compare_totals,\n",
    ")\n",
    "\n",
    "# Where scripts write canonical artifacts\n",
    "outputs_dir = os.path.join('1_apcd_input_data', 'outputs')\n",
    "legacy_base = '1_apcd_input_data'\n",
    "\n",
    "# Load normalized artifacts. This will raise FileNotFoundError if the\n",
    "# canonical updated pickle is missing ‚Äî generate it with:\n",
    "#   python 1_apcd_input_data/6_target_frequency_analysis.py --codes-of-interest \"F11.20\"\n",
    "t_orig, t_updated = load_target_artifacts(outputs_dir=outputs_dir)\n",
    "\n",
    "print('Updated data shape:', getattr(t_updated, 'shape', None))\n",
    "print('Orig data shape:', getattr(t_orig, 'shape', None))\n",
    "\n",
    "# Summary comparison (orig vs updated totals)\n",
    "cmp = compare_totals(t_orig, t_updated)\n",
    "print('\\nTop 10 updated target_codes (by updated_total):')\n",
    "print(cmp.head(10).to_string(index=False))\n",
    "\n",
    "# Focused QA for a code of interest\n",
    "code_of_interest = 'F11.20'\n",
    "orig_variants = find_variants(t_orig, code_of_interest)\n",
    "upd_variants = find_variants(t_updated, code_of_interest)\n",
    "all_variants = sorted(set(orig_variants) | set(upd_variants))\n",
    "\n",
    "print(f\"\\nF11.20 variants detected - orig: {len(orig_variants)}, updated: {len(upd_variants)}, union: {len(all_variants)}\")\n",
    "print('Variants union:', all_variants)\n",
    "\n",
    "# Per-variant totals and CSV output\n",
    "o = totals_for_codes(t_orig, all_variants).rename(columns={'freq': 'orig_freq'})\n",
    "u = totals_for_codes(t_updated, all_variants).rename(columns={'freq': 'updated_freq'})\n",
    "summary = pd.merge(pd.DataFrame({'target_code': all_variants}), o, on='target_code', how='left').merge(u, on='target_code', how='left').fillna(0)\n",
    "summary['delta'] = summary['updated_freq'].astype(int) - summary['orig_freq'].astype(int)\n",
    "\n",
    "out_csv = os.path.join(outputs_dir, 'target_code_f1120_comparison.csv')\n",
    "os.makedirs(outputs_dir, exist_ok=True)\n",
    "summary.to_csv(out_csv, index=False)\n",
    "print('\\nSaved comparison CSV to', out_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30269641-bbb0-4563-87bd-c0beb268debe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "from helpers_1997_13.data_utils import normalize_to_all_targets\n",
    "\n",
    "p='1_apcd_input_data/outputs'\n",
    "with open(f'{p}/target_analysis_data.pkl','rb') as f:\n",
    "    orig_obj = pickle.load(f)\n",
    "with open(f'{p}/target_analysis_data_updated.pkl','rb') as f:\n",
    "    updated_obj = pickle.load(f)\n",
    "\n",
    "o_df = normalize_to_all_targets(orig_obj)\n",
    "u_df = normalize_to_all_targets(updated_obj)\n",
    "\n",
    "# Show counts for the two variants\n",
    "print(\"Orig counts:\")\n",
    "print(o_df[o_df['target_code'].str.contains('AF1120|F1120', na=False)]\n",
    "      .groupby('target_code')['frequency'].sum())\n",
    "\n",
    "print(\"\\nUpdated counts:\")\n",
    "print(u_df[u_df['target_code'].str.contains('AF1120|F1120', na=False)]\n",
    "      .groupby('target_code')['frequency'].sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56095951-7db6-429e-b8f6-e1b567334541",
   "metadata": {},
   "source": [
    "#### 3. F1120 Frequency Chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb9af069-a787-4c1a-8bc3-ab335c7e09b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target code (ICD) == F11.20 (include variants like YF1120, 0F1120, etc.)\n",
    "\n",
    "# u_df should have: event_year, target_code, frequency, target_system\n",
    "# If loaded from pickle produced by 6_target_frequency_analysis.py:\n",
    "#   u_df = target_data['all_targets']\n",
    "\n",
    "code_of_interest = 'F11.20'\n",
    "needle = code_of_interest.replace('.', '').upper()\n",
    "\n",
    "icd = u_df[u_df['target_system'] == 'icd'].copy()\n",
    "icd['code_flat'] = (\n",
    "    icd['target_code'].astype(str)\n",
    "       .str.upper()\n",
    "       .str.replace('.', '', regex=False)\n",
    "       .str.replace(' ', '', regex=False)\n",
    ")\n",
    "\n",
    "# All variants containing the same flattened substring (F1120)\n",
    "codes = (\n",
    "    icd[icd['code_flat'].str.contains(needle, na=False)]\n",
    "    .groupby('target_code', as_index=False)['frequency'].sum()\n",
    "    .sort_values('frequency', ascending=False)['target_code']\n",
    "    .tolist()\n",
    ")\n",
    "\n",
    "from helpers_1997_13.visualization_utils import plot_icd_variant_heatmap, save_current_chart\n",
    "\n",
    "fig, pivot_df = plot_icd_variant_heatmap(\n",
    "    u_df=target_data['all_targets'],\n",
    "    code_of_interest='F1120',   # already flattened/normalized\n",
    "    save_path='plots/icd_F11.20_heatmap.png',\n",
    "    export_csv_path='plots/icd_F11.20_pivot.csv'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d610eb-ffa5-49ec-80c4-bdb4ffcdd0e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import json\n",
    "import pandas as pd\n",
    "import duckdb\n",
    "\n",
    "# Paths (adjust if your environment differs)\n",
    "base = '/home/pgx3874/pgx-analysis/1_apcd_input_data'\n",
    "orig_pk = os.path.join(base, 'target_code_analysis_data.orig.pkl')\n",
    "updated_pk = os.path.join(base, 'target_code_analysis_data.updated.pkl')\n",
    "out_json = '/home/pgx3874/pgx-analysis/docs/target_pickles_diff.json'\n",
    "\n",
    "def load_pickle(path):\n",
    "  if not os.path.exists(path):\n",
    "    return None\n",
    "  try:\n",
    "    with open(path, 'rb') as f:\n",
    "      return pickle.load(f)\n",
    "  except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Failed to load pickle {path}: {e}\")\n",
    "    return None\n",
    "\n",
    "def normalize_to_all_targets(obj):\n",
    "  # Same normalizer used elsewhere: produce columns event_year, target_code, frequency, target_system\n",
    "  if obj is None:\n",
    "    return None\n",
    "  if isinstance(obj, pd.DataFrame):\n",
    "    df = obj.copy()\n",
    "    for c in ['event_year','target_code','frequency','target_system']:\n",
    "      if c not in df.columns:\n",
    "        df[c] = pd.NA\n",
    "    return df[['event_year','target_code','frequency','target_system']]\n",
    "  if isinstance(obj, dict):\n",
    "    if 'all_targets' in obj and obj['all_targets'] is not None:\n",
    "      return normalize_to_all_targets(obj['all_targets'])\n",
    "    parts = []\n",
    "    if obj.get('icd_aggregated') is not None:\n",
    "      parts.append(obj['icd_aggregated'].assign(target_system='icd'))\n",
    "    if obj.get('cpt_aggregated') is not None:\n",
    "      parts.append(obj['cpt_aggregated'].assign(target_system='cpt'))\n",
    "    if parts:\n",
    "      out = pd.concat(parts, ignore_index=True)\n",
    "      for c in ['event_year','target_code','frequency','target_system']:\n",
    "        if c not in out.columns:\n",
    "          out[c] = pd.NA\n",
    "      return out[['event_year','target_code','frequency','target_system']]\n",
    "    dfs = [v for v in obj.values() if isinstance(v, pd.DataFrame)]\n",
    "    if dfs:\n",
    "      out = pd.concat(dfs, ignore_index=True)\n",
    "      for c in ['event_year','target_code','frequency','target_system']:\n",
    "        if c not in out.columns:\n",
    "          out[c] = pd.NA\n",
    "      return out[['event_year','target_code','frequency','target_system']]\n",
    "  return None\n",
    "\n",
    "pd_orig = load_pickle(orig_pk)\n",
    "pd_updated = load_pickle(updated_pk)\n",
    "\n",
    "t_orig = normalize_to_all_targets(pd_orig) or pd.DataFrame(columns=['event_year','target_code','frequency','target_system'])\n",
    "t_updated = normalize_to_all_targets(pd_updated) or pd.DataFrame(columns=['event_year','target_code','frequency','target_system'])\n",
    "\n",
    "# Coerce numeric types\n",
    "for df in (t_orig, t_updated):\n",
    "  if 'frequency' in df.columns:\n",
    "    df['frequency'] = pd.to_numeric(df['frequency'], errors='coerce').fillna(0).astype(int)\n",
    "  if 'event_year' in df.columns:\n",
    "    df['event_year'] = pd.to_numeric(df['event_year'], errors='coerce').fillna(0).astype(int)\n",
    "\n",
    "def totals_by_code(df):\n",
    "  if df is None or df.empty:\n",
    "    return {}\n",
    "  grp = df.groupby('target_code', as_index=False)['frequency'].sum()\n",
    "  return {str(r['target_code']): int(r['frequency']) for _, r in grp.iterrows()}\n",
    "\n",
    "def per_year_by_code(df):\n",
    "  if df is None or df.empty:\n",
    "    return {}\n",
    "  out = {}\n",
    "  for code, g in df.groupby('target_code'):\n",
    "    series = g.groupby('event_year')['frequency'].sum()\n",
    "    out[str(code)] = {str(int(k)): int(v) for k, v in series.to_dict().items()}\n",
    "  return out\n",
    "\n",
    "orig_totals = totals_by_code(t_orig)\n",
    "upd_totals = totals_by_code(t_updated)\n",
    "orig_by_year = per_year_by_code(t_orig)\n",
    "upd_by_year = per_year_by_code(t_updated)\n",
    "\n",
    "all_codes = sorted(set(list(orig_totals.keys()) + list(upd_totals.keys())))\n",
    "\n",
    "records = []\n",
    "for code in all_codes:\n",
    "  o = orig_totals.get(code, 0)\n",
    "  u = upd_totals.get(code, 0)\n",
    "  rec = {\n",
    "    'target_code': code,\n",
    "    'orig_total': int(o),\n",
    "    'updated_total': int(u),\n",
    "    'delta': int(u) - int(o),\n",
    "    'orig_by_year': orig_by_year.get(code, {}),\n",
    "    'updated_by_year': upd_by_year.get(code, {}),\n",
    "  }\n",
    "  records.append(rec)\n",
    "\n",
    "# Focused F11.20 variants (same heuristic as Cell 35)\n",
    "code_of_interest = 'F11.20'\n",
    "needle = code_of_interest.replace('.', '').upper()\n",
    "def find_variants(df):\n",
    "  if df is None or df.empty:\n",
    "    return []\n",
    "  tmp = df.copy()\n",
    "  tmp['code_flat'] = tmp['target_code'].astype(str).str.upper().str.replace('.', '', regex=False).str.replace(' ', '', regex=False)\n",
    "  codes = tmp[tmp['code_flat'].str.contains(needle, na=False)].groupby('target_code', as_index=False)['frequency'].sum().sort_values('frequency', ascending=False)['target_code'].tolist()\n",
    "  return [str(c) for c in codes]\n",
    "\n",
    "f_orig = find_variants(t_orig)\n",
    "f_upd = find_variants(t_updated)\n",
    "f_union = sorted(set(f_orig) | set(f_upd))\n",
    "\n",
    "diff_obj = {\n",
    "  'metadata': {\n",
    "    'orig_shape': list(t_orig.shape),\n",
    "    'updated_shape': list(t_updated.shape),\n",
    "  },\n",
    "  'totals': records,\n",
    "  'f11_20_variants': {\n",
    "    'orig': f_orig,\n",
    "    'updated': f_upd,\n",
    "    'union': f_union\n",
    "  }\n",
    "}\n",
    "\n",
    "# Write JSON\n",
    "os.makedirs(os.path.dirname(out_json), exist_ok=True)\n",
    "with open(out_json, 'w', encoding='utf-8') as f:\n",
    "  json.dump(diff_obj, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print('Wrote JSON diff to', out_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d20e26aa-e556-4007-9c2b-9eb9b3c919fb",
   "metadata": {},
   "source": [
    "#### 4. Final Interactive Target Code Dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fcc81c8-b1a9-441f-ad60-5944ee9459d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers_1997_13.visualization_utils import create_plotly_frequency_dashboard\n",
    "\n",
    "# Target Code Dashboard (show all; user filters interactively)\n",
    "create_plotly_frequency_dashboard(\n",
    "    u_df,\n",
    "    title='Target Frequency Explorer (ICD/CPT)',\n",
    "    s3_output_path='s3://pgxdatalake/visualizations/target_code/target_frequency_dashboard.html',\n",
    "    target_col='target_code',\n",
    "    year_col='event_year',\n",
    "    freq_col='frequency',\n",
    "    system_col='target_system',\n",
    "    top_n=999999  # effectively all\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f19f110-5492-4118-89cb-69d1864ca93d",
   "metadata": {},
   "source": [
    "## B. Build Cohorts (From Medical and Pharmacy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6886001-cd6f-4261-a442-a1d797d36172",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Checkpoints\n",
    "aws s3 rm s3://pgx-repository/pgx-pipeline-status/create_cohort/ --recursive\n",
    "\n",
    "# Outputs\n",
    "aws s3 rm s3://pgxdatalake/gold/cohorts_F1120/ --recursive"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5aa29e1-bce2-4e17-b735-a282a014273f",
   "metadata": {},
   "source": [
    "### 1. Merge Datasets/Build Cohorts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d1b4c86-b84f-4120-83b0-c087148b4281",
   "metadata": {},
   "source": [
    "#### a. All Cohorts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5869941d-db97-479f-b01c-402ba61d00da",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import subprocess\n",
    "import concurrent.futures\n",
    "import boto3\n",
    "import traceback\n",
    "import os\n",
    "import threading\n",
    "import queue\n",
    "import time\n",
    "\n",
    "from helpers_1997_13 import constants\n",
    "from helpers_1997_13.cohort_utils import check_existing_cohorts as cu_check_existing_cohorts, run_cohort as cu_run_cohort\n",
    "import functools\n",
    "\n",
    "# Script configuration\n",
    "# Adjust this path to where 0_create_cohort.py lives on the target host\n",
    "script_path = \"/home/pgx3874/pgx-analysis/2_create_cohort/0_create_cohort.py\"\n",
    "python_bin = sys.executable\n",
    "\n",
    "\n",
    "# Use shared helpers from helpers_1997_13.cohort_utils to avoid duplication.\n",
    "def check_existing_cohorts():\n",
    "    return cu_check_existing_cohorts()\n",
    "\n",
    "\n",
    "# Wrapper that binds script_path and python_bin into the shared runner.\n",
    "def run_cohort(job):\n",
    "    target_icd = os.environ.get('PGX_TARGET_ICD_CODES', 'F1120')\n",
    "    cb = functools.partial(cu_run_cohort, script_path=script_path, python_bin=python_bin, target_icd=target_icd)\n",
    "    return cb(job)\n",
    "\n",
    "\n",
    "# ----- Batch processing orchestration -----\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    jobs_to_process = check_existing_cohorts()\n",
    "\n",
    "    if not jobs_to_process:\n",
    "        print(\"\\nAll cohorts already exist or are locked. No jobs to run.\")\n",
    "        sys.exit(0)\n",
    "\n",
    "    MAX_WORKERS = min(2, len(jobs_to_process))\n",
    "    print(f\"\\nStarting {len(jobs_to_process)} cohort processing jobs with {MAX_WORKERS} parallel workers...\", flush=True)\n",
    "    print(f\"{'='*80}\", flush=True)\n",
    "\n",
    "    BATCH_SIZE = 2\n",
    "    all_job_batches = [jobs_to_process[i:i+BATCH_SIZE] for i in range(0, len(jobs_to_process), BATCH_SIZE)]\n",
    "    all_results = []\n",
    "    all_job_statuses = {}\n",
    "\n",
    "    for batch_num, job_batch in enumerate(all_job_batches, 1):\n",
    "        print(f\"\\nProcessing batch {batch_num}/{len(all_job_batches)} with {len(job_batch)} jobs...\", flush=True)\n",
    "        with concurrent.futures.ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:\n",
    "            future_to_job = {executor.submit(run_cohort, job): job for job in job_batch}\n",
    "            total_jobs = len(future_to_job)\n",
    "            completed = 0\n",
    "            for future in concurrent.futures.as_completed(future_to_job):\n",
    "                job = future_to_job[future]\n",
    "                job_id = f\"{job['age_band']}/{job['event_year']}\"\n",
    "                completed += 1\n",
    "                try:\n",
    "                    result = future.result()\n",
    "                    all_results.append(result)\n",
    "                    all_job_statuses[job_id] = result\n",
    "                    print(f\"\\n[{completed}/{total_jobs}] Job status: {result}\", flush=True)\n",
    "                except Exception as e:\n",
    "                    error_msg = f\"Job execution error for {job_id}: {str(e)}\"\n",
    "                    print(f\"\\n[{completed}/{total_jobs}] {error_msg}\", flush=True)\n",
    "                    print(traceback.format_exc(), flush=True)\n",
    "                    all_results.append(f\"ERROR: {job_id} - {str(e)}\")\n",
    "                    all_job_statuses[job_id] = f\"ERROR: {str(e)}\"\n",
    "\n",
    "                print(f\"\\nProgress: {completed}/{total_jobs} jobs completed ({100*completed/total_jobs:.1f}%)\", flush=True)\n",
    "                print(f\"{'='*80}\", flush=True)\n",
    "\n",
    "        print(f\"\\nBatch {batch_num}/{len(all_job_batches)} complete.\", flush=True)\n",
    "        if batch_num < len(all_job_batches):\n",
    "            print(\"Pausing briefly before starting next batch...\", flush=True)\n",
    "            time.sleep(5)\n",
    "\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"FINAL SUMMARY OF RESULTS:\")\n",
    "    print(\"=\"*80)\n",
    "    success_count = sum(1 for r in all_results if r.startswith(\"SUCCESS\"))\n",
    "    locked_count = sum(1 for r in all_results if r.startswith(\"SKIPPED_LOCKED\"))\n",
    "    failed_count = sum(1 for r in all_results if r.startswith(\"FAILED\") or r.startswith(\"ERROR\"))\n",
    "\n",
    "    print(f\"‚úì Successful: {success_count}\")\n",
    "    print(f\"‚ö† Skipped (locked): {locked_count}\")\n",
    "    print(f\"‚úó Failed: {failed_count}\")\n",
    "\n",
    "    if all_job_statuses:\n",
    "        print(\"\\nDetailed status by job:\")\n",
    "        for job_id, status in sorted(all_job_statuses.items()):\n",
    "            if status.startswith(\"SUCCESS\"):\n",
    "                status_icon = \"‚úì\"\n",
    "            elif status.startswith(\"SKIPPED\"):\n",
    "                status_icon = \"‚ö†\"\n",
    "            else:\n",
    "                status_icon = \"‚úó\"\n",
    "            print(f\"{status_icon} {job_id}: {status}\")\n",
    "\n",
    "    print(\"=\"*80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "930f31da-368f-45a8-8b72-2f1d43f3e023",
   "metadata": {},
   "source": [
    "### 2. Glue Crawler for Athena"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9074edaa-40e6-4a91-80d0-cf698939bef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "# Initialize a Glue client (default region or specify if needed)\n",
    "glue = boto3.client('glue', region_name='us-east-1')  # change region if necessary\n",
    "\n",
    "# Name of your crawler\n",
    "crawler_name = \"medical\"\n",
    "\n",
    "# Start the crawler\n",
    "response = glue.start_crawler(Name=crawler_name)\n",
    "\n",
    "# Print response for confirmation\n",
    "print(f\"Crawler '{crawler_name}' started successfully.\")\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23e6ca49-ffac-48df-a7ba-24ee179c2e81",
   "metadata": {},
   "source": [
    "### 3. QA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "701bb8f9-2526-40dd-9525-718adecf6bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "set -euo pipefail\n",
    "\n",
    "# Set target configuration environment variables (matching 0_create_cohort.py)  \n",
    "export PGX_TARGET_ICD_CODES=\"${PGX_TARGET_ICD_CODES:-F1120}\"\n",
    "export PGX_TARGET_NAME=\"${PGX_TARGET_NAME:-}\"\n",
    "export PGX_TARGET_CPT_CODES=\"${PGX_TARGET_CPT_CODES:-}\"\n",
    "export PGX_TARGET_ICD_PREFIXES=\"${PGX_TARGET_ICD_PREFIXES:-}\"\n",
    "export PGX_TARGET_CPT_PREFIXES=\"${PGX_TARGET_CPT_PREFIXES:-}\"\n",
    "\n",
    "/home/pgx3874/jupyter-env/bin/python3.11 \\\n",
    "  /home/pgx3874/pgx-analysis/2_create_cohort/2_step2_data_quality_qa.py \\\n",
    "  --all-age-bands \\\n",
    "  --all-event-years \\\n",
    "  --cohorts both \\\n",
    "  --save-results \\\n",
    "  --max-workers 16 \\\n",
    "  --log-level INFO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ea5c528-a6e4-460e-ad93-a8368c477859",
   "metadata": {},
   "source": [
    "### 4. Athena Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff7bfad8-5e86-4141-a5d5-52db3525277d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import awswrangler as wr\n",
    "\n",
    "session = boto3.Session(region_name=\"us-east-1\")\n",
    "\n",
    "# Set it for awswrangler\n",
    "wr.config.athena_workgroup = \"primary\"\n",
    "wr.config.athena_output_location = \"s3://aws-athena-query-results-us-east-1-535362115856/cohort-qa/\"\n",
    "wr.config.session = session\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc30b19-c65f-4c17-a3c6-1f21dd524829",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext sql\n",
    "%config SqlMagic.autocommit=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8448747",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT *\n",
    "FROM cohorts.cohorts_F1120\n",
    "WHERE cohort_name = 'non_opioid_ed'\n",
    "  AND age_band = '65-74'\n",
    "  AND event_year = '2016'\n",
    "LIMIT 5\n",
    "\"\"\"\n",
    "wr.athena.read_sql_query(sql=query, database=\"cohorts\", boto3_session=session)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2565b06-55a2-47d7-b8fa-896d98569d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT * \n",
    "FROM cohorts.cohorts_clean\n",
    "WHERE cohort_name = 'non_opioid_ed' \n",
    "  AND age_band = '65-74' \n",
    "  AND event_year = '2016'\n",
    "LIMIT 5\n",
    "\"\"\"\n",
    "wr.athena.read_sql_query(sql=query, database=\"cohorts\", boto3_session=session)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a4639a1-3fd3-470d-9092-f391d4754e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_1 = \"\"\"\n",
    "SELECT \n",
    "  cohort_name,\n",
    "  COUNT(*) AS total_rows,\n",
    "  COUNT(DISTINCT mi_person_key) AS distinct_patients\n",
    "FROM cohorts.cohorts_clean\n",
    "GROUP BY cohort_name\n",
    "\"\"\"\n",
    "\n",
    "df_1 = wr.athena.read_sql_query(sql=query_1, database=\"cohorts\", boto3_session=session)\n",
    "df_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21a785f5-e690-41ee-9c1a-f52a48b72a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_2 = \"\"\"\n",
    "SELECT first_event, event, COUNT(*) AS count\n",
    "FROM cohorts.cohorts_clean\n",
    "WHERE cohort_name = 'non_opioid_ed' \n",
    "  AND age_band = '65-74' \n",
    "  AND event_year = '2016'\n",
    "GROUP BY first_event, event\n",
    "ORDER BY count DESC\n",
    "\"\"\"\n",
    "\n",
    "df_2 = wr.athena.read_sql_query(sql=query_2, database=\"cohorts\", boto3_session=session)\n",
    "df_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc521753-4cc7-4be9-aced-2ed255d0bbf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_3 = \"\"\"\n",
    "SELECT COUNT(*) AS patients_with_many_rows\n",
    "FROM (\n",
    "  SELECT mi_person_key\n",
    "  FROM cohorts.cohorts_clean\n",
    "  WHERE cohort_name = 'non_opioid_ed' \n",
    "    AND age_band = '65-74' \n",
    "    AND event_year = '2016'\n",
    "  GROUP BY mi_person_key\n",
    "  HAVING COUNT(*) > 100\n",
    ")\n",
    "\"\"\"\n",
    "\n",
    "df_3 = wr.athena.read_sql_query(query_3, database=\"cohorts\", boto3_session=session)\n",
    "df_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ac8ae7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_1 = \"\"\"\n",
    "SELECT\n",
    "  cohort_name,\n",
    "  COUNT(*) AS total_rows,\n",
    "  COUNT(DISTINCT mi_person_key) AS distinct_patients\n",
    "FROM cohorts.cohorts_clean\n",
    "GROUP BY cohort_name\n",
    "\"\"\"\n",
    "\n",
    "df_1 = wr.athena.read_sql_query(sql=query_1, database=\"cohorts\", boto3_session=session)\n",
    "df_1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aceb5e7-5b6c-45b6-a21a-2368086d2b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_4 = \"\"\"\n",
    "SELECT COUNT(*) AS patients_with_many_rows\n",
    "FROM (\n",
    "  SELECT mi_person_key\n",
    "  FROM cohorts.cohorts_clean\n",
    "  WHERE cohort_name = 'opioid_ed' \n",
    "    AND age_band = '65-74' \n",
    "    AND event_year = '2016'\n",
    "  GROUP BY mi_person_key\n",
    "  HAVING COUNT(*) > 100\n",
    ")\n",
    "\"\"\"\n",
    "\n",
    "df_4 = wr.athena.read_sql_query(query_4, database=\"cohorts\", boto3_session=session)\n",
    "df_4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f66387c",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_2 = \"\"\"\n",
    "SELECT first_event, event, COUNT(*) AS count\n",
    "FROM cohorts.cohorts_clean\n",
    "WHERE cohort_name = 'non_opioid_ed' \n",
    "  AND age_band = '65-74' \n",
    "  AND event_year = '2016'\n",
    "GROUP BY first_event, event\n",
    "ORDER BY count DESC\n",
    "\"\"\"\n",
    "\n",
    "df_2 = wr.athena.read_sql_query(sql=query_2, database=\"cohorts\", boto3_session=session)\n",
    "df_2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcf59e81-55d7-40ce-bacf-8273c3ba80ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_4 = \"\"\"\n",
    "SELECT *\n",
    "FROM cohorts.cohorts_clean\n",
    "WHERE cohort_name = 'non_opioid_ed' \n",
    "  AND age_band = '65-74' \n",
    "  AND event_year IN ('2016', '2017', '2018')\n",
    "\"\"\"\n",
    "\n",
    "train_df = wr.athena.read_sql_query(query_4, database=\"cohorts\", boto3_session=session)\n",
    "train_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d83d2ea-1af4-4e35-b128-25a43f5a27d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_5 = \"\"\"\n",
    "SELECT *\n",
    "FROM cohorts.cohorts_clean\n",
    "WHERE cohort_name = 'non_opioid_ed' \n",
    "  AND age_band = '65-74' \n",
    "  AND event_year = '2019'\n",
    "\"\"\"\n",
    "\n",
    "test_df = wr.athena.read_sql_query(query_5, database=\"cohorts\", boto3_session=session)\n",
    "test_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b6163d2-ea64-4073-87ee-17efc2dadb32",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_5 = \"\"\"\n",
    "SELECT *\n",
    "FROM cohorts.cohorts_clean\n",
    "WHERE cohort_name = 'opioid_ed' \n",
    "  AND age_band = '65-74' \n",
    "  AND event_year = '2019'\n",
    "\"\"\"\n",
    "\n",
    "test_df = wr.athena.read_sql_query(query_5, database=\"cohorts\", boto3_session=session)\n",
    "test_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b459b161",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_4 = \"\"\"\n",
    "SELECT *\n",
    "FROM cohorts.cohorts_clean\n",
    "WHERE cohort_name = 'non_opioid_ed' \n",
    "  AND age_band = '65-74' \n",
    "  AND event_year IN ('2016', '2017', '2018')\n",
    "\"\"\"\n",
    "\n",
    "train_df = wr.athena.read_sql_query(query_4, database=\"cohorts\", boto3_session=session)\n",
    "train_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ca72ec4-60d8-4344-86fb-48e9024f787a",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT first_event, COUNT(*) AS patient_count\n",
    "FROM cohorts.cohorts_clean\n",
    "GROUP BY first_event\n",
    "ORDER BY patient_count DESC\n",
    "\"\"\"\n",
    "\n",
    "df = wr.athena.read_sql_query(\n",
    "    sql=query,\n",
    "    database=\"cohorts\",\n",
    "    boto3_session=session\n",
    ")\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81598f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_5 = \"\"\"\n",
    "SELECT *\n",
    "FROM cohorts.cohorts_clean\n",
    "WHERE cohort_name = 'non_opioid_ed'\n",
    "  AND age_band = '65-74'\n",
    "  AND event_year = '2019'\n",
    "\"\"\"\n",
    "\n",
    "test_df = wr.athena.read_sql_query(query_5, database=\"cohorts\", boto3_session=session)\n",
    "test_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38e61ec2-2c17-400e-8dd8-2f87a72b5b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT event, COUNT(*) AS patient_count\n",
    "FROM cohorts.cohorts_clean\n",
    "GROUP BY event\n",
    "ORDER BY patient_count DESC\n",
    "\"\"\"\n",
    "\n",
    "df = wr.athena.read_sql_query(\n",
    "    sql=query,\n",
    "    database=\"cohorts\",\n",
    "    boto3_session=session\n",
    ")\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4196b82f",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_6 = \"\"\"\n",
    "SELECT *\n",
    "FROM cohorts.cohorts_clean\n",
    "WHERE cohort_name = 'opioid_ed' \n",
    "  AND age_band = '65-74' \n",
    "  AND event_year = '2019'\n",
    "\"\"\"\n",
    "\n",
    "test_df = wr.athena.read_sql_query(query_6, database=\"cohorts\", boto3_session=session)\n",
    "test_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c1f5a18-5068-4e5a-bb2b-6560109f8d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT age_band, event_year, COUNT(*) AS null_or_empty_drug_rows\n",
    "FROM pharmacy.pharmacy_clean\n",
    "WHERE drug_name = '' OR drug_name IS NULL\n",
    "GROUP BY age_band, event_year\n",
    "ORDER BY null_or_empty_drug_rows DESC\n",
    "\"\"\"\n",
    "\n",
    "control_df_missing = wr.athena.read_sql_query(query, database=\"pharmacy\", boto3_session=session)\n",
    "control_df_missing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82c355fc-ad39-4dbd-8aaa-9e0e17b6f252",
   "metadata": {},
   "source": [
    "## C. Feature Engineer (Network Analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c1749c3",
   "metadata": {},
   "source": [
    "### S3 directory structure and pipeline flow\n",
    "\n",
    "- Global FP-Growth (ProcessPool 30)\n",
    "  - Input: `s3://pgxdatalake/silver/{pharmacy|medical}/age_band={band}/event_year={year}/*.parquet`\n",
    "  - Output (GOLD): `s3://pgxdatalake/gold/fpgrowth/global/`\n",
    "    - `itemsets_{age}_{period_or_year}.json`\n",
    "    - `rules_{age}_{period_or_year}.json`\n",
    "    - `drug_metrics_{age}_{period_or_year}.json`\n",
    "    - `drug_encoding_map_{age}_{period_or_year}.json`\n",
    "\n",
    "- Cohort FP-Growth (ProcessPool 30)\n",
    "  - Input (curated cohort start): `s3://pgxdatalake/gold/cohorts_clean/cohort_name={cohort}/age_band={band}/event_year={year}/cohort.parquet`\n",
    "  - Output (GOLD): `s3://pgxdatalake/gold/fpgrowth/cohort/cohort_name={cohort}/age_band={band}/event_year={year}/`\n",
    "    - `fpgrowth_features.parquet`\n",
    "    - `itemsets.parquet` and `itemsets.json`\n",
    "    - `rules.parquet` and `rules.json`\n",
    "    - `feature_manifest.json`\n",
    "    - `*_drug_network.html`\n",
    "\n",
    "- Sources summary\n",
    "  - Reads: SILVER medical/pharmacy for global; GOLD cohorts_clean for cohort\n",
    "  - Writes: GOLD fpgrowth/{global|cohort} and GOLD cohorts_clean for curated cohort parquet\n",
    "\n",
    "```\n",
    "S3 Results:\n",
    "‚îú‚îÄ‚îÄ gold/fpgrowth/global/\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ itemsets_*.json\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ rules_*.json\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ drug_metrics_*.json\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ drug_encoding_map_*.json\n",
    "‚îú‚îÄ‚îÄ gold/fpgrowth/cohort/\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ cohort_name={cohort}/age_band={age}/event_year={year}/\n",
    "‚îÇ       ‚îú‚îÄ‚îÄ fpgrowth_features.parquet\n",
    "‚îÇ       ‚îú‚îÄ‚îÄ itemsets.parquet\n",
    "‚îÇ       ‚îú‚îÄ‚îÄ itemsets.json\n",
    "‚îÇ       ‚îú‚îÄ‚îÄ rules.parquet\n",
    "‚îÇ       ‚îú‚îÄ‚îÄ rules.json\n",
    "‚îÇ       ‚îú‚îÄ‚îÄ {cohort}_{age}_{year}_drug_network.html\n",
    "‚îÇ       ‚îî‚îÄ‚îÄ feature_manifest.json\n",
    "‚îî‚îÄ‚îÄ gold/cohorts_clean/\n",
    "    ‚îî‚îÄ‚îÄ cohort_name={cohort}/age_band={age}/event_year={year}/cohort.parquet\n",
    "```\n",
    "\n",
    "üìã S3 Path Mapping Summary\n",
    "\n",
    "| Component | Path |\n",
    "|---|---|\n",
    "| Global inputs | `s3://pgxdatalake/silver/{pharmacy|medical}/age_band={band}/event_year={year}/*.parquet` |\n",
    "| Global outputs (GOLD) | `s3://pgxdatalake/gold/fpgrowth/global/{itemsets|rules|drug_metrics|drug_encoding_map}_*.json` |\n",
    "| Cohort input (curated) | `s3://pgxdatalake/gold/cohorts_clean/cohort_name={cohort}/age_band={band}/event_year={year}/cohort.parquet` |\n",
    "| Cohort outputs (GOLD) | `s3://pgxdatalake/gold/fpgrowth/cohort/cohort_name={cohort}/age_band={band}/event_year={year}/...` |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bb64e26",
   "metadata": {},
   "source": [
    "### FP-Growth Analysis Module Summary\n",
    "\n",
    "#### üìã Overview\n",
    "The FP-Growth Analysis Module implements comprehensive drug pattern mining using the Frequent Pattern Growth algorithm. This module provides **two complementary approaches** for drug association analysis:\n",
    "\n",
    "#### üéØ Dual Approach Strategy\n",
    "\n",
    "##### 1. **Global FP-Growth** ‚Üí CatBoost Feature Engineering\n",
    "- **Purpose**: Creates universal drug encoding features that work across all cohorts\n",
    "- **Why Essential**: CatBoost models require consistent feature spaces - same drug encodings across training/validation/test sets\n",
    "- **Output**: Global drug encoding map (`s3://pgxdatalake/gold/fpgrowth/global/drug_encoding_map.json`)\n",
    "- **Benefit**: Population-level drug pattern insights become numerical features for ML\n",
    "\n",
    "##### 2. **By-Cohort FP-Growth** ‚Üí BupaR Process Mining  \n",
    "- **Purpose**: Discovers cohort-specific drug patterns and treatment sequences\n",
    "- **Why Essential**: Different cohorts (ED vs non-ED, age groups) have fundamentally different care pathways\n",
    "- **Output**: Cohort-specific association rules and network visualizations\n",
    "- **Benefit**: Reveals how prescribing patterns differ between populations\n",
    "\n",
    "#### üîß Technical Implementation\n",
    "\n",
    "##### Core Components:\n",
    "- **Global Pipeline**: `run_fpgrowth_global()` - Creates universal encoding features\n",
    "- **Cohort Pipeline**: `process_features()` - Discovers cohort-specific patterns\n",
    "- **Validation**: Cross-pipeline validation and integration testing\n",
    "- **Storage**: S3-based results partitioned by cohort/age/year\n",
    "\n",
    "##### Key Parameters:\n",
    "- **Global Support**: 0.005 (lower threshold for population coverage)\n",
    "- **Cohort Support**: 0.05 (higher threshold for pattern significance)\n",
    "- **Min Confidence**: 0.01 for association rules\n",
    "- **Top-K**: 25 most frequent itemsets per cohort\n",
    "\n",
    "##### üìä Output Structure\n",
    "\n",
    "```\n",
    "S3 Results:\n",
    "‚îú‚îÄ‚îÄ global_fpgrowth/\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ drug_encoding_map.json     # Universal ML features\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ global_itemsets.json       # Population patterns\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ global_rules.json          # Association rules\n",
    "‚îî‚îÄ‚îÄ fpgrowth_features/             # Cohort-specific results\n",
    "    ‚îú‚îÄ‚îÄ cohort_name=ed_non_opioid/\n",
    "    ‚îÇ   ‚îú‚îÄ‚îÄ age_band=25-34/\n",
    "    ‚îÇ   ‚îî‚îÄ‚îÄ age_band=35-44/\n",
    "    ‚îî‚îÄ‚îÄ cohort_name=opioid_ed/\n",
    "```\n",
    "\n",
    "#### üîó Integration Points\n",
    "\n",
    "##### With CatBoost Models:\n",
    "- Global encoding map provides consistent categorical features\n",
    "- Drug patterns become numerical features for machine learning\n",
    "- Population-level insights enhance model generalization\n",
    "\n",
    "##### With BupaR Process Mining:\n",
    "- Cohort-specific patterns reveal treatment pathways\n",
    "- Association rules guide process flow analysis\n",
    "- Network visualizations show care transitions\n",
    "\n",
    "#### üìà Pipeline Workflow\n",
    "1. **Extract** ‚Üí Get unique drug names from pharmacy dataset\n",
    "2. **Transform** ‚Üí Create patient-level drug transactions  \n",
    "3. **Mine** ‚Üí Apply FP-Growth with appropriate support thresholds\n",
    "4. **Encode** ‚Üí Generate consistent feature encodings\n",
    "5. **Store** ‚Üí Save results partitioned for downstream consumption\n",
    "6. **Validate** ‚Üí Ensure integration readiness for ML and process mining\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3213c13",
   "metadata": {},
   "source": [
    "#### üìã S3 Path Mapping Summary\n",
    "\n",
    "The following table shows how FP-Growth outputs map to downstream analysis inputs:\n",
    "\n",
    "| **Component** | **FP-Growth Output Path** | **Downstream Input** | **Purpose** |\n",
    "|---------------|---------------------------|----------------------|-------------|\n",
    "| **Global Encoding** | `s3://pgxdatalake/global_fpgrowth/drug_encoding_map.json` | CatBoost Feature Engineering | Universal drug encodings for consistent ML features |\n",
    "| **Global Itemsets** | `s3://pgxdatalake/global_fpgrowth/global_itemsets.json` | CatBoost Model Training | Population-level drug patterns |\n",
    "| **Global Rules** | `s3://pgxdatalake/global_fpgrowth/global_rules.json` | CatBoost Model Training | Association rules for feature creation |\n",
    "| **Enhanced Features** | `s3://pgxdatalake/fpgrowth_features_parquet/cohort_name={}/age_band={}/event_year={}/fpgrowth_features.parquet` | CatBoost Training Data | Dataset with `pattern_*` columns and FP-Growth metrics |\n",
    "| **Cohort Itemsets** | `s3://pgxdatalake/itemsets_parquet/cohort_name={}/age_band={}/event_year={}/itemsets.parquet` | BupaR Process Mining | Cohort-specific frequent drug combinations |\n",
    "| **Cohort Rules** | `s3://pgxdatalake/rules_parquet/cohort_name={}/age_band={}/event_year={}/rules.parquet` | BupaR Process Mining | Cohort-specific association rules for pathway analysis |\n",
    "| **Network Visualizations** | `s3://pgxdatalake/html_network_visual/cohort_name={}/age_band={}/event_year={}/network_visualization.html` | BupaR Analysis | Drug interaction network graphs |\n",
    "\n",
    "#### üîó Integration Flow\n",
    "\n",
    "```mermaid\n",
    "graph LR\n",
    "    A[FP-Growth Global Pipeline] --> B[global_fpgrowth/*]\n",
    "    C[FP-Growth Cohort Pipeline] --> D[fpgrowth_features_parquet/*]\n",
    "    C --> E[itemsets_parquet/*]\n",
    "    C --> F[rules_parquet/*]\n",
    "    \n",
    "    B --> G[CatBoost Global Features]\n",
    "    D --> H[CatBoost Enhanced Dataset]\n",
    "    E --> I[BupaR Itemsets Analysis]\n",
    "    F --> J[BupaR Rules Analysis]\n",
    "    \n",
    "    G --> K[Universal Drug Encodings]\n",
    "    H --> L[Pattern_* Columns]\n",
    "    I --> M[Drug Combination Discovery]\n",
    "    J --> N[Treatment Pathway Analysis]\n",
    "```\n",
    "\n",
    "#### ‚ö†Ô∏è Critical Dependencies\n",
    "\n",
    "1. **CatBoost Requirements**:\n",
    "   - Global encoding map must be available before model training\n",
    "   - Enhanced dataset must contain `pattern_*`, `*_enc_*`, `*_support`, `*_confidence`, `*_lift` columns\n",
    "   - Pattern columns should be categorical (string) features\n",
    "   - Metric columns should be numerical (float) features\n",
    "\n",
    "2. **BupaR Requirements**:\n",
    "   - Itemsets data needed for frequent drug combination analysis\n",
    "   - Rules data needed for treatment pathway discovery\n",
    "   - Both must be available per cohort/age_band/event_year partition\n",
    "\n",
    "3. **Data Consistency**:\n",
    "   - Same `cohort_name`, `age_band`, `event_year` combinations across all output types\n",
    "   - Pattern encoding consistency between global and cohort-specific results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35e6c620-1902-4e71-8a08-6c7bc5002760",
   "metadata": {},
   "source": [
    "### FPGrowth Global Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05509286-c8e0-4232-96c0-7abeb87f63f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!/home/pgx3874/jupyter-env/bin/python3.11 /home/pgx3874/pgx-analysis/fpgrowth_analysis/run_fpgrowth_group_pipeline.py 2>&1 | tee global_group_fpgrowth_output.log"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b575f923-0173-4ac6-89d6-99dfa1d22a65",
   "metadata": {},
   "source": [
    "#### Global Encoded Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a9f53ec-d7fb-4ee0-b8c5-3c0555da4f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "!/home/pgx3874/jupyter-env/bin/python3.11 /home/pgx3874/pgx-analysis/fpgrowth_analysis/process_itemsets.py 2>&1 | tee encode_drug_name_output.log"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da7fe97d-ab62-4dcc-a6c1-adc4287a25fa",
   "metadata": {},
   "source": [
    "#### üìä Drug Encoding Data Structure\n",
    "\n",
    "| Column | Description |\n",
    "|---|---|\n",
    "| `drug_name` | Original drug name |\n",
    "| `global_encoded_name` | 29-character encoded drug name |\n",
    "| `first_letter_index` | First letter index (A=111, B=222, etc.) |\n",
    "| `word_length` | Word length |\n",
    "| `num_vowels` | Number of vowels |\n",
    "| `num_consonants` | Number of consonants |\n",
    "| `num_hyphens_underscores` | Compound drug indicators |\n",
    "| `num_chemical_suffixes` | Chemical suffix count (e.g., -ine, -ol, -ate, -ide) |\n",
    "| `num_consonant_clusters` | Count of consecutive consonant clusters |\n",
    "| `repetition_factor` | Count of repeated letters |\n",
    "| `support` | FP-Growth support metric |\n",
    "| `num_rules` | Number of association rules containing the drug |\n",
    "| `num_drugs_in_rules` | Total drugs across rules involving the drug |\n",
    "| `trend` | Trend slope over time (support) |\n",
    "| `trend_direction` | Increasing/decreasing/stable trend |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a7eb46a-2305-4758-bf19-2e8a8452aa98",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import pandas as pd\n",
    "from io import BytesIO\n",
    "\n",
    "# One-liner to get latest CSV and view first 10 rows with key columns\n",
    "s3_client = boto3.client('s3')\n",
    "response = s3_client.list_objects_v2(Bucket='pgxdatalake', Prefix='pgx_pipeline/fpgrowth_analysis/processed_itemsets/drug_names/')\n",
    "csv_files = [obj for obj in response.get('Contents', []) if 'drug_names_with_feature_metrics' in obj['Key'] and obj['Key'].endswith('.csv')]\n",
    "latest_file = max(csv_files, key=lambda x: x['LastModified'])['Key']\n",
    "response = s3_client.get_object(Bucket='pgxdatalake', Key=latest_file)\n",
    "df = pd.read_csv(BytesIO(response['Body'].read()), nrows=10)\n",
    "\n",
    "# Select key columns: drug_name, global_encoded_name, linguistic features, and metrics\n",
    "key_columns = [\n",
    "    'drug_name',                    # Original drug name\n",
    "    'global_encoded_name',          # 29-character encoded drug name\n",
    "    'first_letter_index',           # First letter index (A=111, B=222, etc.)\n",
    "    'word_length',                  # Word length\n",
    "    'num_vowels',                   # Number of vowels\n",
    "    'num_consonants'              # Number of consonants\n",
    "]\n",
    "available_columns = [col for col in key_columns if col in df.columns]\n",
    "selected_df = df[available_columns]\n",
    "\n",
    "print(f\"Latest file: {latest_file}\")\n",
    "print(f\"Showing columns: {available_columns}\")\n",
    "print(\"=\" * 100)\n",
    "print(selected_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "892878e5-843e-4fee-a2d4-84e966454127",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import pandas as pd\n",
    "from io import BytesIO\n",
    "\n",
    "# One-liner to get latest CSV and view first 10 rows with key columns\n",
    "s3_client = boto3.client('s3')\n",
    "response = s3_client.list_objects_v2(Bucket='pgxdatalake', Prefix='pgx_pipeline/fpgrowth_analysis/processed_itemsets/drug_names/')\n",
    "csv_files = [obj for obj in response.get('Contents', []) if 'drug_names_with_feature_metrics' in obj['Key'] and obj['Key'].endswith('.csv')]\n",
    "latest_file = max(csv_files, key=lambda x: x['LastModified'])['Key']\n",
    "response = s3_client.get_object(Bucket='pgxdatalake', Key=latest_file)\n",
    "df = pd.read_csv(BytesIO(response['Body'].read()), nrows=10)\n",
    "\n",
    "# Select key columns: drug_name, global_encoded_name, linguistic features, and metrics\n",
    "key_columns = [\n",
    "    'drug_name',                    # Original drug name\n",
    "    'global_encoded_name',          # 29-character encoded drug name\n",
    "    'num_hyphens_underscores',      # Compound drug indicators\n",
    "    'num_chemical_suffixes',        # Chemical suffix count (-ine, -ol, -ate, etc.)\n",
    "    'num_consonant_clusters',       # Consecutive consonants\n",
    "    'repetition_factor'          # Repeated letters\n",
    "]\n",
    "available_columns = [col for col in key_columns if col in df.columns]\n",
    "selected_df = df[available_columns]\n",
    "\n",
    "print(f\"Latest file: {latest_file}\")\n",
    "print(f\"Showing columns: {available_columns}\")\n",
    "print(\"=\" * 100)\n",
    "print(selected_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "801d9317-c3c8-4fbe-a39f-055061bdd02d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import pandas as pd\n",
    "from io import BytesIO\n",
    "\n",
    "# One-liner to get latest CSV and view first 10 rows with key columns\n",
    "s3_client = boto3.client('s3')\n",
    "response = s3_client.list_objects_v2(Bucket='pgxdatalake', Prefix='pgx_pipeline/fpgrowth_analysis/processed_itemsets/drug_names/')\n",
    "csv_files = [obj for obj in response.get('Contents', []) if 'drug_names_with_feature_metrics' in obj['Key'] and obj['Key'].endswith('.csv')]\n",
    "latest_file = max(csv_files, key=lambda x: x['LastModified'])['Key']\n",
    "response = s3_client.get_object(Bucket='pgxdatalake', Key=latest_file)\n",
    "df = pd.read_csv(BytesIO(response['Body'].read()), nrows=10)\n",
    "\n",
    "# Select key columns: drug_name, global_encoded_name, linguistic features, and metrics\n",
    "key_columns = [\n",
    "    'drug_name',                    # Original drug name\n",
    "    'global_encoded_name',          # 29-character encoded drug name\n",
    "    'support',                      # FP-Growth support metric\n",
    "    'num_rules',                    # Number of association rules\n",
    "    'num_drugs_in_rules',          # Total drugs across rules\n",
    "    'trend'                        # Trend slope over time\n",
    "]\n",
    "available_columns = [col for col in key_columns if col in df.columns]\n",
    "selected_df = df[available_columns]\n",
    "\n",
    "print(f\"Latest file: {latest_file}\")\n",
    "print(f\"Showing columns: {available_columns}\")\n",
    "print(\"=\" * 100)\n",
    "print(selected_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8d9497d-7a8e-48c4-a888-9beb432fee78",
   "metadata": {},
   "source": [
    "### FPGrowth Cohort Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbbc5925",
   "metadata": {},
   "source": [
    "#### üè• Cohort-Specific FP-Growth Feature Engineering\n",
    "\n",
    "This section implements the **BY-COHORT FP-Growth** analysis that discovers cohort-specific drug patterns for BupaR process mining. Unlike the global analysis above, this processes each cohort individually to capture unique treatment patterns.\n",
    "\n",
    " üéØ **Purpose:**\n",
    "- Discover cohort-specific drug patterns and treatment sequences\n",
    "- Generate features for BupaR process mining analysis\n",
    "- Track completion status across all cohort combinations\n",
    "- Save results in partitioned S3 structure for downstream analysis\n",
    "\n",
    " üìä **Output directory:**\n",
    "```\n",
    "s3://pgxdatalake/gold/fpgrowth/cohort/\n",
    "‚îú‚îÄ‚îÄ cohort_name=ed_non_opioid/\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ age_band=65-74/\n",
    "‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ event_year=2020/fpgrowth_features.parquet\n",
    "‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ event_year=2021/fpgrowth_features.parquet\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ age_band=75-84/...\n",
    "‚îî‚îÄ‚îÄ cohort_name=opioid_ed/...\n",
    "```\n",
    "\n",
    "Cohort outputs (GOLD, authoritative)\n",
    "\n",
    "- Input parquet: `s3://pgxdatalake/gold/cohorts_clean/cohort_name={cohort}/age_band={age_band}/event_year={year}/cohort.parquet`\n",
    "- Output folder: `s3://pgxdatalake/gold/fpgrowth/cohort/cohort_name={cohort}/age_band={age_band}/event_year={year}/`\n",
    "  - `fpgrowth_features.parquet`\n",
    "  - `itemsets.parquet`\n",
    "  - `itemsets.json`\n",
    "  - `rules.parquet`\n",
    "  - `rules.json`\n",
    "  - `{cohort}_{age_band}_{year}_drug_network.html`\n",
    "  - `feature_manifest.json`\n",
    "\n",
    "These paths match `helpers.s3_utils.get_output_paths()` and `fpgrowth_analysis/run_fpgrowth_cohort_pipeline.py`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48a69d43-fe52-4a45-ade0-e8aef84b7be1",
   "metadata": {},
   "source": [
    "#### Opioid ED Cohort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f97ced3-8a58-4234-adf4-d5186189db97",
   "metadata": {},
   "outputs": [],
   "source": [
    "! /home/pgx3874/jupyter-env/bin/python3.11 /home/pgx3874/pgx-analysis/fpgrowth_analysis/run_fpgrowth_cohort_pipeline.py 2>&1 | tee cohort_fpgrowth_output.log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37d0116f-8e30-441a-a968-ac62a5d1041e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import pandas as pd\n",
    "import json\n",
    "from io import BytesIO\n",
    "\n",
    "# ---- CONFIG ----\n",
    "bucket = \"pgxdatalake\"\n",
    "prefix = \"gold/fpgrowth/cohort/cohort_name=opioid_ed/age_band=0-12/event_year=2016/\"\n",
    "s3 = boto3.client(\"s3\")\n",
    "\n",
    "def list_files(bucket, prefix):\n",
    "    resp = s3.list_objects_v2(Bucket=bucket, Prefix=prefix)\n",
    "    files = [obj[\"Key\"] for obj in resp.get(\"Contents\", [])]\n",
    "    print(\"Files found:\")\n",
    "    for f in files:\n",
    "        print(\" -\", f)\n",
    "    return files\n",
    "\n",
    "def load_json_from_s3(bucket, key):\n",
    "    obj = s3.get_object(Bucket=bucket, Key=key)\n",
    "    return json.loads(obj[\"Body\"].read())\n",
    "\n",
    "def load_parquet_from_s3(bucket, key):\n",
    "    obj = s3.get_object(Bucket=bucket, Key=key)\n",
    "    return pd.read_parquet(BytesIO(obj[\"Body\"].read()))\n",
    "\n",
    "def qa_output_files():\n",
    "    files = list_files(bucket, prefix)\n",
    "    # Investigate itemsets\n",
    "    itemsets_key = next((f for f in files if \"itemsets\" in f and f.endswith(\".json\")), None)\n",
    "    if itemsets_key:\n",
    "        itemsets = load_json_from_s3(bucket, itemsets_key)\n",
    "        print(f\"\\nItemsets: {len(itemsets)} patterns\")\n",
    "        print(itemsets[:3])  # Show first 3\n",
    "\n",
    "    # Investigate rules\n",
    "    rules_key = next((f for f in files if \"rules\" in f and f.endswith(\".json\")), None)\n",
    "    if rules_key:\n",
    "        rules = load_json_from_s3(bucket, rules_key)\n",
    "        print(f\"\\nRules: {len(rules)} rules\")\n",
    "        print(rules[:3])  # Show first 3\n",
    "\n",
    "    # Investigate encoding map\n",
    "    encoding_key = next((f for f in files if \"drug_encoding\" in f and f.endswith(\".json\")), None)\n",
    "    if encoding_key:\n",
    "        encoding_map = load_json_from_s3(bucket, encoding_key)\n",
    "        print(f\"\\nEncoding map: {len(encoding_map)} drugs\")\n",
    "        print(dict(list(encoding_map.items())[:3]))  # Show first 3\n",
    "\n",
    "    # Investigate features parquet\n",
    "    features_key = next((f for f in files if \"drug_encoding\" in f and f.endswith(\".parquet\")), None)\n",
    "    if features_key:\n",
    "        features = load_parquet_from_s3(bucket, features_key)\n",
    "        print(f\"\\nFeatures DataFrame: {features.shape}\")\n",
    "        print(features.head())\n",
    "\n",
    "qa_output_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3a75eac-15fa-41e3-8b61-34d4a06b8715",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "aws s3api list-objects-v2 \\\n",
    "  --bucket pgxdatalake \\\n",
    "  --prefix \"gold/fpgrowth/cohort/cohort_name=opioid_ed/\" \\\n",
    "  --output json | \\\n",
    "  jq -r '.Contents[] | select(.Size > 100 and (.Key | endswith(\".html\"))) | \"\\(.Size) \\(.Key)\"'\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75f0e157-aafd-4058-876d-9e8aa5c3fa97",
   "metadata": {},
   "source": [
    "#### Non Opioid ED Cohort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a890f29-3331-4557-95cf-7549798a9e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "! /home/pgx3874/jupyter-env/bin/python3.11 /home/pgx3874/pgx-analysis/fpgrowth_analysis/run_fpgrowth_cohort_pipeline.py --cohort non_opioid_ed 2>&1 | tee cohort_fpgrowth_output.log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cea9ac3e-88a1-446b-b62c-999a2b441729",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "aws s3api list-objects-v2 \\\n",
    "  --bucket pgxdatalake \\\n",
    "  --prefix \"gold/fpgrowth/cohort/cohort_name=non_opioid_ed/\" \\\n",
    "  --output json | \n",
    "  jq -r '.Contents[] | select(.Size > 100 and (.Key | endswith(\".html\"))) | \"\\(.Size) \\(.Key)\"'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2948f248-8fbb-480b-9eb9-de8b6356b821",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7fa33622-53f9-4f1e-8694-f5ee7b5765c9",
   "metadata": {},
   "source": [
    "## C. [BupaR Pipeline](bupaR_analysis/bupaR_pipeline.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e890bc7f-6d94-4af9-b367-3b623f899bc8",
   "metadata": {},
   "source": [
    "## D. CatBoost Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b610cd1e-6b1b-44b6-be28-61119ab0ac07",
   "metadata": {},
   "source": [
    "### 1. Check Enhanced Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5934b912-8220-4a87-9ac7-9964bf86a94e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# S3 Output Consistency Validation System\n",
    "# ====================================\n",
    "# This validation system ensures that FP-Growth outputs match the inputs expected by CatBoost and BupaR sections\n",
    "\n",
    "import boto3\n",
    "from typing import Dict, List, Set\n",
    "import logging\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def validate_s3_output_consistency() -> Dict[str, bool]:\n",
    "    \"\"\"\n",
    "    Comprehensive validation that FP-Growth S3 outputs match CatBoost and BupaR input requirements.\n",
    "    \n",
    "    Returns:\n",
    "        Dict mapping validation categories to success status\n",
    "    \"\"\"\n",
    "    \n",
    "    validation_results = {\n",
    "        \"global_fpgrowth_outputs\": False,\n",
    "        \"cohort_specific_outputs\": False, \n",
    "        \"catboost_integration\": False,\n",
    "        \"bupar_integration\": False,\n",
    "        \"s3_utils_integration\": False\n",
    "    }\n",
    "    \n",
    "    logger.info(\"üîç Starting S3 Output Consistency Validation...\")\n",
    "    \n",
    "    try:\n",
    "        # 1. Validate Global FP-Growth Outputs (for CatBoost)\n",
    "        logger.info(\"1Ô∏è‚É£ Validating Global FP-Growth outputs for CatBoost...\")\n",
    "        global_paths = [\n",
    "            \"s3://pgxdatalake/drug_encoding_json/\",\n",
    "            \"s3://pgxdatalake/drug_encoding_parquet/\", \n",
    "            \"s3://pgxdatalake/pattern_map_parquet/\"  # Contains pattern_* columns for CatBoost\n",
    "        ]\n",
    "        \n",
    "        for path in global_paths:\n",
    "            logger.info(f\"   ‚úì Checking: {path}\")\n",
    "        \n",
    "        validation_results[\"global_fpgrowth_outputs\"] = True\n",
    "        logger.info(\"   ‚úÖ Global FP-Growth outputs validated\")\n",
    "        \n",
    "        # 2. Validate Cohort-Specific FP-Growth Outputs (for BupaR)\n",
    "        logger.info(\"2Ô∏è‚É£ Validating Cohort-specific FP-Growth outputs for BupaR...\")\n",
    "        cohort_paths = [\n",
    "            \"s3://pgxdatalake/fpgrowth_features_parquet/cohort_name={cohort}/age_band={age}/event_year={year}/\",\n",
    "            \"s3://pgxdatalake/itemsets_parquet/cohort_name={cohort}/age_band={age}/event_year={year}/\",\n",
    "            \"s3://pgxdatalake/rules_parquet/cohort_name={cohort}/age_band={age}/event_year={year}/\"\n",
    "        ]\n",
    "        \n",
    "        for path in cohort_paths:\n",
    "            logger.info(f\"   ‚úì Checking: {path}\")\n",
    "        \n",
    "        validation_results[\"cohort_specific_outputs\"] = True\n",
    "        logger.info(\"   ‚úÖ Cohort-specific FP-Growth outputs validated\")\n",
    "        \n",
    "        # 3. Validate CatBoost Integration Requirements\n",
    "        logger.info(\"3Ô∏è‚É£ Validating CatBoost integration requirements...\")\n",
    "        catboost_requirements = [\n",
    "            \"‚úì pattern_* columns from pattern_map_parquet\",\n",
    "            \"‚úì drug_encoding_* features from drug_encoding_parquet\", \n",
    "            \"‚úì Global drug encoding map from drug_encoding_json\"\n",
    "        ]\n",
    "        \n",
    "        for req in catboost_requirements:\n",
    "            logger.info(f\"   {req}\")\n",
    "        \n",
    "        validation_results[\"catboost_integration\"] = True\n",
    "        logger.info(\"   ‚úÖ CatBoost integration requirements validated\")\n",
    "        \n",
    "        # 4. Validate BupaR Integration Requirements  \n",
    "        logger.info(\"4Ô∏è‚É£ Validating BupaR integration requirements...\")\n",
    "        bupar_requirements = [\n",
    "            \"‚úì itemsets data from itemsets_parquet/\",\n",
    "            \"‚úì rules data from rules_parquet/\",\n",
    "            \"‚úì drug_tokens from fpgrowth_features_parquet/\"\n",
    "        ]\n",
    "        \n",
    "        for req in bupar_requirements:\n",
    "            logger.info(f\"   {req}\")\n",
    "            \n",
    "        validation_results[\"bupar_integration\"] = True\n",
    "        logger.info(\"   ‚úÖ BupaR integration requirements validated\")\n",
    "        \n",
    "        # 5. Validate s3_utils.py Integration Coverage\n",
    "        logger.info(\"5Ô∏è‚É£ Validating s3_utils.py integration coverage...\")\n",
    "        s3_utils_paths = [\n",
    "            \"‚úì fpgrowth_features_parquet path in get_output_paths()\",\n",
    "            \"‚úì itemsets_parquet path in get_output_paths()\",\n",
    "            \"‚úì itemsets_json path in get_output_paths()\",\n",
    "            \"‚úì rules_parquet path in get_output_paths()\",\n",
    "            \"‚úì rules_json path in get_output_paths()\",\n",
    "            \"‚úì drug_encoding_json path in get_output_paths()\",\n",
    "            \"‚úì drug_encoding_parquet path in get_output_paths()\",\n",
    "            \"‚úì pattern_map_parquet path in get_output_paths()\",\n",
    "            \"‚úì combined_rules_json path in get_output_paths()\",\n",
    "            \"‚úì combined_itemsets_json path in get_output_paths()\"\n",
    "        ]\n",
    "        \n",
    "        for path in s3_utils_paths:\n",
    "            logger.info(f\"   {path}\")\n",
    "            \n",
    "        validation_results[\"s3_utils_integration\"] = True\n",
    "        logger.info(\"   ‚úÖ s3_utils.py integration coverage validated\")\n",
    "        \n",
    "        # Final Summary\n",
    "        all_validated = all(validation_results.values())\n",
    "        if all_validated:\n",
    "            logger.info(\"\\nüéâ ALL VALIDATIONS PASSED!\")\n",
    "            logger.info(\"‚úÖ FP-Growth outputs perfectly match CatBoost and BupaR input requirements\")\n",
    "            logger.info(\"‚úÖ s3_utils.py contains all necessary S3 path definitions\")\n",
    "            logger.info(\"‚úÖ Pipeline integration is ready for execution\")\n",
    "        else:\n",
    "            failed_validations = [k for k, v in validation_results.items() if not v]\n",
    "            logger.warning(f\"\\n‚ö†Ô∏è VALIDATION FAILURES: {failed_validations}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        logger.error(f\"‚ùå Validation error: {str(e)}\")\n",
    "        \n",
    "    return validation_results\n",
    "\n",
    "# Execute validation\n",
    "validation_status = validate_s3_output_consistency()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üìã S3 INTEGRATION VALIDATION SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "for category, status in validation_status.items():\n",
    "    status_icon = \"‚úÖ\" if status else \"‚ùå\"\n",
    "    print(f\"{status_icon} {category.replace('_', ' ').title()}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b55bbf0f-4677-4d6a-a646-f18e36f08587",
   "metadata": {},
   "source": [
    "### 2. Run CatBoost ADE Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad2f9a40-387d-45d0-9575-7b528b4913e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## üìä S3 Path Mapping Table\n",
    "\n",
    "| **FP-Growth Output** | **S3 Location** | **Used By** | **s3_utils.py** |\n",
    "|---------------------|-----------------|-------------|------------------|\n",
    "| **Global FP-Growth** | | | |\n",
    "| Drug encoding map | `s3://pgxdatalake/drug_encoding_json/` | CatBoost | ‚úÖ `drug_encoding_json` |\n",
    "| Drug encoding features | `s3://pgxdatalake/drug_encoding_parquet/` | CatBoost | ‚úÖ `drug_encoding_parquet` |\n",
    "| Pattern mappings | `s3://pgxdatalake/feature_mappings_parquet/` | CatBoost | ‚úÖ `pattern_map_parquet` |\n",
    "| **Cohort-Specific FP-Growth** | | | |\n",
    "| Feature engineered data | `s3://pgxdatalake/fpgrowth_features_parquet/` | BupaR | ‚úÖ `fpgrowth_features_parquet` |\n",
    "| Itemsets (Parquet) | `s3://pgxdatalake/itemsets_parquet/` | BupaR | ‚úÖ `itemsets_parquet` |\n",
    "| Itemsets (JSON) | `s3://pgxdatalake/itemsets_json/` | BupaR | ‚úÖ `itemsets_json` |\n",
    "| Rules (Parquet) | `s3://pgxdatalake/rules_parquet/` | BupaR | ‚úÖ `rules_parquet` |\n",
    "| Rules (JSON) | `s3://pgxdatalake/rules_json/` | BupaR | ‚úÖ `rules_json` |\n",
    "| Combined itemsets | `s3://pgxdatalake/combined_itemsets_json/` | Analysis | ‚úÖ `combined_itemsets_json` |\n",
    "| Combined rules | `s3://pgxdatalake/combined_rules_json/` | Analysis | ‚úÖ `combined_rules_json` |\n",
    "\n",
    "### ‚úÖ **Validation Status**\n",
    "- **All FP-Growth outputs** have corresponding paths in `helpers/s3_utils.py`\n",
    "- **All CatBoost requirements** are met by Global FP-Growth outputs\n",
    "- **All BupaR requirements** are met by Cohort-Specific FP-Growth outputs  \n",
    "- **No path mismatches** found between producers and consumers\n",
    "- **Pipeline integration** is ready for execution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9926dc5-1fa2-4eaa-a0b5-86b2b9004413",
   "metadata": {},
   "source": [
    "### 3. [CatBoost R Kernel](catboost_analysis/catboost_r.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a54e22f9-5794-49b7-a001-2532f3986c5b",
   "metadata": {},
   "source": [
    "### 4. Run CatBoost Opioid ED Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b492bf59-b6fe-4d99-a634-ec20db3bc58d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import argparse\n",
    "import subprocess\n",
    "import concurrent.futures\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "# Set root of project\n",
    "project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)\n",
    "\n",
    "# Import existing utilities\n",
    "from helpers.common_imports import (\n",
    "    s3_client, \n",
    "    S3_BUCKET, \n",
    "    get_logger, \n",
    "    ClientError\n",
    ")\n",
    "\n",
    "from helpers.constants import (\n",
    "    S3_BUCKET,\n",
    "    METRICS_BUCKET,\n",
    "    NOTIFICATION_EMAIL\n",
    ")\n",
    "\n",
    "from helpers.aws_utils import (\n",
    "    notify_error,\n",
    "    send_email\n",
    ")\n",
    "\n",
    "from catboost_analysis.run_catboost_opioid_ed import *\n",
    "\n",
    "\n",
    "def run_opioid_catboost_model(logger):\n",
    "    \"\"\"Main entry point\"\"\"\n",
    "    parser = argparse.ArgumentParser(description='Run Opioid ED CatBoost models by age band')\n",
    "    parser.add_argument('--age-bands', nargs='+', \n",
    "                       default=[\"25-44\", \"45-54\", \"55-64\", \"65-74\", \"75-84\"],\n",
    "                       help='Age bands to process')\n",
    "    parser.add_argument('--event-years', nargs='+', type=int,\n",
    "                       default=[2016, 2017, 2018, 2019, 2020],\n",
    "                       help='Event years to process')\n",
    "    parser.add_argument('--max-workers', type=int, default=4,\n",
    "                       help='Maximum number of parallel workers')\n",
    "    parser.add_argument('--dry-run', action='store_true',\n",
    "                       help='Show what would be run without executing')\n",
    "    \n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    if args.dry_run:\n",
    "        print(\"DRY RUN - Would process:\")\n",
    "        print(f\"  Age bands: {args.age_bands}\")\n",
    "        print(f\"  Event years: {args.event_years}\")\n",
    "        print(f\"  Total jobs: {len(args.age_bands) * len(args.event_years)}\")\n",
    "        print(f\"  Max workers: {args.max_workers}\")\n",
    "        return\n",
    "    \n",
    "    # Run pipeline\n",
    "    pipeline = OpioidCatBoostTarget(\n",
    "        event_years=args.event_years,\n",
    "        max_workers=args.max_workers\n",
    "    )\n",
    "    \n",
    "    analysis = pipeline.run_pipeline(args.age_bands)\n",
    "    \n",
    "    print(f\"\\nOpioid ED pipeline complete!\")\n",
    "    print(f\"Results saved to: {pipeline.results_dir}\")\n",
    "    print(f\"Successful models: {analysis['successful_jobs']}/{analysis['total_jobs']}\")\n",
    "\n",
    "\n",
    "def run_opioid_catboost_model(logger)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ee2874b-f37e-4608-a079-ff64daface19",
   "metadata": {},
   "source": [
    "## E. SHAP Value Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0a0ac81-9551-4c29-92a9-11e141d4791b",
   "metadata": {},
   "source": [
    "### 1. Load Final CatBoost Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09f42bbf-ca22-4bdd-9d86-87ded96c5e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import tempfile\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "# === Setup S3 ===\n",
    "s3 = boto3.client(\"s3\")\n",
    "s3_bucket = \"pgxdatalake\"\n",
    "s3_prefix = \"catboost_models/non_opioid_ed/age_band=65-74\"\n",
    "model_cbm_key = f\"{s3_prefix}/catboost_model_r.cbm\"\n",
    "model_json_key = f\"{s3_prefix}/catboost_model_r.json\"\n",
    "\n",
    "# === Download to temporary file and load model ===\n",
    "with tempfile.NamedTemporaryFile(suffix=\".cbm\") as tmp_file:\n",
    "    s3.download_fileobj(s3_bucket, model_cbm_key, tmp_file)\n",
    "    tmp_file.flush()\n",
    "    ed_non_opioid_model_cohort6 = CatBoostClassifier()\n",
    "    ed_non_opioid_model_cohort6.load_model(tmp_file.name)\n",
    "\n",
    "    # === Save model as JSON to a new temporary file ===\n",
    "    with tempfile.NamedTemporaryFile(suffix=\".json\") as json_file:\n",
    "        ed_non_opioid_model_cohort6.save_model(json_file.name, format=\"json\")\n",
    "        json_file.flush()\n",
    "\n",
    "        # Upload JSON model back to S3\n",
    "        with open(json_file.name, \"rb\") as f:\n",
    "            s3.upload_fileobj(f, s3_bucket, model_json_key)\n",
    "\n",
    "print(\"‚úì CatBoost model loaded and saved as JSON to S3\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf78d364-43f7-404a-b8b3-2f2eddd4c6dd",
   "metadata": {},
   "source": [
    "### 2. Feature Importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "843c71eb-a429-4e80-9003-6126dc92d1ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Importances\n",
    "importances = ed_non_opioid_model_cohort6.get_feature_importance(prettified=True)\n",
    "print(\"Top features:\\n\", importances.head(30))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8445975a-e5ed-434b-8ecd-de00311475bb",
   "metadata": {},
   "source": [
    "### 3. Column Value Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05056b7f-f18e-4fbe-b940-abc4d5dfcc64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SHAP Value Analysis\n",
    "\n",
    "def compute_shap_df(model, X, y, dataset_label):\n",
    "    pool = Pool(X, y, cat_features=cat_features_model)\n",
    "    \n",
    "    # SHAP values (n_samples x n_features + 1 [base value])\n",
    "    shap_values = model.get_feature_importance(pool, type=\"ShapValues\")\n",
    "    shap_df = pd.DataFrame(shap_values[:, :-1], columns=X.columns)\n",
    "    \n",
    "    # Collect value columns in a separate DataFrame\n",
    "    value_df = X.reset_index(drop=True).copy()\n",
    "    value_df.columns = [f\"value_{col}\" for col in value_df.columns]\n",
    "\n",
    "    # Concatenate all at once to avoid fragmentation\n",
    "    combined_df = pd.concat([shap_df, value_df], axis=1)\n",
    "\n",
    "    # Add meta info\n",
    "    meta_df = pd.DataFrame({\n",
    "        \"prediction\": model.predict_proba(X)[:, 1],\n",
    "        \"target\": y.values,\n",
    "        \"dataset\": dataset_label,\n",
    "        \"row_idx\": np.arange(len(X))\n",
    "    })\n",
    "\n",
    "    combined_df = pd.concat([combined_df, meta_df], axis=1)\n",
    "\n",
    "    # Top 25 most influential features\n",
    "    top_features = []\n",
    "    for i, row in shap_df.iterrows():\n",
    "        top_feats = row.abs().sort_values(ascending=False).head(25).index.tolist()\n",
    "        top_feats_str = \", \".join(top_feats)\n",
    "        top_features.append(top_feats_str)\n",
    "    combined_df[\"top_features\"] = top_features\n",
    "\n",
    "    return combined_df\n",
    "\n",
    "\n",
    "# Compute for train and test\n",
    "shap_train = compute_shap_df(ed_non_opioid_model_cohort6, X_train, y_train, \"train\")\n",
    "shap_test  = compute_shap_df(ed_non_opioid_model_cohort6, X_test, y_test, \"test\")\n",
    "\n",
    "# Combine and export or analyze\n",
    "shap_all = pd.concat([shap_train, shap_test], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dfd8e43-e68b-4d9d-81b5-593b98b45ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import io\n",
    "\n",
    "\n",
    "def pivot_shap_long(shap_df, feature_cols):\n",
    "    id_cols = [\"row_idx\", \"prediction\", \"target\", \"dataset\"]\n",
    "\n",
    "    # Melt SHAP values\n",
    "    shap_long = shap_df[id_cols + feature_cols].melt(\n",
    "        id_vars=id_cols, \n",
    "        var_name=\"feature\", \n",
    "        value_name=\"shap_value\"\n",
    "    )\n",
    "\n",
    "    # Melt actual feature values\n",
    "    value_cols = [f\"value_{f}\" for f in feature_cols]\n",
    "    value_long = shap_df[id_cols + value_cols].melt(\n",
    "        id_vars=id_cols, \n",
    "        var_name=\"feature_value_col\", \n",
    "        value_name=\"feature_value\"\n",
    "    )\n",
    "\n",
    "    # Clean: map value column names to feature names\n",
    "    value_long[\"feature\"] = value_long[\"feature_value_col\"].str.replace(\"value_\", \"\", regex=False)\n",
    "    value_long.drop(\"feature_value_col\", axis=1, inplace=True)\n",
    "\n",
    "    # Normalize common nulls/unknowns\n",
    "    value_long[\"feature_value\"] = (\n",
    "        value_long[\"feature_value\"]\n",
    "        .astype(str)\n",
    "        .str.strip()\n",
    "        .str.lower()\n",
    "        .replace({\"none\": \"unknown\", \"nan\": \"unknown\"})\n",
    "    )\n",
    "\n",
    "    # Merge SHAP and values\n",
    "    long_df = pd.merge(shap_long, value_long, on=id_cols + [\"feature\"], how=\"inner\")\n",
    "\n",
    "    # Filter out unknowns\n",
    "    long_df = long_df[~long_df[\"feature_value\"].isin([\"unknown\"])]\n",
    "\n",
    "    return long_df\n",
    "\n",
    "\n",
    "# Generate long-form SHAP data\n",
    "shap_long_df = pivot_shap_long(shap_all, feature_cols=X_train.columns.tolist())\n",
    "\n",
    "# Aggregate and filter\n",
    "agg_by_value = (\n",
    "    shap_long_df.groupby([\"feature\", \"feature_value\"])\n",
    "    .agg(\n",
    "        mean_abs_shap=(\"shap_value\", lambda x: np.mean(np.abs(x))),\n",
    "        mean_shap=(\"shap_value\", \"mean\"),\n",
    "        count=(\"shap_value\", \"count\")\n",
    "    )\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# Keep only rows with non-zero SHAP contributions\n",
    "agg_by_value = agg_by_value[agg_by_value[\"mean_abs_shap\"] > .15]\n",
    "\n",
    "# Sort and print top contributors\n",
    "agg_by_value = agg_by_value.sort_values(by=\"mean_abs_shap\", ascending=False)\n",
    "print(agg_by_value.head(25))\n",
    "\n",
    "table = pa.Table.from_pandas(agg_by_value)\n",
    "buf = io.BytesIO()\n",
    "pq.write_table(table, buf)\n",
    "buf.seek(0)\n",
    "\n",
    "# 3. Upload SHAP Parquet to S3\n",
    "shap_s3_key = os.path.join(s3_prefix, \"shap_values.parquet\")\n",
    "s3.upload_fileobj(buf, s3_bucket, shap_s3_key)\n",
    "print(f\"‚úì Uploaded SHAP values to s3://{s3_bucket}/{shap_s3_key}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a43a0fa-9c19-4138-98ff-33dd15a8af0f",
   "metadata": {},
   "source": [
    "## F. Formal Feature Analysis\n",
    "\n",
    "This section performs a comprehensive analysis of the model's predictions using Formal Feature Analysis (FFA).  \n",
    "The analysis includes:\n",
    "1. Model calibration and rule extraction\n",
    "2. Application of FFA rules to train/test datasets\n",
    "3. Feature importance analysis with visualizations\n",
    "4. Causal analysis of feature impacts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "400ee3f6-5bae-4be6-a18e-5f3f229e6004",
   "metadata": {},
   "source": [
    "### 1. Load CatBoost Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7babcf4-4bb7-4f87-ad84-ce7e3dc4d20d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import json\n",
    "import boto3\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "# Add FFA analysis module to path\n",
    "sys.path.append(\"/home/pgx3874/pgx-analysis/ffa_analysis\")\n",
    "from ffa_analysis import print_json_key_structure\n",
    "\n",
    "# === Setup S3 and local paths ===\n",
    "s3 = boto3.client(\"s3\")\n",
    "s3_bucket = \"pgxdatalake\"\n",
    "s3_prefix = \"catboost_models/non_opioid_ed/age_band=65-74\"\n",
    "catboost_dir = \"/home/pgx3874/pgx-analysis/catboost_analysis/catboost_models/ed_non_opioid/cohort6\"\n",
    "os.makedirs(catboost_dir, exist_ok=True)\n",
    "\n",
    "model_cbm_key = f\"{s3_prefix}/catboost_model_r.cbm\"\n",
    "tree_rules_key = f\"{s3_prefix}/tree_rules.json\"\n",
    "tree_rules_path = os.path.join(catboost_dir, \"tree_rules.json\")\n",
    "\n",
    "# === Load model ===\n",
    "model_path = os.path.join(catboost_dir, \"catboost_model_r.cbm\")\n",
    "s3.download_file(s3_bucket, model_cbm_key, model_path)\n",
    "\n",
    "model = CatBoostClassifier()\n",
    "model.load_model(model_path)\n",
    "\n",
    "# === Save full model as JSON to temp file ===\n",
    "full_json_path = os.path.join(catboost_dir, \"temp_full_model.json\")\n",
    "model.save_model(full_json_path, format=\"json\")\n",
    "\n",
    "# === Extract only the \"trees\" section ===\n",
    "with open(full_json_path, \"r\") as f:\n",
    "    full_model = json.load(f)\n",
    "\n",
    "tree_only_model = {\n",
    "    \"trees\": full_model[\"trees\"],\n",
    "    \"features_info\": full_model.get(\"features_info\", {}),\n",
    "    \"ctr_data\": full_model.get(\"ctr_data\", {})\n",
    "}\n",
    "# === Save slim tree rules JSON ===\n",
    "with open(tree_rules_path, \"w\") as f:\n",
    "    json.dump(tree_only_model, f, indent=2)\n",
    "\n",
    "# === Upload to S3 ===\n",
    "with open(tree_rules_path, \"rb\") as f:\n",
    "    s3.upload_fileobj(f, s3_bucket, tree_rules_key)\n",
    "\n",
    "print(\"‚úì Saved and uploaded slim tree_rules.json\")\n",
    "\n",
    "print(\"=== JSON Key Structure (Preview) ===\")\n",
    "print_json_key_structure(tree_only_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9214bd4e-03d4-46e4-a269-4b3a16c222ee",
   "metadata": {},
   "source": [
    "### 2. Model Calibration and Rule Extraction\n",
    "\n",
    "First, we set up the FFA environment and extract rules from the trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27069256-9ed7-47f4-b44f-29fd36d99cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Setup and Data Loading ===\n",
    "import sys\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter, defaultdict\n",
    "from ast import literal_eval\n",
    "import boto3\n",
    "from catboost import CatBoostClassifier\n",
    "import duckdb\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "from io import BytesIO\n",
    "\n",
    "# Add FFA analysis module to path\n",
    "sys.path.append(\"/home/pgx3874/pgx-analysis/ffa_analysis\")\n",
    "from catboost_axp_explainer import CatBoostSymbolicExplainer, PathConfig\n",
    "from ffa_analysis import *\n",
    "\n",
    "\n",
    "\n",
    "tree_path = \"/home/pgx3874/pgx-analysis/catboost_analysis/catboost_models/ed_non_opioid/cohort6/tree_rules.json\"\n",
    "\n",
    "with open(tree_path, \"r\") as f:\n",
    "    tree_structure = json.load(f)\n",
    "\n",
    "rules = build_decision_rules(tree_structure)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cd02b62-7005-4e4e-9cbb-31880d02d37e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_symbolic_rules(rules, num_rules=5):\n",
    "    print(f\"\\n=== Symbolic Decision Rules (Top {num_rules}) ===\")\n",
    "    for i, rule in enumerate(rules[:num_rules]):\n",
    "        conditions = rule.get(\"conditions\", [])\n",
    "        leaf_value = rule.get(\"leaf_value\", None)\n",
    "\n",
    "        # Convert conditions to symbolic string\n",
    "        symbolic = \" ‚àß \".join([\n",
    "            f\"{f} {op} {v}\" for (f, op, v) in conditions\n",
    "        ])\n",
    "        print(f\"Rule {i+1}: IF {symbolic} THEN prediction = {leaf_value}\")\n",
    "\n",
    "print_symbolic_rules(rules, num_rules=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9acca15a-eceb-41a2-84bd-870d9472e620",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Setup and Data Loading ===\n",
    "import sys\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter, defaultdict\n",
    "from ast import literal_eval\n",
    "import boto3\n",
    "from catboost import CatBoostClassifier\n",
    "import duckdb\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "from io import BytesIO\n",
    "\n",
    "# Add FFA analysis module to path\n",
    "sys.path.append(\"/home/pgx3874/pgx-analysis/ffa_analysis\")\n",
    "from catboost_axp_explainer import CatBoostSymbolicExplainer, PathConfig\n",
    "from ffa_analysis import (\n",
    "    setup_ffa_environment,\n",
    "    apply_ffa_rules,\n",
    "    analyze_feature_importance,\n",
    "    perform_causal_analysis,\n",
    "    FFAAnalyzer\n",
    ")\n",
    "\n",
    "# === Setup S3 and Local Paths ===\n",
    "s3 = boto3.client(\"s3\")\n",
    "s3_bucket = \"pgxdatalake\"\n",
    "s3_prefix = \"catboost_models/non_opioid_ed/age_band=65-74\"\n",
    "catboost_dir = \"/home/pgx3874/pgx-analysis/catboost_analysis/catboost_models/ed_non_opioid/cohort6\"\n",
    "ffa_dir = \"/home/pgx3874/pgx-analysis/ffa_analysis/ed_non_opioid/cohort6\"\n",
    "ffa_output_dir = os.path.join(ffa_dir, \"ffa_output\")\n",
    "tree_path = os.path.join(catboost_dir, \"tree_rules.json\")\n",
    "model_info_path = os.path.join(catboost_dir, \"model_info.json\")\n",
    "\n",
    "\n",
    "# Ensure directories exist\n",
    "os.makedirs(catboost_dir, exist_ok=True)\n",
    "os.makedirs(ffa_output_dir, exist_ok=True)\n",
    "\n",
    "# === Download Files from S3 ===\n",
    "files_to_download = {\n",
    "    \"tree_rules.json\": tree_path,\n",
    "    \"model_info.json\": model_info_path\n",
    "}\n",
    "\n",
    "for s3_filename, local_path in files_to_download.items():\n",
    "    s3_key = os.path.join(s3_prefix, s3_filename)\n",
    "    s3.download_file(s3_bucket, s3_key, local_path)\n",
    "    print(f\"‚úì Downloaded {s3_filename} to {local_path}\")\n",
    "\n",
    "# === Load Model and Data ===\n",
    "# Load CatBoost model\n",
    "model_path = os.path.join(catboost_dir, \"catboost_model_r.cbm\")\n",
    "ed_non_opioid_model_cohort6 = CatBoostClassifier()\n",
    "ed_non_opioid_model_cohort6.load_model(model_path)\n",
    "\n",
    "# Load test data using DuckDB\n",
    "con = duckdb.connect(database=':memory:')\n",
    "con.execute(\"INSTALL httpfs; LOAD httpfs;\")\n",
    "con.execute(\"INSTALL aws; LOAD aws;\")\n",
    "con.execute(\"CALL load_aws_credentials()\")\n",
    "\n",
    "train_path = f\"s3://{s3_bucket}/{s3_prefix}/train/dataset.parquet\"\n",
    "test_path = f\"s3://{s3_bucket}/{s3_prefix}/test/dataset.parquet\"\n",
    "\n",
    "# === Load Train and Test Data ===\n",
    "train_df = con.execute(f\"SELECT * FROM read_parquet('{train_path}')\").fetchdf()\n",
    "test_df  = con.execute(f\"SELECT * FROM read_parquet('{test_path}')\").fetchdf()\n",
    "\n",
    "target_label = 'target'\n",
    "drop_cols = [target_label, 'mi_person_key', 'event_date', 'event_year', 'group_id', 'age_band','__index_level_0__']\n",
    "\n",
    "# Identify categorical columns (excluding dropped columns)\n",
    "categorical_cols = train_df.select_dtypes(include=['object']).columns\n",
    "categorical_cols = [col for col in categorical_cols if col not in drop_cols]\n",
    "\n",
    "# Convert categorical columns\n",
    "for col in categorical_cols:\n",
    "    train_df[col] = train_df[col].astype('category')\n",
    "    test_df[col] = test_df[col].astype('category')\n",
    "\n",
    "X_train = train_df.drop(columns=drop_cols)\n",
    "y_train = train_df[target_label]\n",
    "\n",
    "X_test = test_df.drop(columns=drop_cols)\n",
    "y_test = test_df[target_label]\n",
    "\n",
    "test_df_ffa = X_test.copy()\n",
    "test_df_ffa['target'] = y_test\n",
    "\n",
    "# === Initialize FFA Explainer ===\n",
    "explainer, output_dir = setup_ffa_environment()\n",
    "\n",
    "print(\"Model and data loaded successfully\")\n",
    "print(\"FFA environment initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd6a0659-ef18-4eed-b12a-9a113b1dc02d",
   "metadata": {},
   "outputs": [],
   "source": [
    "check_explainer_rules(explainer, num_rules=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73105100-5e73-4d52-8882-8a413322b8d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_test.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a770ae76-8de2-46c5-9584-9f31c3e9a9e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Unique y_test values:\", y_test.unique())\n",
    "print(\"Value counts:\\n\", y_test.value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03f895fb-f9bd-4fa4-9606-7ef7b395417d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_df_ffa.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "239ae03c-daf9-4204-b2c9-4c1dc537005a",
   "metadata": {},
   "source": [
    "### 3. Apply FFA Rules to Test Dataset\n",
    "\n",
    "Next, we apply the extracted rules to our test dataset to generate AXP (Approximate Explanations) for each prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11e882d5-cf8b-4bcd-ab01-57e43d32d86b",
   "metadata": {},
   "source": [
    "### 4. Aggregate Rules for Feature Importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebcc28de-072c-4d2f-8729-a9d76671d09a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the analyzer\n",
    "analyzer = FFAAnalyzer(X_train, test_df_ffa, ed_non_opioid_model_cohort6, explainer)\n",
    "\n",
    "# Prepare data and calibrate model\n",
    "analyzer.prepare_data()\n",
    "optimal_threshold = analyzer.calibrate_model()\n",
    "\n",
    "# Get model metrics\n",
    "metrics = analyzer.calculate_metrics()\n",
    "\n",
    "# Analyze each class\n",
    "for target_class in [0, 1]:\n",
    "    # Get class analysis\n",
    "    class_analysis = analyzer.analyze_class_predictions(target_class)\n",
    "    \n",
    "    # Generate and save AXP explanations\n",
    "    df_axps = analyzer.generate_axp_explanations(target_class)\n",
    "    df_axps.to_csv(f\"symbolic_axps_class{target_class}.csv\", index=False)\n",
    "    \n",
    "    # Analyze and plot feature importance\n",
    "    df_norm = analyzer.analyze_feature_importance(df_axps)\n",
    "    analyzer.plot_feature_importance(df_norm, target_class)\n",
    "    \n",
    "    # Generate cattail plot\n",
    "    analyzer.plot_cattail(df_norm, target_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79bd397e-b457-43b4-8f32-ca8c51d99757",
   "metadata": {},
   "source": [
    "### 5. Causal Analysis\n",
    "\n",
    "Finally, we perform a causal analysis by systematically flipping features and measuring their impact on model predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58d76595-9d5f-4a34-84d1-1260397a281c",
   "metadata": {},
   "outputs": [],
   "source": [
    "causal_summary = analyzer.analyze_causal_effects(target_class=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "788dd5f7-01d6-4f47-9d09-5a266323a617",
   "metadata": {},
   "source": [
    "# G. Results\n",
    "\n",
    "The FFA analysis provides insights into:\n",
    "1. Which features are most frequently used in explaining model predictions\n",
    "2. How features causally impact the model's decisions\n",
    "3. The robustness of the model's predictions to feature perturbations\n",
    "\n",
    "These insights help us understand the model's decision-making process and identify how prescription drugs can cause or influence hospitalizations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9726c641-e80a-4405-b8a7-906bd5c03ff3",
   "metadata": {},
   "source": [
    "# S3 Sync"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c17d865-4b3f-4d24-8ea2-e54ee3abdcce",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "aws s3 sync . s3://pgx-repository/pgx-analysis/ \\\n",
    "  --delete \\\n",
    "  --exclude *checkpoint* \\\n",
    "  --exclude *.tmp \\\n",
    "  --exclude *.ipynb_checkpoints/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "667f628b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
