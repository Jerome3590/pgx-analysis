{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CatBoost Formal Feature Attribution Analysis\n",
    "\n",
    "This notebook processes CatBoost JSON model files to perform formal feature attribution analysis, including:\n",
    "\n",
    "- **Model Loading**: Load CatBoost models from JSON format\n",
    "- **Rule Extraction**: Extract decision rules from tree structures\n",
    "- **Anchored Explanations (AXP)**: Generate explanations for model predictions\n",
    "- **Feature Importance**: Calculate feature importance from explanations\n",
    "- **Causal Analysis**: Measure causal responsibility of features\n",
    "- **Visualization**: Create comprehensive visualizations and reports\n",
    "\n",
    "**\u00f0\u0178\u201c\u2013 Documentation**: See [`README_ffa_analysis.md`](README_ffa_analysis.md) for detailed documentation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Configuration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Optional, Tuple, Any\n",
    "from collections import defaultdict, Counter\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# CatBoost and ML libraries\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score, accuracy_score, precision_score, recall_score,\n",
    "    f1_score, log_loss, confusion_matrix, roc_curve, precision_recall_curve\n",
    ")\n",
    "\n",
    "# FFA Analysis modules\n",
    "try:\n",
    "    from ffa_analysis import (\n",
    "        validate_explainer_structure,\n",
    "        analyze_ctr_hash_maps,\n",
    "        print_json_key_structure\n",
    "    )\n",
    "    from catboost_axp_explainer import CatBoostAXPExplainer, PathConfig, AnalysisConfig\n",
    "except ImportError as e:\n",
    "    print(f\"Warning: Could not import FFA modules: {e}\")\n",
    "    print(\"Some functionality may be limited.\")\n",
    "\n",
    "# AWS S3 (if needed)\n",
    "try:\n",
    "    import boto3\n",
    "except ImportError:\n",
    "    print(\"Warning: boto3 not available. S3 functionality disabled.\")\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"\u00e2\u0153\u201c All libraries imported successfully\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CONFIGURATION\n",
    "# ============================================================\n",
    "\n",
    "# Model paths (update these to your actual paths)\n",
    "MODEL_CONFIG = {\n",
    "    'model_json_path': 'catboost_models/opioid_ed/25_44_2016/catboost_model.json',  # Local or S3 path\n",
    "    'model_cbm_path': 'catboost_models/opioid_ed/25_44_2016/model.cbm',  # Optional: CBM format\n",
    "    'model_info_json': 'catboost_models/opioid_ed/25_44_2016/model_info.json',  # Optional: metadata\n",
    "    'feature_importance_csv': 'catboost_models/opioid_ed/25_44_2016/feature_importance.csv'  # Optional\n",
    "}\n",
    "\n",
    "# Data paths (for explanations and causal analysis)\n",
    "DATA_CONFIG = {\n",
    "    'train_data_path': None,  # Optional: path to training data\n",
    "    'test_data_path': None,   # Optional: path to test data\n",
    "    'use_s3': False,          # Set to True if using S3 paths\n",
    "    's3_bucket': 'pgxdatalake',\n",
    "    's3_prefix': 'catboost_models/opioid_ed/age_band=25-44/event_year=2016'\n",
    "}\n",
    "\n",
    "# Analysis configuration\n",
    "ANALYSIS_CONFIG = {\n",
    "    'target_class': 1,              # Class to explain (1 for positive predictions)\n",
    "    'target_threshold': 0.5,        # Probability threshold for positive predictions\n",
    "    'top_k_features': 20,           # Number of top features to display\n",
    "    'min_coverage': 0.01,           # Minimum rule coverage\n",
    "    'n_permutations': 100,          # Number of permutations for causal analysis\n",
    "    'random_seed': 1997\n",
    "}\n",
    "\n",
    "# Output configuration\n",
    "OUTPUT_CONFIG = {\n",
    "    'output_dir': 'ffa_results',\n",
    "    'save_plots': True,\n",
    "    'save_results': True,\n",
    "    'plot_format': 'png',\n",
    "    'plot_dpi': 300\n",
    "}\n",
    "\n",
    "# Create output directory\n",
    "os.makedirs(OUTPUT_CONFIG['output_dir'], exist_ok=True)\n",
    "\n",
    "print(\"\u00e2\u0153\u201c Configuration loaded\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load CatBoost Model from JSON\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_catboost_json(model_json_path: str) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Load CatBoost model from JSON file.\n",
    "    \n",
    "    Args:\n",
    "        model_json_path: Path to the CatBoost JSON model file\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary containing model structure\n",
    "    \"\"\"\n",
    "    print(f\"Loading CatBoost model from: {model_json_path}\")\n",
    "    \n",
    "    if DATA_CONFIG['use_s3']:\n",
    "        # Load from S3\n",
    "        s3 = boto3.client('s3')\n",
    "        bucket = DATA_CONFIG['s3_bucket']\n",
    "        key = model_json_path.replace(f\"s3://{bucket}/\", \"\")\n",
    "        \n",
    "        # Download to temp file\n",
    "        import tempfile\n",
    "        with tempfile.NamedTemporaryFile(mode='w+', suffix='.json', delete=False) as tmp:\n",
    "            s3.download_fileobj(bucket, key, tmp)\n",
    "            tmp_path = tmp.name\n",
    "        \n",
    "        with open(tmp_path, 'r') as f:\n",
    "            model_json = json.load(f)\n",
    "        \n",
    "        os.unlink(tmp_path)\n",
    "    else:\n",
    "        # Load from local file\n",
    "        if not os.path.exists(model_json_path):\n",
    "            raise FileNotFoundError(f\"Model file not found: {model_json_path}\")\n",
    "        with open(model_json_path, 'r') as f:\n",
    "            model_json = json.load(f)\n",
    "    \n",
    "    print(f\"\u00e2\u0153\u201c Model loaded successfully\")\n",
    "    print(f\"  - Keys: {list(model_json.keys())}\")\n",
    "    \n",
    "    # Validate structure\n",
    "    required_keys = ['oblivious_trees', 'features_info']\n",
    "    for key in required_keys:\n",
    "        if key not in model_json:\n",
    "            raise ValueError(f\"Missing required key: {key}\")\n",
    "    \n",
    "    print(f\"  - Number of trees: {len(model_json.get('oblivious_trees', []))}\")\n",
    "    print(f\"  - Features info present: {bool(model_json.get('features_info'))}\")\n",
    "    print(f\"  - CTR data present: {bool(model_json.get('ctr_data'))}\")\n",
    "    \n",
    "    return model_json\n",
    "\n",
    "# Load the model\n",
    "model_json = load_catboost_json(MODEL_CONFIG['model_json_path'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect model structure\n",
    "def inspect_model_structure(model_json: Dict[str, Any]):\n",
    "    \"\"\"Inspect and print model structure details.\"\"\"\n",
    "    print(\"\\n=== Model Structure Inspection ===\\n\")\n",
    "    \n",
    "    # Trees\n",
    "    trees = model_json.get('oblivious_trees', [])\n",
    "    print(f\"Trees: {len(trees)} total\")\n",
    "    if trees:\n",
    "        first_tree = trees[0]\n",
    "        print(f\"  - First tree keys: {list(first_tree.keys())}\")\n",
    "        print(f\"  - First tree splits: {len(first_tree.get('splits', []))}\")\n",
    "        print(f\"  - First tree leaf values: {len(first_tree.get('leaf_values', []))}\")\n",
    "    \n",
    "    # Features info\n",
    "    features_info = model_json.get('features_info', {})\n",
    "    print(f\"\\nFeatures Info:\")\n",
    "    print(f\"  - Float features: {len(features_info.get('float_features', []))}\")\n",
    "    print(f\"  - Categorical features: {len(features_info.get('cat_features', []))}\")\n",
    "    \n",
    "    if features_info.get('float_features'):\n",
    "        float_feat = features_info['float_features'][0]\n",
    "        print(f\"  - First float feature keys: {list(float_feat.keys())}\")\n",
    "    \n",
    "    if features_info.get('cat_features'):\n",
    "        cat_feat = features_info['cat_features'][0]\n",
    "        print(f\"  - First cat feature keys: {list(cat_feat.keys())}\")\n",
    "    \n",
    "    # CTR data\n",
    "    ctr_data = model_json.get('ctr_data', {})\n",
    "    print(f\"\\nCTR Data:\")\n",
    "    print(f\"  - CTR entries: {len(ctr_data)}\")\n",
    "    if ctr_data:\n",
    "        first_ctr_key = list(ctr_data.keys())[0]\n",
    "        print(f\"  - First CTR key: {first_ctr_key[:100]}...\")\n",
    "        print(f\"  - First CTR value keys: {list(ctr_data[first_ctr_key].keys())}\")\n",
    "\n",
    "inspect_model_structure(model_json)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Extract Feature Information and CTR Mappings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_feature_mappings(model_json: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Extract feature name mappings from model JSON.\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with feature mappings\n",
    "    \"\"\"\n",
    "    features_info = model_json.get('features_info', {})\n",
    "    \n",
    "    # Float features\n",
    "    float_features = features_info.get('float_features', [])\n",
    "    float_idx_to_name = {}\n",
    "    float_borders = {}\n",
    "    \n",
    "    for feat in float_features:\n",
    "        feat_idx = feat.get('float_feature_index')\n",
    "        feat_name = feat.get('feature_name', f'float_feature_{feat_idx}')\n",
    "        borders = feat.get('borders', [])\n",
    "        float_idx_to_name[feat_idx] = feat_name\n",
    "        float_borders[feat_idx] = borders\n",
    "    \n",
    "    # Categorical features\n",
    "    cat_features = features_info.get('cat_features', [])\n",
    "    cat_idx_to_name = {}\n",
    "    \n",
    "    for feat in cat_features:\n",
    "        feat_idx = feat.get('cat_feature_index')\n",
    "        feat_name = feat.get('feature_name', f'cat_feature_{feat_idx}')\n",
    "        cat_idx_to_name[feat_idx] = feat_name\n",
    "    \n",
    "    # CTR mappings\n",
    "    ctr_data = model_json.get('ctr_data', {})\n",
    "    ctr_mappings = {}\n",
    "    \n",
    "    for ctr_key, ctr_value in ctr_data.items():\n",
    "        try:\n",
    "            ctr_info = json.loads(ctr_key)\n",
    "            if 'identifier' in ctr_info:\n",
    "                for identifier in ctr_info['identifier']:\n",
    "                    if 'cat_feature_index' in identifier:\n",
    "                        cat_idx = identifier['cat_feature_index']\n",
    "                        if cat_idx not in ctr_mappings:\n",
    "                            ctr_mappings[cat_idx] = {\n",
    "                                'hash_map': ctr_value.get('hash_map', []),\n",
    "                                'borders': ctr_value.get('borders', []),\n",
    "                                'feature_name': cat_idx_to_name.get(cat_idx, f'cat_feature_{cat_idx}')\n",
    "                            }\n",
    "        except (json.JSONDecodeError, KeyError):\n",
    "            continue\n",
    "    \n",
    "    mappings = {\n",
    "        'float_idx_to_name': float_idx_to_name,\n",
    "        'float_borders': float_borders,\n",
    "        'cat_idx_to_name': cat_idx_to_name,\n",
    "        'ctr_mappings': ctr_mappings\n",
    "    }\n",
    "    \n",
    "    print(f\"\u00e2\u0153\u201c Feature mappings extracted:\")\n",
    "    print(f\"  - Float features: {len(float_idx_to_name)}\")\n",
    "    print(f\"  - Categorical features: {len(cat_idx_to_name)}\")\n",
    "    print(f\"  - CTR mappings: {len(ctr_mappings)}\")\n",
    "    \n",
    "    return mappings\n",
    "\n",
    "# Extract feature mappings\n",
    "feature_mappings = extract_feature_mappings(model_json)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze CTR hash maps if present\n",
    "if model_json.get('ctr_data'):\n",
    "    try:\n",
    "        ctr_analysis = analyze_ctr_hash_maps(model_json['ctr_data'])\n",
    "        print(\"\\n=== CTR Hash Map Analysis ===\")\n",
    "        print(f\"Total entries: {ctr_analysis['total_entries']}\")\n",
    "        print(f\"Feature stats: {len(ctr_analysis['feature_stats'])} features analyzed\")\n",
    "    except Exception as e:\n",
    "        print(f\"\u00e2\u0161\u00a0 Could not analyze CTR hash maps: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Load Model Info and Feature Importance (if available)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model info JSON if available\n",
    "model_info = None\n",
    "feature_importance_df = None\n",
    "\n",
    "if MODEL_CONFIG.get('model_info_json') and os.path.exists(MODEL_CONFIG['model_info_json']):\n",
    "    with open(MODEL_CONFIG['model_info_json'], 'r') as f:\n",
    "        model_info = json.load(f)\n",
    "    print(f\"\u00e2\u0153\u201c Model info loaded\")\n",
    "    print(f\"  - Model type: {model_info.get('model_type')}\")\n",
    "    print(f\"  - Age band: {model_info.get('age_band')}\")\n",
    "    print(f\"  - Event year: {model_info.get('event_year')}\")\n",
    "    print(f\"  - Metrics: {model_info.get('metrics', {})}\")\n",
    "\n",
    "# Load feature importance CSV if available\n",
    "if MODEL_CONFIG.get('feature_importance_csv') and os.path.exists(MODEL_CONFIG['feature_importance_csv']):\n",
    "    feature_importance_df = pd.read_csv(MODEL_CONFIG['feature_importance_csv'])\n",
    "    print(f\"\\n\u00e2\u0153\u201c Feature importance loaded: {len(feature_importance_df)} features\")\n",
    "    print(f\"\\nTop 10 features:\")\n",
    "    print(feature_importance_df.head(10).to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Initialize FFA Explainer (if available)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize FFA Explainer if available\n",
    "explainer = None\n",
    "\n",
    "try:\n",
    "    # Initialize path configuration\n",
    "    path_config = PathConfig(\n",
    "        model_path=MODEL_CONFIG['model_json_path'],\n",
    "        data_dir=DATA_CONFIG.get('test_data_path', ''),\n",
    "        output_dir=OUTPUT_CONFIG['output_dir'],\n",
    "        tree_rules_path=None,\n",
    "        age_band=None\n",
    "    )\n",
    "    \n",
    "    # Initialize analysis configuration\n",
    "    analysis_config = AnalysisConfig(\n",
    "        top_k=ANALYSIS_CONFIG['top_k_features'],\n",
    "        min_coverage=ANALYSIS_CONFIG['min_coverage'],\n",
    "        significance_threshold=0.05,\n",
    "        n_permutations=ANALYSIS_CONFIG['n_permutations']\n",
    "    )\n",
    "    \n",
    "    # Initialize explainer\n",
    "    explainer = CatBoostAXPExplainer(path_config)\n",
    "    \n",
    "    # Load model into explainer\n",
    "    explainer.load_model_json(MODEL_CONFIG['model_json_path'])\n",
    "    \n",
    "    print(\"\u00e2\u0153\u201c FFA Explainer initialized\")\n",
    "    \n",
    "    # Validate explainer structure\n",
    "    if hasattr(explainer, 'feature_names'):\n",
    "        validation_passed = validate_explainer_structure(explainer)\n",
    "        if not validation_passed:\n",
    "            print(\"\\n\u00e2\u0161\u00a0 Warning: Explainer validation failed. Proceed with caution.\")\n",
    "except Exception as e:\n",
    "    print(f\"\u00e2\u0161\u00a0 Could not initialize FFA Explainer: {e}\")\n",
    "    print(\"Continuing with basic analysis...\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Load Test Data (if available)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test data if available\n",
    "X_test = None\n",
    "y_test = None\n",
    "\n",
    "if DATA_CONFIG.get('test_data_path') and os.path.exists(DATA_CONFIG['test_data_path']):\n",
    "    print(f\"Loading test data from: {DATA_CONFIG['test_data_path']}\")\n",
    "    \n",
    "    # Try to load as CSV or Parquet\n",
    "    if DATA_CONFIG['test_data_path'].endswith('.parquet'):\n",
    "        test_data = pd.read_parquet(DATA_CONFIG['test_data_path'])\n",
    "    else:\n",
    "        test_data = pd.read_csv(DATA_CONFIG['test_data_path'])\n",
    "    \n",
    "    # Separate features and target\n",
    "    if 'target' in test_data.columns:\n",
    "        y_test = test_data['target'].values\n",
    "        X_test = test_data.drop('target', axis=1)\n",
    "    elif 'is_target_case' in test_data.columns:\n",
    "        y_test = test_data['is_target_case'].values\n",
    "        X_test = test_data.drop('is_target_case', axis=1)\n",
    "    else:\n",
    "        print(\"\u00e2\u0161\u00a0 No target column found. Using all columns as features.\")\n",
    "        X_test = test_data\n",
    "        y_test = None\n",
    "    \n",
    "    print(f\"\u00e2\u0153\u201c Test data loaded: {X_test.shape[0]} samples, {X_test.shape[1]} features\")\n",
    "    if y_test is not None:\n",
    "        print(f\"  - Target distribution: {Counter(y_test)}\")\n",
    "else:\n",
    "    print(\"\u00e2\u0161\u00a0 Test data not available. Skipping data-dependent analyses.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "me"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explanations = []\n",
    "\n",
    "if X_test is not None and y_test is not None and explainer is not None:\n",
    "    print(\"\\n=== Generating Anchored Explanations (AXP) ===\\n\")\n",
    "    \n",
    "    try:\n",
    "        # Generate explanations for target class\n",
    "        explanations = explainer.explain_dataset(\n",
    "            X_test,\n",
    "            y_test,\n",
    "            target_class=ANALYSIS_CONFIG['target_class'],\n",
    "            strategy='AXP'\n",
    "        )\n",
    "        \n",
    "        print(f\"\u00e2\u0153\u201c Generated {len(explanations)} explanations\")\n",
    "        \n",
    "        # Analyze unmatched instances\n",
    "        if hasattr(explainer, 'unmatched'):\n",
    "            unmatched_count = len(explainer.unmatched)\n",
    "            print(f\"  - Matched instances: {len(explanations) - unmatched_count}\")\n",
    "            print(f\"  - Unmatched instances: {unmatched_count}\")\n",
    "            if unmatched_count > 0:\n",
    "                unmatched_pct = (unmatched_count / len(explanations)) * 100\n",
    "                print(f\"  - Unmatched percentage: {unmatched_pct:.2f}%\")\n",
    "    except Exception as e:\n",
    "        print(f\"\u00e2\u0161\u00a0 Error generating explanations: {e}\")\n",
    "        explanations = []\n",
    "else:\n",
    "    print(\"\u00e2\u0161\u00a0 Test data or explainer not available. Skipping AXP generation.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Calculate Feature Importance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_feature_importance_from_explanations(explanations: List[Dict], \n",
    "                                                   top_k: int = 20) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Calculate feature importance from explanations.\n",
    "    \n",
    "    Args:\n",
    "        explanations: List of explanation dictionaries\n",
    "        top_k: Number of top features to return\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with feature importance scores\n",
    "    \"\"\"\n",
    "    feature_counts = Counter()\n",
    "    \n",
    "    for explanation in explanations:\n",
    "        if 'conditions' in explanation:\n",
    "            for condition in explanation['conditions']:\n",
    "                if 'feature_name' in condition:\n",
    "                    feature_counts[condition['feature_name']] += 1\n",
    "                elif 'feature' in condition:\n",
    "                    feature_counts[condition['feature']] += 1\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    if not feature_counts:\n",
    "        return pd.DataFrame(columns=['feature', 'count', 'importance'])\n",
    "    \n",
    "    importance_df = pd.DataFrame([\n",
    "        {'feature': feat, 'count': count, 'importance': count / len(explanations)}\n",
    "        for feat, count in feature_counts.most_common(top_k)\n",
    "    ])\n",
    "    \n",
    "    return importance_df\n",
    "\n",
    "# Calculate feature importance\n",
    "if explanations:\n",
    "    feature_importance_axp = calculate_feature_importance_from_explanations(\n",
    "        explanations,\n",
    "        top_k=ANALYSIS_CONFIG['top_k_features']\n",
    "    )\n",
    "    \n",
    "    print(\"\\n=== Feature Importance from AXP Explanations ===\\n\")\n",
    "    print(feature_importance_axp.to_string(index=False))\n",
    "else:\n",
    "    print(\"\u00e2\u0161\u00a0 No explanations available. Using model's built-in feature importance.\")\n",
    "    if feature_importance_df is not None:\n",
    "        feature_importance_axp = feature_importance_df.head(ANALYSIS_CONFIG['top_k_features']).copy()\n",
    "        if 'importance' not in feature_importance_axp.columns:\n",
    "            # Use first numeric column as importance\n",
    "            numeric_cols = feature_importance_axp.select_dtypes(include=[np.number]).columns\n",
    "            if len(numeric_cols) > 0:\n",
    "                feature_importance_axp['importance'] = feature_importance_axp[numeric_cols[0]]\n",
    "    else:\n",
    "        feature_importance_axp = None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Causal Analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_causal_importance(X_test: pd.DataFrame,\n",
    "                               explanations: List[Dict],\n",
    "                               model: CatBoostClassifier,\n",
    "                               n_permutations: int = 100) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Calculate causal importance by measuring prediction changes when features are modified.\n",
    "    \n",
    "    Args:\n",
    "        X_test: Test features\n",
    "        explanations: List of explanations\n",
    "        model: Trained CatBoost model\n",
    "        n_permutations: Number of permutations for causal analysis\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with causal importance scores\n",
    "    \"\"\"\n",
    "    print(\"\\n=== Calculating Causal Importance ===\\n\")\n",
    "    \n",
    "    feature_causal_scores = defaultdict(list)\n",
    "    \n",
    "    # Get baseline predictions\n",
    "    baseline_probs = model.predict_proba(X_test)[:, ANALYSIS_CONFIG['target_class']]\n",
    "    \n",
    "    # For each feature mentioned in explanations\n",
    "    all_features = set()\n",
    "    for explanation in explanations:\n",
    "        if 'conditions' in explanation:\n",
    "            for condition in explanation['conditions']:\n",
    "                feat_name = condition.get('feature_name') or condition.get('feature')\n",
    "                if feat_name:\n",
    "                    all_features.add(feat_name)\n",
    "    \n",
    "    print(f\"Analyzing {len(all_features)} features...\")\n",
    "    \n",
    "    for feat_name in list(all_features)[:ANALYSIS_CONFIG['top_k_features']]:\n",
    "        if feat_name not in X_test.columns:\n",
    "            continue\n",
    "        \n",
    "        # Create counterfactual by modifying this feature\n",
    "        X_counterfactual = X_test.copy()\n",
    "        \n",
    "        # For numerical features: add/subtract standard deviation\n",
    "        if X_test[feat_name].dtype in ['float64', 'int64']:\n",
    "            std_dev = X_test[feat_name].std()\n",
    "            if std_dev > 0:\n",
    "                X_counterfactual[feat_name] = X_test[feat_name] + std_dev\n",
    "            else:\n",
    "                X_counterfactual[feat_name] = X_test[feat_name] + 1\n",
    "        # For binary features: flip values\n",
    "        elif X_test[feat_name].dtype == 'bool' or X_test[feat_name].nunique() == 2:\n",
    "            X_counterfactual[feat_name] = 1 - X_test[feat_name]\n",
    "        \n",
    "        # Calculate new predictions\n",
    "        counterfactual_probs = model.predict_proba(X_counterfactual)[:, ANALYSIS_CONFIG['target_class']]\n",
    "        \n",
    "        # Calculate average change in probability\n",
    "        prob_change = np.mean(np.abs(counterfactual_probs - baseline_probs))\n",
    "        feature_causal_scores[feat_name].append(prob_change)\n",
    "    \n",
    "    # Aggregate scores\n",
    "    if not feature_causal_scores:\n",
    "        return pd.DataFrame(columns=['feature', 'causal_importance', 'std'])\n",
    "    \n",
    "    causal_importance_df = pd.DataFrame([\n",
    "        {\n",
    "            'feature': feat,\n",
    "            'causal_importance': np.mean(scores),\n",
    "            'std': np.std(scores)\n",
    "        }\n",
    "        for feat, scores in feature_causal_scores.items()\n",
    "    ]).sort_values('causal_importance', ascending=False)\n",
    "    \n",
    "    return causal_importance_df\n",
    "\n",
    "# Calculate causal importance if model and data are available\n",
    "causal_importance_df = None\n",
    "\n",
    "if X_test is not None and explanations and MODEL_CONFIG.get('model_cbm_path'):\n",
    "    # Load CatBoost model\n",
    "    if os.path.exists(MODEL_CONFIG['model_cbm_path']):\n",
    "        try:\n",
    "            model = CatBoostClassifier()\n",
    "            model.load_model(MODEL_CONFIG['model_cbm_path'])\n",
    "            \n",
    "            causal_importance_df = calculate_causal_importance(\n",
    "                X_test,\n",
    "                explanations,\n",
    "                model,\n",
    "                n_permutations=ANALYSIS_CONFIG['n_permutations']\n",
    "            )\n",
    "            \n",
    "            print(\"\\n=== Causal Importance Results ===\\n\")\n",
    "            print(causal_importance_df.head(20).to_string(index=False))\n",
    "        except Exception as e:\n",
    "            print(f\"\u00e2\u0161\u00a0 Error in causal analysis: {e}\")\n",
    "    else:\n",
    "        print(\"\u00e2\u0161\u00a0 Model CBM file not found. Skipping causal analysis.\")\n",
    "else:\n",
    "    print(\"\u00e2\u0161\u00a0 Test data or explanations not available. Skipping causal analysis.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Visualizations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Importance Visualization\n",
    "if feature_importance_axp is not None and len(feature_importance_axp) > 0:\n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "    \n",
    "    top_features = feature_importance_axp.head(ANALYSIS_CONFIG['top_k_features'])\n",
    "    \n",
    "    ax.barh(range(len(top_features)), top_features['importance'].values)\n",
    "    ax.set_yticks(range(len(top_features)))\n",
    "    ax.set_yticklabels(top_features['feature'].values)\n",
    "    ax.set_xlabel('Importance Score', fontsize=12)\n",
    "    ax.set_ylabel('Feature', fontsize=12)\n",
    "    ax.set_title('Feature Importance from AXP Explanations', fontsize=14, fontweight='bold')\n",
    "    ax.invert_yaxis()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if OUTPUT_CONFIG['save_plots']:\n",
    "        plot_path = os.path.join(OUTPUT_CONFIG['output_dir'], 'feature_importance_axp.png')\n",
    "        plt.savefig(plot_path, dpi=OUTPUT_CONFIG['plot_dpi'], bbox_inches='tight')\n",
    "        print(f\"\u00e2\u0153\u201c Saved plot to: {plot_path}\")\n",
    "    \n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"\u00e2\u0161\u00a0 No feature importance data available for visualization.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Causal Importance Visualization\n",
    "if causal_importance_df is not None and len(causal_importance_df) > 0:\n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "    \n",
    "    top_causal = causal_importance_df.head(ANALYSIS_CONFIG['top_k_features'])\n",
    "    \n",
    "    x_pos = range(len(top_causal))\n",
    "    ax.barh(x_pos, top_causal['causal_importance'].values, \n",
    "            xerr=top_causal['std'].values, capsize=5)\n",
    "    ax.set_yticks(x_pos)\n",
    "    ax.set_yticklabels(top_causal['feature'].values)\n",
    "    ax.set_xlabel('Causal Importance Score', fontsize=12)\n",
    "    ax.set_ylabel('Feature', fontsize=12)\n",
    "    ax.set_title('Causal Feature Importance', fontsize=14, fontweight='bold')\n",
    "    ax.invert_yaxis()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if OUTPUT_CONFIG['save_plots']:\n",
    "        plot_path = os.path.join(OUTPUT_CONFIG['output_dir'], 'causal_importance.png')\n",
    "        plt.savefig(plot_path, dpi=OUTPUT_CONFIG['plot_dpi'], bbox_inches='tight')\n",
    "        print(f\"\u00e2\u0153\u201c Saved plot to: {plot_path}\")\n",
    "    \n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"\u00e2\u0161\u00a0 No causal importance data available for visualization.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Save Results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if OUTPUT_CONFIG['save_results']:\n",
    "    print(\"\\n=== Saving Results ===\\n\")\n",
    "    \n",
    "    # Save feature importance\n",
    "    if feature_importance_axp is not None:\n",
    "        output_path = os.path.join(OUTPUT_CONFIG['output_dir'], 'feature_importance_axp.csv')\n",
    "        feature_importance_axp.to_csv(output_path, index=False)\n",
    "        print(f\"\u00e2\u0153\u201c Saved feature importance to: {output_path}\")\n",
    "    \n",
    "    # Save causal importance\n",
    "    if causal_importance_df is not None:\n",
    "        output_path = os.path.join(OUTPUT_CONFIG['output_dir'], 'causal_importance.csv')\n",
    "        causal_importance_df.to_csv(output_path, index=False)\n",
    "        print(f\"\u00e2\u0153\u201c Saved causal importance to: {output_path}\")\n",
    "    \n",
    "    # Save explanations summary\n",
    "    if explanations:\n",
    "        explanations_summary = pd.DataFrame([\n",
    "            {\n",
    "                'instance_id': i,\n",
    "                'num_conditions': len(exp.get('conditions', [])),\n",
    "                'prediction': exp.get('prediction', 'N/A'),\n",
    "                'rule_id': exp.get('rule_id', 'N/A')\n",
    "            }\n",
    "            for i, exp in enumerate(explanations)\n",
    "        ])\n",
    "        \n",
    "        output_path = os.path.join(OUTPUT_CONFIG['output_dir'], 'explanations_summary.csv')\n",
    "        explanations_summary.to_csv(output_path, index=False)\n",
    "        print(f\"\u00e2\u0153\u201c Saved explanations summary to: {output_path}\")\n",
    "    \n",
    "    # Save model info\n",
    "    if model_info:\n",
    "        output_path = os.path.join(OUTPUT_CONFIG['output_dir'], 'model_info.json')\n",
    "        with open(output_path, 'w') as f:\n",
    "            json.dump(model_info, f, indent=2)\n",
    "        print(f\"\u00e2\u0153\u201c Saved model info to: {output_path}\")\n",
    "    \n",
    "    print(f\"\\n\u00e2\u0153\u201c All results saved to: {OUTPUT_CONFIG['output_dir']}\")\n",
    "else:\n",
    "    print(\"\u00e2\u0161\u00a0 Results saving disabled in configuration.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Summary Report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FORMAL FEATURE ATTRIBUTION ANALYSIS SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "if model_info:\n",
    "    print(f\"\\nModel Information:\")\n",
    "    print(f\"  - Type: {model_info.get('model_type', 'N/A')}\")\n",
    "    print(f\"  - Age Band: {model_info.get('age_band', 'N/A')}\")\n",
    "    print(f\"  - Event Year: {model_info.get('event_year', 'N/A')}\")\n",
    "    if 'metrics' in model_info:\n",
    "        print(f\"  - Metrics: {model_info['metrics']}\")\n",
    "\n",
    "print(f\"\\nModel Structure:\")\n",
    "print(f\"  - Number of trees: {len(model_json.get('oblivious_trees', []))}\")\n",
    "print(f\"  - Float features: {len(feature_mappings['float_idx_to_name'])}\")\n",
    "print(f\"  - Categorical features: {len(feature_mappings['cat_idx_to_name'])}\")\n",
    "print(f\"  - CTR mappings: {len(feature_mappings['ctr_mappings'])}\")\n",
    "\n",
    "if X_test is not None:\n",
    "    print(f\"\\nTest Data:\")\n",
    "    print(f\"  - Samples: {X_test.shape[0]}\")\n",
    "    print(f\"  - Features: {X_test.shape[1]}\")\n",
    "    if y_test is not None:\n",
    "        print(f\"  - Target distribution: {Counter(y_test)}\")\n",
    "\n",
    "if explanations:\n",
    "    print(f\"\\nExplanations:\")\n",
    "    print(f\"  - Total explanations: {len(explanations)}\")\n",
    "    if hasattr(explainer, 'unmatched') and explainer:\n",
    "        print(f\"  - Unmatched instances: {len(explainer.unmatched)}\")\n",
    "\n",
    "if feature_importance_axp is not None and len(feature_importance_axp) > 0:\n",
    "    print(f\"\\nTop 5 Features (AXP Importance):\")\n",
    "    for idx, row in feature_importance_axp.head(5).iterrows():\n",
    "        print(f\"  {idx+1}. {row['feature']}: {row['importance']:.4f}\")\n",
    "\n",
    "if causal_importance_df is not None and len(causal_importance_df) > 0:\n",
    "    print(f\"\\nTop 5 Features (Causal Importance):\")\n",
    "    for idx, row in causal_importance_df.head(5).iterrows():\n",
    "        print(f\"  {idx+1}. {row['feature']}: {row['causal_importance']:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Analysis Complete!\")\n",
    "print(\"=\"*80)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.9",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}