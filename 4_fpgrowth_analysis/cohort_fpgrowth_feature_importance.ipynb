{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cohort-Specific FPGrowth Feature Importance Analysis\n",
    "\n",
    "**Purpose:** Cohort-level frequent pattern mining for process mining and comparative analysis  \n",
    "**Updated:** November 23, 2025  \n",
    "**Hardware:** Optimized for EC2 (32 cores, 1TB RAM)  \n",
    "**Output:** `s3://pgxdatalake/gold/fpgrowth/cohort/{item_type}/cohort_name={cohort}/...`\n",
    "\n",
    "Key Features\n",
    "\n",
    "‚úÖ **Three Item Types** - Drugs, ICD codes, CPT codes  \n",
    "‚úÖ **Cohort-Specific Patterns** - Discovers patterns unique to each cohort  \n",
    "‚úÖ **Parallel Processing** - Processes multiple cohorts simultaneously  \n",
    "‚úÖ **BupaR Integration** - Outputs ready for process mining workflows  \n",
    "‚úÖ **Comparative Analysis** - Compare patterns between OPIOID_ED and ED_NON_OPIOID cohorts\n",
    "\n",
    "Methodology\n",
    "\n",
    "For each combination of (cohort, age_band, event_year, item_type):\n",
    "1. Extract items from cohort-specific data\n",
    "2. Create patient-level transactions\n",
    "3. Encode transactions into binary matrix\n",
    "4. Run FP-Growth to find frequent itemsets\n",
    "5. Generate association rules\n",
    "6. Save results to S3 in organized structure\n",
    "\n",
    "Key Differences from Global Analysis\n",
    "\n",
    "| Aspect | Global FPGrowth | Cohort FPGrowth |\n",
    "|--------|-----------------|-----------------|\n",
    "| **Scope** | All patients (~5.7M) | Individual cohorts (~10K-100K) |\n",
    "| **Purpose** | Universal ML features | Process mining patterns |\n",
    "| **Support Threshold** | 0.01 (1%) | 0.05 (5%) |\n",
    "| **Output** | `global/{item_type}/` | `cohort/{item_type}/cohort_name={c}/...` |\n",
    "| **Use Case** | CatBoost consistency | BupaR pathway analysis |\n",
    "| **Parallelization** | Sequential by item type | Parallel by cohort |\n",
    "\n",
    "Expected Runtime (EC2: 32 cores, 1TB RAM)\n",
    "\n",
    "- **Cohorts**: 2 (opioid_ed, ed_non_opioid)\n",
    "- **Age bands √ó Years**: ~100 combinations per cohort\n",
    "- **Item types**: 3 (drug_name, icd_code, cpt_code)\n",
    "- **Total jobs**: ~600 combinations\n",
    "- **Avg time per job**: ~1-2 minutes\n",
    "- **Total runtime**: ~2-4 hours (with MAX_WORKERS=4)\n",
    "\n",
    "S3 Output Structure\n",
    "\n",
    "```\n",
    "s3://pgxdatalake/gold/fpgrowth/cohort/\n",
    "‚îú‚îÄ‚îÄ drug_name/\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ cohort_name=opioid_ed/\n",
    "‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ age_band=65-74/event_year=2020/\n",
    "‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ itemsets.json\n",
    "‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ rules.json\n",
    "‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ summary.json\n",
    "‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ ...\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ cohort_name=ed_non_opioid/...\n",
    "‚îú‚îÄ‚îÄ icd_code/\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ (same structure)\n",
    "‚îî‚îÄ‚îÄ cpt_code/\n",
    "    ‚îî‚îÄ‚îÄ (same structure)\n",
    "```\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Setup and Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Project root: /home/pgx3874/pgx-analysis\n",
      "‚úì All imports successful\n",
      "‚úì Timestamp: 2025-11-24 12:27:05\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import logging\n",
    "from pathlib import Path\n",
    "import psutil\n",
    "import duckdb\n",
    "import time\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "\n",
    "# MLxtend for FP-Growth\n",
    "from mlxtend.frequent_patterns import fpgrowth, association_rules\n",
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "\n",
    "# Project root\n",
    "project_root = Path.cwd().parent if Path.cwd().name == '3_fpgrowth_analysis' else Path.cwd()\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "# Project utilities\n",
    "from helpers_1997_13.common_imports import s3_client, S3_BUCKET\n",
    "from helpers_1997_13.duckdb_utils import get_duckdb_connection\n",
    "from helpers_1997_13.s3_utils import save_to_s3_json, save_to_s3_parquet, get_cohort_parquet_path\n",
    "from helpers_1997_13.fpgrowth_utils import run_fpgrowth_drug_token_with_fallback, convert_frozensets\n",
    "from helpers_1997_13.visualization_utils import create_network_visualization\n",
    "from helpers_1997_13.constants import AGE_BANDS, EVENT_YEARS\n",
    "\n",
    "print(f\"‚úì Project root: {project_root}\")\n",
    "print(f\"‚úì All imports successful\")\n",
    "print(f\"‚úì Timestamp: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EC2 Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Min Support (drug/ICD): 0.05 (5%)\n",
      "‚úì Min Support (CPT): 0.15 (15% - focuses on common patterns)\n",
      "‚úì Min Confidence (drug/ICD): 0.5 (50% - strong associations)\n",
      "‚úì Min Confidence (CPT): 0.6 (60% - very strong associations)\n",
      "‚úì Max Rules per Cohort: 1,000 (top rules by lift)\n",
      "‚úì Item Types: ['drug_name', 'icd_code', 'cpt_code']\n",
      "‚úì Max Workers: 1 (sequential - prevents OOM)\n",
      "‚úì DRY RUN MODE: Processing only 5 cohort combinations\n",
      "  ‚Üí Set DRY_RUN = False to process all cohorts\n",
      "‚úì Cohorts: ['opioid_ed', 'ed_non_opioid']\n",
      "‚úì S3 Output: s3://pgxdatalake/gold/fpgrowth/cohort\n",
      "‚úì S3 Retry: 3 attempts with exponential backoff\n",
      "‚úì Local Data: /mnt/nvme/cohorts\n",
      "‚úì Local Data Exists: True\n",
      "‚úì Detailed logs ‚Üí /home/pgx3874/pgx-analysis/3_fpgrowth_analysis/cohort_fpgrowth_execution.log\n",
      "‚úì Console output: WARNING level only (check log file for progress)\n",
      "\n",
      "üéØ Quality Over Quantity Approach:\n",
      "  - High confidence thresholds (50-60%) = meaningful patterns only\n",
      "  - CPT uses 15% support (vs 5%) = focuses on common procedures\n",
      "  - Top 1,000 rules by lift = actionable insights, not exhaustive lists\n",
      "  - 2 parallel workers = stable memory usage\n",
      "\n",
      "üéØ TARGET-FOCUSED RULE MINING: ENABLED\n",
      "  - Target ICD codes: ['F11.20', 'F11.21', 'F11.22', 'F11.23', 'F11.24', 'F11.25', 'F11.29']\n",
      "  - Target HCG lines (ED visits): ['P51 - ER Visits and Observation Care', 'O11 - Emergency Room', 'P33 - Urgent Care Visits']\n",
      "  - Only generates rules that PREDICT target outcomes\n",
      "  - Example: {Metoprolol, Gabapentin} ‚Üí {TARGET_ICD:OPIOID_DEPENDENCE}\n",
      "  - Example: {99213: Office Visit, J0670: Morphine} ‚Üí {TARGET_ED:EMERGENCY_DEPT}\n",
      "  ‚úÖ Drastically reduces rule count (only predictive patterns)\n",
      "  ‚úÖ More actionable for BupaR (pathways to target)\n",
      "  ‚úÖ Better for CatBoost (features that predict outcome)\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# EC2 CONFIGURATION (32 cores, 1TB RAM)\n",
    "# =============================================================================\n",
    "\n",
    "# FP-Growth parameters (higher threshold for cohort-specific patterns)\n",
    "MIN_SUPPORT = 0.05       # 5% support (items must appear in 5% of patients within cohort)\n",
    "MIN_CONFIDENCE = 0.5     # 50% confidence - only strong associations\n",
    "\n",
    "# CPT-specific parameters (prevent memory exhaustion from millions of rules)\n",
    "MIN_SUPPORT_CPT = 0.15   # 15% support for CPT codes (focuses on common patterns)\n",
    "MIN_CONFIDENCE_CPT = 0.6 # 60% confidence for CPT (very strong associations only)\n",
    "\n",
    "# Rule limits (focus on most important rules)\n",
    "MAX_RULES_PER_COHORT = 1000  # Keep top 1000 rules by lift (practical limit)\n",
    "\n",
    "# Itemset filtering (remove common/trivial itemsets)\n",
    "MIN_ITEMSET_LIFT = 1.1  # Filter itemsets with lift < 1.1 (items are independent/not interesting)\n",
    "\n",
    "# Target-focused rule mining (NEW!)\n",
    "TARGET_FOCUSED = True  # Only generate rules that predict target outcomes\n",
    "TARGET_ICD_CODES = ['F11.20', 'F11.21', 'F11.22', 'F11.23', 'F11.24', 'F11.25', 'F11.29']  # Opioid dependence codes\n",
    "TARGET_HCG_LINES = [\n",
    "    \"P51 - ER Visits and Observation Care\",\n",
    "    \"O11 - Emergency Room\",\n",
    "    \"P33 - Urgent Care Visits\"\n",
    "]  # ED visits (HCG Line codes - matches phase2_event_processing.py)\n",
    "TARGET_PREFIXES = ['TARGET_ICD:', 'TARGET_ED:']  # Prefixes for target items in transactions\n",
    "\n",
    "# Item types to process\n",
    "# NOTE: drug_name processed separately (pharmacy events)\n",
    "#       icd_code + cpt_code combined as 'medical_codes' (both from medical events)\n",
    "ITEM_TYPES = ['drug_name', 'medical_codes']  # Changed: combine ICD + CPT into medical_codes\n",
    "\n",
    "# Processing parameters\n",
    "MAX_WORKERS = 1  # Sequential processing to prevent memory issues\n",
    "\n",
    "# DEBUG MODE (enable detailed logging)\n",
    "DEBUG_MODE = True  # Set to True for DEBUG level logging, False for INFO level\n",
    "\n",
    "# DRY RUN MODE (test with limited cohorts first)\n",
    "DRY_RUN = True  # Set to False to process all cohorts\n",
    "DRY_RUN_LIMIT = 5  # Number of cohort combinations to process in dry run\n",
    "COHORTS_TO_PROCESS = ['opioid_ed', 'non_opioid_ed']  # Specify cohorts to process\n",
    "\n",
    "# Paths\n",
    "S3_OUTPUT_BASE = f\"s3://{S3_BUCKET}/gold/fpgrowth/cohort\"\n",
    "LOCAL_DATA_PATH = Path(\"/mnt/nvme/cohorts\")  # Instance storage (NVMe SSD for fast I/O)\n",
    "\n",
    "# Setup logger with file output (prevents Jupyter rate limit issues)\n",
    "logger = logging.getLogger('cohort_fpgrowth')\n",
    "\n",
    "# Set logger level based on DEBUG_MODE\n",
    "if DEBUG_MODE:\n",
    "    logger.setLevel(logging.DEBUG)\n",
    "    log_level_str = \"DEBUG\"\n",
    "else:\n",
    "    logger.setLevel(logging.INFO)\n",
    "    log_level_str = \"INFO\"\n",
    "\n",
    "logger.handlers.clear()  # Clear any existing handlers\n",
    "\n",
    "# File handler - full logs to file (always captures DEBUG if DEBUG_MODE is on)\n",
    "log_file = project_root / \"3_fpgrowth_analysis\" / \"logs\" / \"cohort_fpgrowth_execution.log\"\n",
    "log_file.parent.mkdir(exist_ok=True)  # Create logs directory if it doesn't exist\n",
    "file_handler = logging.FileHandler(log_file)\n",
    "file_handler.setLevel(logging.DEBUG if DEBUG_MODE else logging.INFO)  # Capture DEBUG in debug mode\n",
    "file_handler.setFormatter(logging.Formatter('%(asctime)s - %(levelname)s - %(name)s - %(message)s'))\n",
    "logger.addHandler(file_handler)\n",
    "\n",
    "# Console handler - only major milestones (prevents Jupyter rate limit)\n",
    "console_handler = logging.StreamHandler()\n",
    "if DEBUG_MODE:\n",
    "    console_handler.setLevel(logging.DEBUG)  # Show DEBUG in console too when in debug mode\n",
    "else:\n",
    "    console_handler.setLevel(logging.WARNING)  # Only warnings/errors to console\n",
    "console_handler.setFormatter(logging.Formatter('%(asctime)s - %(levelname)s - %(message)s'))\n",
    "logger.addHandler(console_handler)\n",
    "\n",
    "print(f\"‚úì Min Support (drug_name): {MIN_SUPPORT} (5%)\")\n",
    "print(f\"‚úì Min Support (medical_codes): {MIN_SUPPORT_CPT} (15% - ICD+CPT combined)\")\n",
    "print(f\"‚úì Min Confidence (drug_name): {MIN_CONFIDENCE} (50% - strong associations)\")\n",
    "print(f\"‚úì Min Confidence (medical_codes): {MIN_CONFIDENCE_CPT} (60% - very strong associations)\")\n",
    "print(f\"‚úì Max Rules per Cohort: {MAX_RULES_PER_COHORT:,} (top rules by lift)\")\n",
    "print(f\"‚úì Min Itemset Lift: {MIN_ITEMSET_LIFT} (filters common/trivial itemsets)\")\n",
    "print(f\"‚úì Item Types: {ITEM_TYPES}\")\n",
    "print(f\"‚úì Max Workers: {MAX_WORKERS} (sequential - prevents OOM)\")\n",
    "if DRY_RUN:\n",
    "    print(f\"‚úì DRY RUN MODE: Processing only {DRY_RUN_LIMIT} cohort combinations\")\n",
    "    print(f\"  ‚Üí Set DRY_RUN = False to process all cohorts\")\n",
    "else:\n",
    "    print(f\"‚úì FULL RUN MODE: Processing all cohorts\")\n",
    "print(f\"‚úì Cohorts: {COHORTS_TO_PROCESS}\")\n",
    "print(f\"‚úì S3 Output: {S3_OUTPUT_BASE}\")\n",
    "print(f\"‚úì S3 Retry: 3 attempts with exponential backoff\")\n",
    "print(f\"‚úì Local Data: {LOCAL_DATA_PATH}\")\n",
    "print(f\"‚úì Local Data Exists: {LOCAL_DATA_PATH.exists()}\")\n",
    "print(f\"‚úì Logging Level: {log_level_str}\")\n",
    "print(f\"‚úì DEBUG Mode: {'ENABLED' if DEBUG_MODE else 'DISABLED'}\")\n",
    "print(f\"‚úì Detailed logs ‚Üí {log_file}\")\n",
    "print(f\"‚úì Log directory: {log_file.parent}\")\n",
    "print(f\"‚úì Log file exists: {log_file.exists()}\")\n",
    "if DEBUG_MODE:\n",
    "    print(f\"‚úì Console output: DEBUG level (all messages)\")\n",
    "else:\n",
    "    print(f\"‚úì Console output: WARNING level only (check log file for progress)\")\n",
    "print(\"\\nüéØ Quality Over Quantity Approach:\")\n",
    "print(\"  - High confidence thresholds (50-60%) = meaningful patterns only\")\n",
    "print(\"  - CPT uses 15% support (vs 5%) = focuses on common procedures\")\n",
    "print(\"  - Top 1,000 rules by lift = actionable insights, not exhaustive lists\")\n",
    "print(\"  - 2 parallel workers = stable memory usage\")\n",
    "print(f\"\\nüéØ TARGET-FOCUSED RULE MINING: {'ENABLED' if TARGET_FOCUSED else 'DISABLED'}\")\n",
    "if TARGET_FOCUSED:\n",
    "    print(f\"  - Target ICD codes: {TARGET_ICD_CODES}\")\n",
    "    print(f\"  - Target HCG lines (ED visits): {TARGET_HCG_LINES}\")\n",
    "    print(\"  - Only generates rules that PREDICT target outcomes\")\n",
    "    print(\"  - Example: {Metoprolol, Gabapentin} ‚Üí {TARGET_ICD:OPIOID_DEPENDENCE}\")\n",
    "    print(\"  - Example: {99213: Office Visit, J0670: Morphine} ‚Üí {TARGET_ED:EMERGENCY_DEPT}\")\n",
    "    print(\"  ‚úÖ Drastically reduces rule count (only predictive patterns)\")\n",
    "    print(\"  ‚úÖ More actionable for BupaR (pathways to target)\")\n",
    "    print(\"  ‚úÖ Better for CatBoost (features that predict outcome)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Memory Monitoring\n",
    "\n",
    "Helper function to log memory usage at critical points.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diagnostic: Check what cohorts actually exist\n",
    "print(\"üîç Checking available cohorts...\")\n",
    "available_cohorts = set()\n",
    "for cohort_dir in LOCAL_DATA_PATH.glob(\"cohort_name=*\"):\n",
    "    cohort_name = cohort_dir.name.replace(\"cohort_name=\", \"\")\n",
    "    available_cohorts.add(cohort_name)\n",
    "    print(f\"  Found: {cohort_name}\")\n",
    "\n",
    "print(f\"\\nüìã Summary:\")\n",
    "print(f\"  Available: {sorted(available_cohorts)}\")\n",
    "print(f\"  Requested: {COHORTS_TO_PROCESS}\")\n",
    "missing = set(COHORTS_TO_PROCESS) - available_cohorts\n",
    "if missing:\n",
    "    print(f\"  ‚ö†Ô∏è  Missing: {missing}\")\n",
    "    # Check for case mismatches\n",
    "    cohort_lower_map = {c.lower(): c for c in available_cohorts}\n",
    "    corrected_cohorts = []\n",
    "    for m in missing:\n",
    "        m_lower = m.lower()\n",
    "        if m_lower in cohort_lower_map:\n",
    "            actual_name = cohort_lower_map[m_lower]\n",
    "            print(f\"  üí° Case mismatch: '{m}' ‚Üí '{actual_name}'\")\n",
    "            corrected_cohorts.append(actual_name)\n",
    "    \n",
    "    if corrected_cohorts:\n",
    "        print(f\"\\n  üí° RECOMMENDATION: Update COHORTS_TO_PROCESS to:\")\n",
    "        final_list = [c for c in COHORTS_TO_PROCESS if c not in missing] + corrected_cohorts\n",
    "        print(f\"     COHORTS_TO_PROCESS = {final_list}\")\n",
    "else:\n",
    "    print(f\"  ‚úÖ All requested cohorts found!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Memory logging function defined\n"
     ]
    }
   ],
   "source": [
    "def log_memory(logger, stage=\"\", show_details=False):\n",
    "    \"\"\"\n",
    "    Comprehensive memory monitoring with detailed breakdown.\n",
    "    \n",
    "    Args:\n",
    "        logger: Logger instance\n",
    "        stage: Description of current pipeline stage\n",
    "        show_details: If True, show process-level memory breakdown\n",
    "    \"\"\"\n",
    "    try:\n",
    "        mem = psutil.virtual_memory()\n",
    "        mem_used_gb = mem.used / (1024**3)\n",
    "        mem_total_gb = mem.total / (1024**3)\n",
    "        mem_percent = mem.percent\n",
    "        mem_avail_gb = mem.available / (1024**3)\n",
    "        mem_cached_gb = mem.cached / (1024**3) if hasattr(mem, 'cached') else 0\n",
    "        \n",
    "        # Get process memory\n",
    "        process = psutil.Process()\n",
    "        process_mem = process.memory_info()\n",
    "        process_rss_gb = process_mem.rss / (1024**3)\n",
    "        process_vms_gb = process_mem.vms / (1024**3)\n",
    "        \n",
    "        logger.info(f\"[MEMORY {stage}]\")\n",
    "        logger.info(f\"  System: {mem_used_gb:.1f} GB / {mem_total_gb:.1f} GB ({mem_percent:.1f}%) | Available: {mem_avail_gb:.1f} GB\")\n",
    "        logger.info(f\"  Process RSS: {process_rss_gb:.2f} GB | VMS: {process_vms_gb:.2f} GB\")\n",
    "        \n",
    "        # Warning thresholds\n",
    "        if mem_percent > 90:\n",
    "            logger.error(f\"üö® CRITICAL MEMORY: {mem_percent:.1f}% - OOM imminent!\")\n",
    "        elif mem_percent > 85:\n",
    "            logger.warning(f\"‚ö†Ô∏è  HIGH MEMORY USAGE: {mem_percent:.1f}% - May cause OOM!\")\n",
    "        elif mem_percent > 75:\n",
    "            logger.warning(f\"‚ö†Ô∏è  ELEVATED MEMORY: {mem_percent:.1f}% - Monitor closely\")\n",
    "        \n",
    "        # Process-level details if requested\n",
    "        if show_details:\n",
    "            try:\n",
    "                # Get top memory-consuming processes\n",
    "                processes = []\n",
    "                for p in psutil.process_iter(['pid', 'name', 'memory_info']):\n",
    "                    try:\n",
    "                        mem_info = p.info['memory_info']\n",
    "                        if mem_info:\n",
    "                            processes.append({\n",
    "                                'pid': p.info['pid'],\n",
    "                                'name': p.info['name'],\n",
    "                                'rss': mem_info.rss / (1024**3)\n",
    "                            })\n",
    "                    except (psutil.NoSuchProcess, psutil.AccessDenied):\n",
    "                        continue\n",
    "                \n",
    "                processes.sort(key=lambda x: x['rss'], reverse=True)\n",
    "                logger.info(f\"  Top 5 processes by memory:\")\n",
    "                for proc in processes[:5]:\n",
    "                    logger.info(f\"    {proc['name']} (PID {proc['pid']}): {proc['rss']:.2f} GB\")\n",
    "            except Exception as e:\n",
    "                logger.debug(f\"Could not get process details: {e}\")\n",
    "        \n",
    "        return {\n",
    "            'system_percent': mem_percent,\n",
    "            'system_used_gb': mem_used_gb,\n",
    "            'system_avail_gb': mem_avail_gb,\n",
    "            'process_rss_gb': process_rss_gb,\n",
    "            'process_vms_gb': process_vms_gb\n",
    "        }\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error getting memory info: {e}\")\n",
    "        return {'system_percent': 0.0}\n",
    "\n",
    "print(\"‚úì Enhanced memory logging function defined\")\n",
    "\n",
    "\n",
    "def filter_itemsets_by_lift(\n",
    "    itemsets: pd.DataFrame,\n",
    "    df_encoded: pd.DataFrame,\n",
    "    min_lift: float,\n",
    "    logger: logging.Logger\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Filter itemsets by lift to remove common/trivial itemsets.\n",
    "    \n",
    "    Lift measures how much more likely items are to appear together than by chance.\n",
    "    Lift = 1.0 means items are independent (not interesting)\n",
    "    Lift > 1.0 means positive association (interesting)\n",
    "    Lift < 1.0 means negative association (also interesting, but we filter these out)\n",
    "    \n",
    "    Args:\n",
    "        itemsets: DataFrame with 'itemsets' and 'support' columns\n",
    "        df_encoded: Encoded transaction DataFrame (needed to calculate individual item supports)\n",
    "        min_lift: Minimum lift threshold (e.g., 1.1 = 10% more likely than chance)\n",
    "        logger: Logger instance\n",
    "    \n",
    "    Returns:\n",
    "        Filtered DataFrame with only itemsets above min_lift threshold\n",
    "    \"\"\"\n",
    "    if len(itemsets) == 0:\n",
    "        return itemsets\n",
    "    \n",
    "    logger.info(f\"Filtering {len(itemsets):,} itemsets by lift (min_lift={min_lift})...\")\n",
    "    \n",
    "    # Calculate individual item supports (needed for lift calculation)\n",
    "    item_supports = {}\n",
    "    total_transactions = len(df_encoded)\n",
    "    \n",
    "    for col in df_encoded.columns:\n",
    "        item_supports[col] = df_encoded[col].sum() / total_transactions\n",
    "    \n",
    "    # Calculate lift for each itemset\n",
    "    def calculate_lift(row):\n",
    "        itemset = row['itemsets']\n",
    "        itemset_support = row['support']\n",
    "        \n",
    "        # For single-item itemsets, lift is undefined (or 1.0 by convention)\n",
    "        if len(itemset) == 1:\n",
    "            return 1.0  # Single items don't have lift\n",
    "        \n",
    "        # For multi-item itemsets: lift = itemset_support / (item1_support * item2_support * ...)\n",
    "        expected_support = 1.0\n",
    "        for item in itemset:\n",
    "            if item in item_supports:\n",
    "                expected_support *= item_supports[item]\n",
    "            else:\n",
    "                # Item not found in transactions (shouldn't happen, but handle gracefully)\n",
    "                return 0.0\n",
    "        \n",
    "        if expected_support == 0:\n",
    "            return 0.0\n",
    "        \n",
    "        lift = itemset_support / expected_support\n",
    "        \n",
    "        # CRITICAL: Cap extreme lift values to prevent numerical instability\n",
    "        # With very small cohorts, expected_support can be extremely small, causing\n",
    "        # lift values in the billions. Cap at reasonable maximum (e.g., 1000)\n",
    "        MAX_LIFT = 1000.0\n",
    "        if lift > MAX_LIFT:\n",
    "            logger.debug(f\"Capping extreme lift {lift:.2e} to {MAX_LIFT} for itemset {itemset}\")\n",
    "            return MAX_LIFT\n",
    "        \n",
    "        return lift\n",
    "    \n",
    "    itemsets['lift'] = itemsets.apply(calculate_lift, axis=1)\n",
    "    \n",
    "    # Filter by lift threshold\n",
    "    original_count = len(itemsets)\n",
    "    itemsets_filtered = itemsets[itemsets['lift'] >= min_lift].copy()\n",
    "    filtered_count = len(itemsets_filtered)\n",
    "    removed_count = original_count - filtered_count\n",
    "    \n",
    "    logger.info(f\"  Original itemsets: {original_count:,}\")\n",
    "    logger.info(f\"  Filtered itemsets: {filtered_count:,} (lift >= {min_lift})\")\n",
    "    logger.info(f\"  Removed common/trivial: {removed_count:,} ({removed_count/original_count*100:.1f}%)\")\n",
    "    \n",
    "    if filtered_count > 0:\n",
    "        logger.info(f\"  Lift range: {itemsets_filtered['lift'].min():.3f} - {itemsets_filtered['lift'].max():.3f}\")\n",
    "    \n",
    "    return itemsets_filtered.drop(columns=['lift'])  # Remove lift column (not needed in output)\n",
    "\n",
    "print(\"‚úì Itemset filtering by lift function defined\")\n",
    "\n",
    "# =============================================================================\n",
    "# HOW LIFT FILTERING REMOVES COMMON ITEMSETS\n",
    "# =============================================================================\n",
    "\"\"\"\n",
    "Lift measures how much MORE likely items are to appear together than by chance.\n",
    "\n",
    "Formula: Lift = P(A and B) / (P(A) * P(B))\n",
    "\n",
    "Interpretation:\n",
    "- Lift = 1.0 ‚Üí Items are INDEPENDENT (appear together by chance)\n",
    "- Lift > 1.0 ‚Üí Items have POSITIVE association (more likely together than chance)\n",
    "- Lift < 1.0 ‚Üí Items have NEGATIVE association (less likely together than chance)\n",
    "\n",
    "EXAMPLE: Why common itemsets get filtered out\n",
    "\n",
    "Scenario: 100 transactions, 2 very common items\n",
    "- Item A appears in 80 transactions (P(A) = 0.8)\n",
    "- Item B appears in 70 transactions (P(B) = 0.7)\n",
    "- They appear together in 56 transactions (P(A and B) = 0.56)\n",
    "\n",
    "Expected if independent: P(A) * P(B) = 0.8 * 0.7 = 0.56\n",
    "Actual: 0.56\n",
    "Lift = 0.56 / 0.56 = 1.0 ‚Üí INDEPENDENT (filtered out!)\n",
    "\n",
    "Even though {A, B} has high support (56%), it's just chance - not interesting.\n",
    "\n",
    "EXAMPLE: Meaningful association\n",
    "\n",
    "Scenario: 100 transactions\n",
    "- Item C appears in 20 transactions (P(C) = 0.2)\n",
    "- Item D appears in 15 transactions (P(D) = 0.15)\n",
    "- They appear together in 10 transactions (P(C and D) = 0.10)\n",
    "\n",
    "Expected if independent: P(C) * P(D) = 0.2 * 0.15 = 0.03\n",
    "Actual: 0.10\n",
    "Lift = 0.10 / 0.03 = 3.33 ‚Üí STRONG ASSOCIATION (kept!)\n",
    "\n",
    "Even though {C, D} has lower support (10%), it's 3.3x more likely than chance.\n",
    "\n",
    "WHY THIS MATTERS:\n",
    "- Common items (like \"aspirin\" or \"blood pressure check\") appear in many transactions\n",
    "- They often appear together just because they're both common\n",
    "- Lift filtering removes these \"common but independent\" patterns\n",
    "- Keeps only patterns where items are MORE associated than chance would predict\n",
    "\"\"\"\n",
    "print(\"‚úì Lift filtering explanation documented\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Discover Available Cohorts\n",
    "\n",
    "Scan local data to find all available cohort combinations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚ö†Ô∏è  DRY RUN: Limiting from 45 to 5 cohort combinations\n",
      "\n",
      "üìä Discovered Cohorts:\n",
      "  Total combinations: 5\n",
      "  opioid_ed: 5 combinations\n",
      "\n",
      "  Sample jobs:\n",
      "    opioid_ed/13-24/2016\n",
      "    opioid_ed/0-12/2016\n",
      "    opioid_ed/25-44/2016\n",
      "    opioid_ed/55-64/2016\n",
      "    opioid_ed/45-54/2016\n"
     ]
    }
   ],
   "source": [
    "def discover_cohorts(local_data_path, cohort_filter=None):\n",
    "    \"\"\"\n",
    "    Discover all available cohort combinations from local data.\n",
    "    \"\"\"\n",
    "    cohort_jobs = []\n",
    "    \n",
    "    for cohort_dir in local_data_path.glob(\"cohort_name=*\"):\n",
    "        cohort_name = cohort_dir.name.replace(\"cohort_name=\", \"\")\n",
    "        \n",
    "        # Filter if specified\n",
    "        if cohort_filter and cohort_name not in cohort_filter:\n",
    "            continue\n",
    "        \n",
    "        for year_dir in cohort_dir.glob(\"event_year=*\"):\n",
    "            event_year = year_dir.name.replace(\"event_year=\", \"\")\n",
    "            \n",
    "            for age_dir in year_dir.glob(\"age_band=*\"):\n",
    "                age_band = age_dir.name.replace(\"age_band=\", \"\")\n",
    "                \n",
    "                # Check if cohort file exists\n",
    "                cohort_file = age_dir / \"cohort.parquet\"\n",
    "                if cohort_file.exists():\n",
    "                    cohort_jobs.append({\n",
    "                        'cohort': cohort_name,\n",
    "                        'age_band': age_band,\n",
    "                        'event_year': event_year,\n",
    "                        'local_path': str(cohort_file)\n",
    "                    })\n",
    "    \n",
    "    return cohort_jobs\n",
    "\n",
    "# Discover available cohorts\n",
    "cohort_jobs = discover_cohorts(LOCAL_DATA_PATH, cohort_filter=COHORTS_TO_PROCESS)\n",
    "\n",
    "# Apply DRY_RUN limit if enabled\n",
    "if DRY_RUN and len(cohort_jobs) > DRY_RUN_LIMIT:\n",
    "    print(f\"\\n‚ö†Ô∏è  DRY RUN: Limiting from {len(cohort_jobs)} to {DRY_RUN_LIMIT} cohort combinations\")\n",
    "    cohort_jobs = cohort_jobs[:DRY_RUN_LIMIT]\n",
    "\n",
    "print(f\"\\nüìä Discovered Cohorts:\")\n",
    "print(f\"  Total combinations: {len(cohort_jobs)}\")\n",
    "\n",
    "# Group by cohort\n",
    "cohort_counts = {}\n",
    "for job in cohort_jobs:\n",
    "    cohort_counts[job['cohort']] = cohort_counts.get(job['cohort'], 0) + 1\n",
    "\n",
    "for cohort, count in cohort_counts.items():\n",
    "    print(f\"  {cohort}: {count} combinations\")\n",
    "\n",
    "print(f\"\\n  Sample jobs:\")\n",
    "for job in cohort_jobs[:5]:\n",
    "    print(f\"    {job['cohort']}/{job['age_band']}/{job['event_year']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Define Cohort Processing Function\n",
    "\n",
    "Create a function to process a single cohort for a specific item type with FP-Growth.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Cohort processing function defined\n"
     ]
    }
   ],
   "source": [
    "def process_single_cohort(job, item_type):\n",
    "    \"\"\"Process a single cohort for a specific item type with FP-Growth analysis.\"\"\"\n",
    "    cohort = job['cohort']\n",
    "    age_band = job['age_band']\n",
    "    event_year = job['event_year']\n",
    "    local_path = job['local_path']\n",
    "    \n",
    "    # Create cohort-specific logger that inherits from main logger\n",
    "    # Use the main logger's handlers so logs go to the same file\n",
    "    cohort_logger = logging.getLogger(f\"{cohort}_{age_band}_{event_year}_{item_type}\")\n",
    "    \n",
    "    # Set level based on DEBUG_MODE (need to check if DEBUG_MODE is available in this scope)\n",
    "    # Get DEBUG_MODE from globals or use INFO as default\n",
    "    debug_mode = globals().get('DEBUG_MODE', False)\n",
    "    cohort_logger.setLevel(logging.DEBUG if debug_mode else logging.INFO)\n",
    "    \n",
    "    # CRITICAL: Add handlers from main logger so logs are written to file\n",
    "    # Without this, cohort_logger logs go nowhere!\n",
    "    if not cohort_logger.handlers:\n",
    "        # Get the main logger and copy its handlers\n",
    "        main_logger = logging.getLogger('cohort_fpgrowth')\n",
    "        for handler in main_logger.handlers:\n",
    "            # For FileHandler, create new handler with same file\n",
    "            if isinstance(handler, logging.FileHandler):\n",
    "                new_handler = logging.FileHandler(handler.baseFilename)\n",
    "            # For StreamHandler, create new handler with same stream\n",
    "            elif isinstance(handler, logging.StreamHandler):\n",
    "                new_handler = logging.StreamHandler(handler.stream)\n",
    "            else:\n",
    "                # Fallback: try to create same type\n",
    "                new_handler = type(handler)(handler.baseFilename if hasattr(handler, 'baseFilename') else handler.stream)\n",
    "            \n",
    "            new_handler.setLevel(handler.level)\n",
    "            new_handler.setFormatter(handler.formatter)\n",
    "            cohort_logger.addHandler(new_handler)\n",
    "    \n",
    "    # Prevent propagation to avoid duplicate logs\n",
    "    cohort_logger.propagate = False\n",
    "    \n",
    "    # Debug mode: log initial setup\n",
    "    if debug_mode:\n",
    "        cohort_logger.debug(f\"Logger initialized for {cohort}/{age_band}/{event_year}/{item_type}\")\n",
    "        cohort_logger.debug(f\"Logger level: {logging.getLevelName(cohort_logger.level)}\")\n",
    "        cohort_logger.debug(f\"Handlers: {len(cohort_logger.handlers)}\")\n",
    "    \n",
    "    try:\n",
    "        start_time = time.time()\n",
    "        cohort_logger.info(f\"Processing {cohort}/{age_band}/{event_year} - {item_type}\")\n",
    "        mem_start = log_memory(cohort_logger, \"START\", show_details=True)\n",
    "        \n",
    "        # Extract items based on type + TARGET MARKERS (for target-focused rules)\n",
    "        # Simple in-memory connection (no AWS needed for local parquet reads)\n",
    "        cohort_logger.info(\"Creating DuckDB connection...\")\n",
    "        cohort_logger.debug(f\"Local path: {local_path}\")\n",
    "        cohort_logger.debug(f\"Item type: {item_type}\")\n",
    "        \n",
    "        con = duckdb.connect(':memory:')\n",
    "        con.sql(\"SET threads = 1\")\n",
    "        cohort_logger.debug(\"DuckDB connection created, threads set to 1\")\n",
    "        log_memory(cohort_logger, \"After DuckDB connection\")\n",
    "        \n",
    "        if item_type == 'drug_name':\n",
    "            # Pharmacy events: drug names only\n",
    "            query = f\"\"\"\n",
    "            SELECT mi_person_key, drug_name as item\n",
    "            FROM read_parquet('{local_path}')\n",
    "            WHERE drug_name IS NOT NULL AND drug_name != '' AND event_type = 'pharmacy'\n",
    "            \"\"\"\n",
    "        elif item_type == 'medical_codes':\n",
    "            # Medical events: combine ICD codes (diagnoses) + CPT codes (procedures)\n",
    "            # This creates richer transactions showing both diagnoses and procedures together\n",
    "            query = f\"\"\"\n",
    "            WITH all_icds AS (\n",
    "                SELECT mi_person_key, primary_icd_diagnosis_code as code FROM read_parquet('{local_path}') \n",
    "                WHERE primary_icd_diagnosis_code IS NOT NULL AND event_type = 'medical'\n",
    "                UNION ALL\n",
    "                SELECT mi_person_key, two_icd_diagnosis_code as code FROM read_parquet('{local_path}') \n",
    "                WHERE two_icd_diagnosis_code IS NOT NULL AND event_type = 'medical'\n",
    "                UNION ALL\n",
    "                SELECT mi_person_key, three_icd_diagnosis_code as code FROM read_parquet('{local_path}') \n",
    "                WHERE three_icd_diagnosis_code IS NOT NULL AND event_type = 'medical'\n",
    "                UNION ALL\n",
    "                SELECT mi_person_key, four_icd_diagnosis_code as code FROM read_parquet('{local_path}') \n",
    "                WHERE four_icd_diagnosis_code IS NOT NULL AND event_type = 'medical'\n",
    "                UNION ALL\n",
    "                SELECT mi_person_key, five_icd_diagnosis_code as code FROM read_parquet('{local_path}') \n",
    "                WHERE five_icd_diagnosis_code IS NOT NULL AND event_type = 'medical'\n",
    "            ),\n",
    "            all_cpts AS (\n",
    "                SELECT mi_person_key, procedure_code as code\n",
    "                FROM read_parquet('{local_path}')\n",
    "                WHERE procedure_code IS NOT NULL AND procedure_code != '' AND event_type = 'medical'\n",
    "            ),\n",
    "            combined_medical AS (\n",
    "                SELECT mi_person_key, code as item FROM all_icds WHERE code != ''\n",
    "                UNION ALL\n",
    "                SELECT mi_person_key, code as item FROM all_cpts WHERE code != ''\n",
    "            )\n",
    "            SELECT mi_person_key, item FROM combined_medical\n",
    "            \"\"\"\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown item_type: {item_type}. Expected 'drug_name' or 'medical_codes'\")\n",
    "        \n",
    "        cohort_logger.info(f\"Executing query to extract {item_type}...\")\n",
    "        if debug_mode:\n",
    "            cohort_logger.debug(f\"Query: {query[:200]}...\")  # Log first 200 chars of query\n",
    "        \n",
    "        df = con.execute(query).df()\n",
    "        cohort_logger.info(f\"Extracted {len(df):,} rows, {df['mi_person_key'].nunique():,} unique patients\")\n",
    "        \n",
    "        if debug_mode:\n",
    "            cohort_logger.debug(f\"DataFrame shape: {df.shape}\")\n",
    "            cohort_logger.debug(f\"Unique items: {df['item'].nunique() if 'item' in df.columns else 'N/A'}\")\n",
    "            cohort_logger.debug(f\"Memory usage: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "        \n",
    "        mem_after_extract = log_memory(cohort_logger, \"After data extraction\")\n",
    "        \n",
    "        # Check memory increase\n",
    "        if mem_start['system_percent'] > 0:\n",
    "            mem_delta = mem_after_extract['system_percent'] - mem_start['system_percent']\n",
    "            if mem_delta > 10:\n",
    "                cohort_logger.warning(f\"‚ö†Ô∏è  Large memory increase during extraction: {mem_delta:.1f}%\")\n",
    "        \n",
    "        # Add target markers if TARGET_FOCUSED mode is enabled\n",
    "        if TARGET_FOCUSED:\n",
    "            cohort_logger.info(\"Adding target markers...\")\n",
    "            \n",
    "            # Get target information for each patient\n",
    "            target_query = f\"\"\"\n",
    "            SELECT DISTINCT \n",
    "                mi_person_key,\n",
    "                primary_icd_diagnosis_code,\n",
    "                hcg_line\n",
    "            FROM read_parquet('{local_path}')\n",
    "            WHERE mi_person_key IS NOT NULL\n",
    "            \"\"\"\n",
    "df_targets = con.execute(target_query).df()\n",
    "            cohort_logger.debug(f\"Target query returned {len(df_targets):,} rows\")\n",
    "            \n",
    "            # OPTIMIZED: Use vectorized operations instead of iterrows() (much faster for large datasets!)\n",
    "            # iterrows() is extremely slow - can take minutes for 393K+ rows\n",
    "            target_items_list = []\n",
    "            \n",
    "            # Check for opioid ICD codes (vectorized)\n",
    "            if 'primary_icd_diagnosis_code' in df_targets.columns:\n",
    "                # Create list of ICD code prefixes to match (without dots)\n",
    "                icd_prefixes = [code.replace('.', '') for code in TARGET_ICD_CODES]\n",
    "                \n",
    "                # Vectorized check: check if any ICD code starts with any prefix\n",
    "                icd_mask = df_targets['primary_icd_diagnosis_code'].notna()\n",
    "                if icd_mask.any():\n",
    "                    # For each row, check if ICD code starts with any prefix (vectorized)\n",
    "                    icd_matches = df_targets.loc[icd_mask, 'primary_icd_diagnosis_code'].apply(\n",
    "                        lambda x: any(str(x).startswith(prefix) for prefix in icd_prefixes) if pd.notna(x) else False\n",
    "                    )\n",
    "                    # Get patient IDs with opioid ICD codes\n",
    "                    opioid_patients = df_targets.loc[icd_mask & icd_matches, 'mi_person_key']\n",
    "                    if len(opioid_patients) > 0:\n",
    "                        target_items_list.append(\n",
    "                            pd.DataFrame({\n",
    "                                'mi_person_key': opioid_patients,\n",
    "                                'item': 'TARGET_ICD:OPIOID_DEPENDENCE'\n",
    "                            })\n",
    "                        )\n",
    "                        cohort_logger.debug(f\"Found {len(opioid_patients):,} patients with opioid ICD codes\")\n",
    "            \n",
    "            # Check for ED visits (vectorized - much faster than iterrows!)\n",
    "            if 'hcg_line' in df_targets.columns:\n",
    "                ed_mask = df_targets['hcg_line'].isin(TARGET_HCG_LINES)\n",
    "                ed_patients = df_targets.loc[ed_mask, 'mi_person_key']\n",
    "                if len(ed_patients) > 0:\n",
    "                    target_items_list.append(\n",
    "                        pd.DataFrame({\n",
    "                            'mi_person_key': ed_patients,\n",
    "                            'item': 'TARGET_ED:EMERGENCY_DEPT'\n",
    "                        })\n",
    "                    )\n",
    "                    cohort_logger.debug(f\"Found {len(ed_patients):,} patients with ED visits\")\n",
    "            \n",
    "            # Combine all target items\n",
    "            if target_items_list:\n",
    "                df_targets_items = pd.concat(target_items_list, ignore_index=True)\n",
    "                df = pd.concat([df, df_targets_items], ignore_index=True)\n",
    "                cohort_logger.info(f\"Added {len(df_targets_items):,} target markers\")\n",
    "                mem_after_targets = log_memory(cohort_logger, \"After target markers\")\n",
    "                \n",
    "                # Check memory increase\n",
    "                if mem_after_extract['system_percent'] > 0:\n",
    "                    mem_delta = mem_after_targets['system_percent'] - mem_after_extract['system_percent']\n",
    "                    if mem_delta > 5:\n",
    "                        cohort_logger.warning(f\"‚ö†Ô∏è  Memory increase during target markers: {mem_delta:.1f}%\")\n",
    "            else:\n",
    "                cohort_logger.info(\"No target markers found (no opioid ICD codes or ED visits)\")\n",
    "                mem_after_targets = log_memory(cohort_logger, \"After target markers (none found)\")\n",
    "        con.close()\n",
    "        \n",
    "        if df.empty:\n",
    "            cohort_logger.warning(f\"No {item_type} data for {cohort}/{age_band}/{event_year}\")\n",
    "            return (cohort, age_band, event_year, item_type, False, \"No data\")\n",
    "        \n",
    "        # Create transactions (group items by patient)\n",
    "        cohort_logger.info(f\"Building transactions from {len(df):,} rows...\")\n",
    "        mem_before_transactions = log_memory(cohort_logger, \"Before transaction building\")\n",
    "        \n",
    "        transactions = (\n",
    "            df.groupby('mi_person_key')['item']\n",
    "            .apply(lambda x: sorted(set(x.tolist())))\n",
    "            .tolist()\n",
    "        )\n",
    "        \n",
    "        mem_after_transactions = log_memory(cohort_logger, \"After transaction building\")\n",
    "        cohort_logger.info(f\"Created {len(transactions):,} transactions\")\n",
    "        \n",
    "        # Check transaction sizes\n",
    "        if transactions:\n",
    "            transaction_sizes = [len(t) for t in transactions]\n",
    "            cohort_logger.info(f\"Transaction size stats: min={min(transaction_sizes)}, max={max(transaction_sizes)}, \"\n",
    "                             f\"mean={np.mean(transaction_sizes):.1f}, median={np.median(transaction_sizes):.1f}\")\n",
    "            \n",
    "            # Warn if very large transactions\n",
    "            if max(transaction_sizes) > 1000:\n",
    "                cohort_logger.warning(f\"‚ö†Ô∏è  Very large transactions detected (max={max(transaction_sizes)}) - may cause memory issues\")\n",
    "        \n",
    "        if not transactions:\n",
    "            cohort_logger.warning(f\"No valid transactions for {cohort}/{age_band}/{event_year}\")\n",
    "            return (cohort, age_band, event_year, item_type, False, \"No transactions\")\n",
    "        \n",
    "        # CRITICAL: For small cohorts, min_support needs to be higher to prevent explosion\n",
    "        # With only 4 transactions, min_support=0.05 means itemset needs to appear in 1 transaction\n",
    "        # This is way too low and generates millions of trivial itemsets\n",
    "        MIN_TRANSACTIONS_FOR_STABLE_FPGROWTH = 10\n",
    "        if len(transactions) < MIN_TRANSACTIONS_FOR_STABLE_FPGROWTH:\n",
    "            cohort_logger.warning(f\"‚ö†Ô∏è  Very small cohort: {len(transactions)} transactions\")\n",
    "            cohort_logger.warning(f\"   FP-Growth may generate excessive itemsets. Consider skipping or using higher min_support.\")\n",
    "            # Increase min_support for small cohorts\n",
    "            min_sup_original = MIN_SUPPORT_CPT if item_type == 'medical_codes' else MIN_SUPPORT\n",
    "            # For <10 transactions, require itemset to appear in at least 2 transactions\n",
    "            min_sup_adjusted = max(min_sup_original, 2.0 / len(transactions))\n",
    "            if min_sup_adjusted > min_sup_original:\n",
    "                cohort_logger.info(f\"   Adjusted min_support from {min_sup_original} to {min_sup_adjusted:.3f} for small cohort\")\n",
    "                min_sup = min_sup_adjusted\n",
    "            else:\n",
    "                min_sup = min_sup_original\n",
    "        else:\n",
    "            min_sup = MIN_SUPPORT_CPT if item_type == 'medical_codes' else MIN_SUPPORT\n",
    "        \n",
    "        # Encode transactions\n",
    "        cohort_logger.info(f\"Encoding {len(transactions):,} transactions...\")\n",
    "        mem_before_encode = log_memory(cohort_logger, \"Before encoding\")\n",
    "        \n",
    "        if debug_mode:\n",
    "            cohort_logger.debug(f\"Sample transaction (first 3): {transactions[:3] if len(transactions) >= 3 else transactions}\")\n",
    "            cohort_logger.debug(f\"Total unique items across all transactions: {len(set(item for t in transactions for item in t))}\")\n",
    "        \n",
    "        te = TransactionEncoder()\n",
    "        cohort_logger.debug(\"Fitting TransactionEncoder...\")\n",
    "        te_ary = te.fit(transactions).transform(transactions)\n",
    "        cohort_logger.debug(\"Transforming transactions to binary matrix...\")\n",
    "        df_encoded = pd.DataFrame(te_ary, columns=te.columns_)\n",
    "        \n",
    "        mem_after_encode = log_memory(cohort_logger, \"After encoding\")\n",
    "        cohort_logger.info(f\"Encoded matrix shape: {df_encoded.shape} ({df_encoded.shape[0]*df_encoded.shape[1]:,} elements)\")\n",
    "        \n",
    "        if debug_mode:\n",
    "            cohort_logger.debug(f\"Matrix memory usage: {df_encoded.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "            cohort_logger.debug(f\"Matrix sparsity: {(1 - df_encoded.sum().sum() / (df_encoded.shape[0] * df_encoded.shape[1])) * 100:.2f}%\")\n",
    "        \n",
    "        # Check memory increase during encoding (this is often where OOM happens)\n",
    "        if mem_before_encode['system_percent'] > 0:\n",
    "            mem_delta = mem_after_encode['system_percent'] - mem_before_encode['system_percent']\n",
    "            if mem_delta > 15:\n",
    "                cohort_logger.error(f\"üö® CRITICAL: Large memory spike during encoding: {mem_delta:.1f}%\")\n",
    "                cohort_logger.error(f\"   Matrix size may be too large - consider filtering transactions\")\n",
    "            elif mem_delta > 10:\n",
    "                cohort_logger.warning(f\"‚ö†Ô∏è  Significant memory increase during encoding: {mem_delta:.1f}%\")\n",
    "        \n",
    "        # Run FP-Growth (min_sup already set above with small-cohort adjustment)\n",
    "        cohort_logger.info(f\"Running FP-Growth (min_support={min_sup})...\")\n",
    "        mem_before_fpgrowth = log_memory(cohort_logger, \"Before FP-Growth\")\n",
    "        \n",
    "        if debug_mode:\n",
    "            cohort_logger.debug(f\"FP-Growth parameters: min_support={min_sup}, matrix_shape={df_encoded.shape}\")\n",
    "            cohort_logger.debug(f\"Minimum support count: {int(min_sup * len(transactions))} transactions\")\n",
    "        \n",
    "        itemsets = fpgrowth(df_encoded, min_support=min_sup, use_colnames=True)\n",
    "        itemsets = itemsets.sort_values('support', ascending=False).reset_index(drop=True)\n",
    "        \n",
    "        mem_after_fpgrowth = log_memory(cohort_logger, \"After FP-Growth\")\n",
    "        \n",
    "        if debug_mode:\n",
    "            cohort_logger.debug(f\"FP-Growth completed, processing {len(itemsets)} itemsets\")\n",
    "        \n",
    "        if itemsets.empty:\n",
    "            cohort_logger.warning(f\"No itemsets found for {cohort}/{age_band}/{event_year}\")\n",
    "            return (cohort, age_band, event_year, item_type, False, \"No itemsets\")\n",
    "        \n",
    "        # Warn if still too many itemsets (indicates min_support too low)\n",
    "        if len(itemsets) > 100000:\n",
    "            cohort_logger.warning(f\"‚ö†Ô∏è  WARNING: {len(itemsets):,} itemsets is excessive!\")\n",
    "            cohort_logger.warning(f\"   Consider increasing min_support or filtering transactions\")\n",
    "            cohort_logger.warning(f\"   Current min_support: {min_sup}, Transactions: {len(transactions)}\")\n",
    "        \n",
    "        cohort_logger.info(f\"Found {len(itemsets):,} itemsets (before lift filtering)\")\n",
    "        \n",
    "        # CRITICAL: Filter out common/trivial itemsets by lift BEFORE generating rules\n",
    "        # This prevents memory issues from keeping millions of trivial itemsets\n",
    "        # For small cohorts, lift becomes unreliable, so use higher threshold or skip\n",
    "        if len(transactions) < MIN_TRANSACTIONS_FOR_STABLE_FPGROWTH:\n",
    "            # For very small cohorts, lift is unreliable - use much higher threshold\n",
    "            min_lift_adjusted = max(MIN_ITEMSET_LIFT, 2.0)  # Require at least 2x lift\n",
    "            cohort_logger.info(f\"Using adjusted min_lift={min_lift_adjusted} for small cohort (lift unreliable with <{MIN_TRANSACTIONS_FOR_STABLE_FPGROWTH} transactions)\")\n",
    "        else:\n",
    "            min_lift_adjusted = MIN_ITEMSET_LIFT\n",
    "        \n",
    "        itemsets = filter_itemsets_by_lift(\n",
    "            itemsets, \n",
    "            df_encoded, \n",
    "            min_lift_adjusted, \n",
    "            cohort_logger\n",
    "        )\n",
    "        mem_after_filtering = log_memory(cohort_logger, \"After lift filtering\")\n",
    "        \n",
    "        if itemsets.empty:\n",
    "            cohort_logger.warning(f\"No itemsets remaining after lift filtering for {cohort}/{age_band}/{event_year}\")\n",
    "            return (cohort, age_band, event_year, item_type, False, \"No itemsets after filtering\")\n",
    "        \n",
    "        cohort_logger.info(f\"Found {len(itemsets):,} itemsets (after lift filtering)\")\n",
    "        \n",
    "        # CRITICAL: Filter out single-item itemsets before rule generation\n",
    "        # association_rules() requires multi-item itemsets (can't generate rules from single items)\n",
    "        # Check itemset sizes more carefully\n",
    "        itemset_sizes = itemsets['itemsets'].apply(len)\n",
    "        single_item_count = (itemset_sizes == 1).sum()\n",
    "        \n",
    "        if debug_mode:\n",
    "            cohort_logger.debug(f\"Itemset size distribution: {itemset_sizes.value_counts().to_dict()}\")\n",
    "        \n",
    "        if single_item_count > 0:\n",
    "            cohort_logger.info(f\"Filtering out {single_item_count:,} single-item itemsets (cannot generate rules)\")\n",
    "            itemsets = itemsets[itemset_sizes > 1].copy()\n",
    "            cohort_logger.info(f\"Remaining multi-item itemsets: {len(itemsets):,}\")\n",
    "        \n",
    "        if itemsets.empty:\n",
    "            cohort_logger.warning(f\"No multi-item itemsets remaining for {cohort}/{age_band}/{event_year}\")\n",
    "            return (cohort, age_band, event_year, item_type, False, \"No multi-item itemsets\")\n",
    "        \n",
    "        # Double-check: verify all remaining itemsets are multi-item\n",
    "        final_itemset_sizes = itemsets['itemsets'].apply(len)\n",
    "        if (final_itemset_sizes == 1).any():\n",
    "            cohort_logger.error(f\"‚ö†Ô∏è  ERROR: Single-item itemsets still present after filtering!\")\n",
    "            cohort_logger.error(f\"   This should not happen - removing them now\")\n",
    "            itemsets = itemsets[final_itemset_sizes > 1].copy()\n",
    "            if itemsets.empty:\n",
    "                cohort_logger.warning(f\"No multi-item itemsets remaining after cleanup\")\n",
    "                return (cohort, age_band, event_year, item_type, False, \"No multi-item itemsets after cleanup\")\n",
    "        \n",
    "        # Check memory increase\n",
    "        if mem_before_fpgrowth['system_percent'] > 0:\n",
    "            mem_delta = mem_after_fpgrowth['system_percent'] - mem_before_fpgrowth['system_percent']\n",
    "            if mem_delta > 10:\n",
    "                cohort_logger.warning(f\"‚ö†Ô∏è  Memory increase during FP-Growth: {mem_delta:.1f}%\")\n",
    "        \n",
    "        # Log itemset statistics (after single-item filtering)\n",
    "        if len(itemsets) > 0:\n",
    "            final_itemset_sizes = itemsets['itemsets'].apply(len)\n",
    "            cohort_logger.info(f\"Itemset size stats: min={final_itemset_sizes.min()}, max={final_itemset_sizes.max()}, \"\n",
    "                             f\"mean={final_itemset_sizes.mean():.1f}, median={final_itemset_sizes.median():.1f}\")\n",
    "            \n",
    "            # Verify no single-item itemsets remain\n",
    "            if (final_itemset_sizes == 1).any():\n",
    "                cohort_logger.error(f\"üö® CRITICAL: Single-item itemsets detected before rule generation!\")\n",
    "                cohort_logger.error(f\"   This will cause association_rules() to fail\")\n",
    "        \n",
    "        # Generate rules (with appropriate thresholds and limits)\n",
    "        min_conf = MIN_CONFIDENCE_CPT if item_type == 'medical_codes' else MIN_CONFIDENCE\n",
    "        cohort_logger.info(f\"Generating rules (min_confidence={min_conf})...\")\n",
    "        mem_before_rules = log_memory(cohort_logger, \"Before rule generation\")\n",
    "        \n",
    "        if debug_mode:\n",
    "            cohort_logger.debug(f\"Rule generation parameters: min_confidence={min_conf}, itemsets={len(itemsets)}\")\n",
    "            cohort_logger.debug(f\"Expected max rules (theoretical): {len(itemsets) * (len(itemsets) - 1) / 2:,}\")\n",
    "        \n",
    "        # Final safety check: ensure no single-item itemsets\n",
    "        itemset_lengths = itemsets['itemsets'].apply(len)\n",
    "        if (itemset_lengths == 1).any():\n",
    "            cohort_logger.error(f\"üö® CRITICAL: Cannot generate rules - single-item itemsets present!\")\n",
    "            cohort_logger.error(f\"   Single-item count: {(itemset_lengths == 1).sum()}\")\n",
    "            cohort_logger.error(f\"   This is a bug - single-item itemsets should have been filtered\")\n",
    "            rules = pd.DataFrame()\n",
    "            rules_control = pd.DataFrame()\n",
    "        else:\n",
    "            try:\n",
    "                # Verify itemsets structure before calling association_rules\n",
    "                if debug_mode:\n",
    "                    cohort_logger.debug(f\"Itemsets DataFrame columns: {itemsets.columns.tolist()}\")\n",
    "                    cohort_logger.debug(f\"Itemsets shape: {itemsets.shape}\")\n",
    "                    if len(itemsets) > 0:\n",
    "                        cohort_logger.debug(f\"Sample itemset: {itemsets.iloc[0]['itemsets']}\")\n",
    "                        cohort_logger.debug(f\"Sample itemset size: {len(itemsets.iloc[0]['itemsets'])}\")\n",
    "                \n",
    "                # mlxtend association_rules() requires itemsets with at least 2 items\n",
    "                # Double-check one more time before calling\n",
    "                final_check = itemsets['itemsets'].apply(len)\n",
    "                if (final_check == 1).any():\n",
    "                    raise ValueError(\"Single-item itemsets detected - cannot generate association rules\")\n",
    "                \n",
    "                # Try generating full rules with confidence/lift first\n",
    "                # If this fails (mlxtend bug with certain itemset structures), fall back to support_only\n",
    "                try:\n",
    "                    all_rules = association_rules(itemsets, metric=\"confidence\", min_threshold=min_conf)\n",
    "                    mem_after_rules = log_memory(cohort_logger, \"After rule generation\")\n",
    "                except Exception as e:\n",
    "                    # mlxtend sometimes fails with \"missing antecedent/consequent\" error\n",
    "                    # This can happen if itemsets structure doesn't match what mlxtend expects\n",
    "                    # Fall back to support_only mode (but we lose confidence/lift metrics)\n",
    "                    error_msg = str(e)\n",
    "                    if \"antecedent\" in error_msg.lower() or \"consequent\" in error_msg.lower():\n",
    "                        cohort_logger.warning(f\"‚ö†Ô∏è  association_rules() failed with itemset structure issue\")\n",
    "                        cohort_logger.warning(f\"   Error: {error_msg[:200]}\")\n",
    "                        cohort_logger.warning(f\"   Falling back to support_only=True (will only have support metrics)\")\n",
    "                        cohort_logger.warning(f\"   This is a workaround for mlxtend bug - investigating itemsets structure\")\n",
    "                        \n",
    "                        # Log itemset details for debugging\n",
    "                        if debug_mode:\n",
    "                            cohort_logger.debug(f\"Itemsets that caused error:\")\n",
    "                            for idx, row in itemsets.head(10).iterrows():\n",
    "                                cohort_logger.debug(f\"  [{idx}] {row['itemsets']} (size={len(row['itemsets'])}, support={row['support']:.4f})\")\n",
    "                        \n",
    "                        # Fallback: use support_only (but we lose confidence/lift)\n",
    "                        all_rules = association_rules(itemsets, metric=\"support\", min_threshold=0.0, support_only=True)\n",
    "                        mem_after_rules = log_memory(cohort_logger, \"After rule generation (support_only fallback)\")\n",
    "                        cohort_logger.warning(f\"‚ö†Ô∏è  Generated {len(all_rules):,} rules with support_only=True (no confidence/lift metrics)\")\n",
    "                    else:\n",
    "                        # Re-raise if it's a different error\n",
    "                        raise\n",
    "                \n",
    "                # Check if we're in support_only mode (no confidence/lift)\n",
    "                support_only_mode = 'confidence' not in all_rules.columns if len(all_rules) > 0 else False\n",
    "                \n",
    "                if debug_mode:\n",
    "                    cohort_logger.debug(f\"Generated {len(all_rules):,} association rules\")\n",
    "                    if len(all_rules) > 0:\n",
    "                        if support_only_mode:\n",
    "                            cohort_logger.debug(f\"‚ö†Ô∏è  Rules in support_only mode - no confidence/lift metrics available\")\n",
    "                            cohort_logger.debug(f\"Rule support range: {all_rules['support'].min():.3f} - {all_rules['support'].max():.3f}\")\n",
    "                        else:\n",
    "                            cohort_logger.debug(f\"Rule confidence range: {all_rules['confidence'].min():.3f} - {all_rules['confidence'].max():.3f}\")\n",
    "                            cohort_logger.debug(f\"Rule lift range: {all_rules['lift'].min():.3f} - {all_rules['lift'].max():.3f}\")\n",
    "                \n",
    "                # Check memory increase (rule generation can be memory-intensive)\n",
    "                if mem_before_rules['system_percent'] > 0:\n",
    "                    mem_delta = mem_after_rules['system_percent'] - mem_before_rules['system_percent']\n",
    "                    if mem_delta > 20:\n",
    "                        cohort_logger.error(f\"üö® CRITICAL: Very large memory spike during rule generation: {mem_delta:.1f}%\")\n",
    "                        cohort_logger.error(f\"   Consider increasing min_confidence or filtering itemsets before rule generation\")\n",
    "                    elif mem_delta > 15:\n",
    "                        cohort_logger.warning(f\"‚ö†Ô∏è  Large memory increase during rule generation: {mem_delta:.1f}%\")\n",
    "                \n",
    "                if len(all_rules) > 0:\n",
    "                    # Split rules: target-predicting vs control (non-target)\n",
    "                    if TARGET_FOCUSED:\n",
    "                        # Target rules: consequent contains target marker\n",
    "                        target_mask = all_rules['consequents'].apply(\n",
    "                            lambda x: any(item.startswith(tuple(TARGET_PREFIXES)) for item in x)\n",
    "                        )\n",
    "                        rules_target = all_rules[target_mask].copy()\n",
    "                        rules_control = all_rules[~target_mask].copy()\n",
    "                        \n",
    "                        cohort_logger.info(f\"Split: {len(rules_target)} target rules, {len(rules_control)} control rules\")\n",
    "                        \n",
    "                        # Limit both sets to top N (by lift if available, otherwise by support)\n",
    "                        sort_col = 'lift' if 'lift' in all_rules.columns else 'support'\n",
    "                        sort_ascending = False  # Descending (highest first)\n",
    "                        \n",
    "                        if len(rules_target) > 0:\n",
    "                            rules_target = rules_target.sort_values(sort_col, ascending=sort_ascending)\n",
    "                            if len(rules_target) > MAX_RULES_PER_COHORT:\n",
    "                                cohort_logger.info(f\"Keeping top {MAX_RULES_PER_COHORT} target rules (from {len(rules_target)})\")\n",
    "                                rules_target = rules_target.head(MAX_RULES_PER_COHORT)\n",
    "                            rules_target = rules_target.reset_index(drop=True)\n",
    "                        \n",
    "                        if len(rules_control) > 0:\n",
    "                            rules_control = rules_control.sort_values(sort_col, ascending=sort_ascending)\n",
    "                            if len(rules_control) > MAX_RULES_PER_COHORT:\n",
    "                                cohort_logger.info(f\"Keeping top {MAX_RULES_PER_COHORT} control rules (from {len(rules_control)})\")\n",
    "                                rules_control = rules_control.head(MAX_RULES_PER_COHORT)\n",
    "                            rules_control = rules_control.reset_index(drop=True)\n",
    "                        \n",
    "                        # Keep target rules as main 'rules' for backward compatibility\n",
    "                        rules = rules_target\n",
    "                    else:\n",
    "                        # Not target-focused: all rules are kept\n",
    "                        sort_col = 'lift' if 'lift' in all_rules.columns else 'support'\n",
    "                        rules = all_rules.sort_values(sort_col, ascending=False).head(MAX_RULES_PER_COHORT).reset_index(drop=True)\n",
    "                        rules_control = pd.DataFrame()\n",
    "                    \n",
    "                    cohort_logger.info(f\"Final: {len(rules)} target rules, {len(rules_control)} control rules\")\n",
    "                    mem_after_filtering = log_memory(cohort_logger, \"After rule filtering\")\n",
    "                else:\n",
    "                    cohort_logger.info(f\"No rules met confidence threshold of {min_conf}\")\n",
    "                    rules = pd.DataFrame()\n",
    "                    rules_control = pd.DataFrame()\n",
    "                    \n",
    "            except MemoryError as e:\n",
    "                cohort_logger.error(f\"MemoryError during rule generation - skipping rules\")\n",
    "                rules = pd.DataFrame()\n",
    "                rules_control = pd.DataFrame()\n",
    "            except Exception as e:\n",
    "                cohort_logger.error(f\"Error generating rules: {e}\")\n",
    "                rules = pd.DataFrame()\n",
    "                rules_control = pd.DataFrame()\n",
    "        \n",
    "        # Convert frozensets for JSON\n",
    "        itemsets_json = itemsets.copy()\n",
    "        itemsets_json['itemsets'] = itemsets_json['itemsets'].apply(list)\n",
    "        \n",
    "        # Prepare rules for saving (split target rules by type, plus control)\n",
    "        rules_by_target = {}\n",
    "        \n",
    "        # Process target rules (split by ICD vs ED)\n",
    "        if not rules.empty:\n",
    "            rules_json = rules.copy()\n",
    "            rules_json['antecedents'] = rules_json['antecedents'].apply(list)\n",
    "            rules_json['consequents'] = rules_json['consequents'].apply(list)\n",
    "            \n",
    "            # Split target rules by outcome type\n",
    "            rules_by_target['TARGET_ICD'] = rules_json[\n",
    "                rules_json['consequents'].apply(lambda x: any('TARGET_ICD:' in str(item) for item in x))\n",
    "            ]\n",
    "            rules_by_target['TARGET_ED'] = rules_json[\n",
    "                rules_json['consequents'].apply(lambda x: any('TARGET_ED:' in str(item) for item in x))\n",
    "            ]\n",
    "        \n",
    "        # Process control rules (non-target patterns)\n",
    "        if not rules_control.empty:\n",
    "            rules_control_json = rules_control.copy()\n",
    "            rules_control_json['antecedents'] = rules_control_json['antecedents'].apply(list)\n",
    "            rules_control_json['consequents'] = rules_control_json['consequents'].apply(list)\n",
    "            rules_by_target['CONTROL'] = rules_control_json\n",
    "        \n",
    "        if rules_by_target:\n",
    "            cohort_logger.info(f\"Prepared for S3: {len(rules_by_target.get('TARGET_ICD', pd.DataFrame()))} ICD, \"\n",
    "                             f\"{len(rules_by_target.get('TARGET_ED', pd.DataFrame()))} ED, \"\n",
    "                             f\"{len(rules_by_target.get('CONTROL', pd.DataFrame()))} control\")\n",
    "        \n",
    "        # Save to S3 (with retry logic for reliability)\n",
    "        s3_base = f\"{S3_OUTPUT_BASE}/{item_type}/cohort_name={cohort}/age_band={age_band}/event_year={event_year}\"\n",
    "        \n",
    "        cohort_logger.info(f\"Saving results to S3...\")\n",
    "        max_retries = 3\n",
    "        for attempt in range(max_retries):\n",
    "            try:\n",
    "                itemsets_path = f\"{s3_base}/itemsets.json\"\n",
    "                save_to_s3_json(itemsets_json.to_dict(orient='records'), itemsets_path)\n",
    "                \n",
    "                # Save rules by target type (separate files)\n",
    "                if rules_by_target:\n",
    "                    for target_type, target_rules in rules_by_target.items():\n",
    "                        if not target_rules.empty:\n",
    "                            rules_path = f\"{s3_base}/rules_{target_type}.json\"\n",
    "                            save_to_s3_json(target_rules.to_dict(orient='records'), rules_path)\n",
    "                            cohort_logger.info(f\"Saved {len(target_rules)} {target_type} rules\")\n",
    "                \n",
    "                summary = {\n",
    "                    'timestamp': datetime.now().isoformat(),\n",
    "                    'cohort': cohort, 'age_band': age_band, 'event_year': event_year,\n",
    "                    'item_type': item_type,\n",
    "                    'total_patients': len(transactions),\n",
    "                    'total_itemsets': len(itemsets),\n",
    "                    'total_rules': len(rules),\n",
    "                    'rules_by_target': {\n",
    "                        'TARGET_ICD': len(rules_by_target.get('TARGET_ICD', pd.DataFrame())),\n",
    "                        'TARGET_ED': len(rules_by_target.get('TARGET_ED', pd.DataFrame())),\n",
    "                        'CONTROL': len(rules_by_target.get('CONTROL', pd.DataFrame()))\n",
    "                    } if rules_by_target else {'TARGET_ICD': 0, 'TARGET_ED': 0, 'CONTROL': 0},\n",
    "                    'min_support': min_sup,\n",
    "                    'min_confidence': min_conf,\n",
    "                    'max_rules_limit': MAX_RULES_PER_COHORT,\n",
    "                    'rules_truncated': len(rules) == MAX_RULES_PER_COHORT,\n",
    "                    'target_focused': TARGET_FOCUSED,\n",
    "                    'target_icd_codes': TARGET_ICD_CODES if TARGET_FOCUSED else None,\n",
    "                    'target_hcg_lines': TARGET_HCG_LINES if TARGET_FOCUSED else None\n",
    "                }\n",
    "                summary_path = f\"{s3_base}/summary.json\"\n",
    "                save_to_s3_json(summary, summary_path)\n",
    "                \n",
    "                cohort_logger.info(f\"‚úì Saved to S3 successfully\")\n",
    "                break  # Success - exit retry loop\n",
    "                \n",
    "            except Exception as s3_error:\n",
    "                if attempt < max_retries - 1:\n",
    "                    cohort_logger.warning(f\"S3 upload attempt {attempt+1} failed: {s3_error}, retrying...\")\n",
    "                    time.sleep(2 ** attempt)  # Exponential backoff\n",
    "                else:\n",
    "                    cohort_logger.error(f\"S3 upload failed after {max_retries} attempts: {s3_error}\")\n",
    "                    raise\n",
    "        \n",
    "        elapsed = time.time() - start_time\n",
    "        log_memory(cohort_logger, \"END\")\n",
    "        cohort_logger.info(f\"‚úì Completed in {elapsed:.1f}s\")\n",
    "        \n",
    "        return (cohort, age_band, event_year, item_type, True, f\"{len(itemsets)} itemsets, {len(rules)} rules\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        cohort_logger.error(f\"Error: {e}\")\n",
    "        return (cohort, age_band, event_year, item_type, False, str(e))\n",
    "\n",
    "print(\"‚úì Cohort processing function defined\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Process Cohorts in Parallel\n",
    "\n",
    "Run FP-Growth for all cohort combinations using parallel processing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "COHORT FPGROWTH ANALYSIS - START\n",
      "================================================================================\n",
      "Cohorts: 5 combinations\n",
      "Item types: ['drug_name', 'icd_code', 'cpt_code']\n",
      "Total jobs: 15\n",
      "Max workers: 1\n",
      "Detailed progress ‚Üí Check log file\n",
      "\n",
      "\n",
      "Checking for existing results in S3...\n",
      "Total jobs: 15\n",
      "Already completed: 3\n",
      "To process: 12\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"COHORT FPGROWTH ANALYSIS - START\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Cohorts: {len(cohort_jobs)} combinations\")\n",
    "print(f\"Item types: {ITEM_TYPES}\")\n",
    "print(f\"Total jobs: {len(cohort_jobs) * len(ITEM_TYPES)}\")\n",
    "print(f\"Max workers: {MAX_WORKERS}\")\n",
    "print(f\"Detailed progress ‚Üí Check log file\")\n",
    "print()\n",
    "\n",
    "logger.info(f\"\\n{'='*80}\")\n",
    "logger.info(f\"COHORT FPGROWTH ANALYSIS - START\")\n",
    "logger.info(f\"{'='*80}\")\n",
    "logger.info(f\"Cohorts: {len(cohort_jobs)} combinations\")\n",
    "logger.info(f\"Item types: {ITEM_TYPES}\")\n",
    "logger.info(f\"Total jobs: {len(cohort_jobs) * len(ITEM_TYPES)}\")\n",
    "\n",
    "# Helper function to check if cohort results exist in S3\n",
    "def check_cohort_exists(item_type: str, cohort: str, age_band: str, event_year: str) -> bool:\n",
    "    \"\"\"Check if cohort results already exist in S3 (by checking for summary.json).\"\"\"\n",
    "    s3 = boto3.client('s3')\n",
    "    key = f\"gold/fpgrowth/cohort/{item_type}/cohort_name={cohort}/age_band={age_band}/event_year={event_year}/summary.json\"\n",
    "    try:\n",
    "        s3.head_object(Bucket='pgxdatalake', Key=key)\n",
    "        return True\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "start_time = time.time()\n",
    "results = []\n",
    "completed = 0\n",
    "failed = 0\n",
    "skipped = 0\n",
    "\n",
    "# Create all combinations of cohorts and item types\n",
    "all_jobs_initial = [(job, item_type) for job in cohort_jobs for item_type in ITEM_TYPES]\n",
    "\n",
    "# Filter out already-completed jobs\n",
    "print(\"\\nChecking for existing results in S3...\")\n",
    "all_jobs = []\n",
    "for job, item_type in all_jobs_initial:\n",
    "    if check_cohort_exists(item_type, job['cohort'], job['age_band'], job['event_year']):\n",
    "        logger.info(f\"Skipping {job['cohort']}/{job['age_band']}/{job['event_year']}/{item_type} - already exists\")\n",
    "        skipped += 1\n",
    "        results.append({\n",
    "            'cohort': job['cohort'],\n",
    "            'age_band': job['age_band'],\n",
    "            'event_year': job['event_year'],\n",
    "            'item_type': item_type,\n",
    "            'success': True,\n",
    "            'message': 'Already exists in S3 (skipped)'\n",
    "        })\n",
    "    else:\n",
    "        all_jobs.append((job, item_type))\n",
    "\n",
    "total_jobs = len(all_jobs_initial)\n",
    "print(f\"Total jobs: {total_jobs}\")\n",
    "print(f\"Already completed: {skipped}\")\n",
    "print(f\"To process: {len(all_jobs)}\")\n",
    "print()\n",
    "\n",
    "with ProcessPoolExecutor(max_workers=MAX_WORKERS) as executor:\n",
    "    # Submit all jobs\n",
    "    future_to_params = {executor.submit(process_single_cohort, job, item_type): (job, item_type) \n",
    "                        for job, item_type in all_jobs}\n",
    "    \n",
    "    # Process results as they complete\n",
    "    for future in as_completed(future_to_params):\n",
    "        job, item_type = future_to_params[future]\n",
    "        try:\n",
    "            cohort, age_band, event_year, item_type, success, message = future.result()\n",
    "            results.append({\n",
    "                'cohort': cohort,\n",
    "                'age_band': age_band,\n",
    "                'event_year': event_year,\n",
    "                'item_type': item_type,\n",
    "                'success': success,\n",
    "                'message': message\n",
    "            })\n",
    "            \n",
    "            if success:\n",
    "                completed += 1\n",
    "                logger.info(f\"[{completed + failed}/{len(all_jobs)}] ‚úì {cohort}/{age_band}/{event_year}/{item_type}: {message}\")\n",
    "                # Print every 10 successes or milestones\n",
    "                if completed % 10 == 0 or (completed + failed) == len(all_jobs):\n",
    "                    print(f\"Progress: {completed}/{len(all_jobs)} completed ({completed/len(all_jobs)*100:.1f}%), {failed} failed, {skipped} skipped\")\n",
    "            else:\n",
    "                failed += 1\n",
    "                logger.warning(f\"[{completed + failed}/{total_jobs}] ‚úó {cohort}/{age_band}/{event_year}/{item_type}: {message}\")\n",
    "                print(f\"‚ö† Failed: {cohort}/{age_band}/{event_year}/{item_type}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            failed += 1\n",
    "            logger.error(f\"[{completed + failed}/{total_jobs}] ‚úó {job['cohort']}/{job['age_band']}/{job['event_year']}/{item_type}: {e}\")\n",
    "            print(f\"‚ö† Error: {job['cohort']}/{job['age_band']}/{job['event_year']}/{item_type}\")\n",
    "            results.append({\n",
    "                'cohort': job['cohort'],\n",
    "                'age_band': job['age_band'],\n",
    "                'event_year': job['event_year'],\n",
    "                'item_type': item_type,\n",
    "                'success': False,\n",
    "                'message': str(e)\n",
    "            })\n",
    "\n",
    "elapsed = time.time() - start_time\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"COHORT FPGROWTH ANALYSIS - COMPLETE\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"  Total jobs: {total_jobs}\")\n",
    "print(f\"  Already in S3 (skipped): {skipped}\")\n",
    "print(f\"  Newly processed: {completed}\")\n",
    "print(f\"  Failed: {failed}\")\n",
    "print(f\"  Overall success rate: {(skipped + completed)/total_jobs*100:.1f}%\")\n",
    "print(f\"  Total time: {elapsed:.1f}s ({elapsed/60:.1f}min)\")\n",
    "if len(all_jobs) > 0:\n",
    "    print(f\"  Avg time per new job: {elapsed/len(all_jobs):.1f}s\")\n",
    "else:\n",
    "    print(f\"  (No new jobs processed - all results already in S3)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Analyze Results\n",
    "\n",
    "Review processing results and identify any issues.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert results to DataFrame for analysis\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "print(\"\\nüìä Results by Cohort and Item Type:\")\n",
    "summary = results_df.groupby(['cohort', 'item_type'])['success'].agg([\n",
    "    ('total', 'count'), \n",
    "    ('successful', 'sum'),\n",
    "    ('success_rate', lambda x: f\"{x.mean()*100:.1f}%\")\n",
    "])\n",
    "print(summary)\n",
    "\n",
    "print(\"\\nüìä Results by Item Type:\")\n",
    "item_summary = results_df.groupby('item_type')['success'].agg([\n",
    "    ('total', 'count'), \n",
    "    ('successful', 'sum'),\n",
    "    ('success_rate', lambda x: f\"{x.mean()*100:.1f}%\")\n",
    "])\n",
    "print(item_summary)\n",
    "\n",
    "print(\"\\n‚ùå Failed Jobs:\")\n",
    "failed_df = results_df[~results_df['success']]\n",
    "if not failed_df.empty:\n",
    "    print(failed_df[['cohort', 'age_band', 'event_year', 'item_type', 'message']].to_string())\n",
    "else:\n",
    "    print(\"  None! All jobs completed successfully.\")\n",
    "\n",
    "print(\"\\n‚úì Successful Jobs Sample:\")\n",
    "success_df = results_df[results_df['success']]\n",
    "if not success_df.empty:\n",
    "    print(success_df[['cohort', 'age_band', 'event_year', 'item_type', 'message']].head(15))\n",
    "else:\n",
    "    print(\"  No successful jobs.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Next Steps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"COHORT FPGROWTH ANALYSIS - SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nüìä Processing Statistics:\")\n",
    "print(f\"  Cohort combinations: {len(cohort_jobs)}\")\n",
    "print(f\"  Item types: {len(ITEM_TYPES)} (drug_name, icd_code, cpt_code)\")\n",
    "print(f\"  Total jobs: {total_jobs}\")\n",
    "print(f\"  Successfully processed: {completed}\")\n",
    "print(f\"  Failed: {failed}\")\n",
    "print(f\"  Success rate: {completed/total_jobs*100:.1f}%\")\n",
    "print(f\"  Processing time: {elapsed:.1f}s ({elapsed/60:.1f}min)\")\n",
    "\n",
    "print(f\"\\nüîç FP-Growth Configuration:\")\n",
    "print(f\"  Min support: {MIN_SUPPORT} ({MIN_SUPPORT*100:.1f}%)\")\n",
    "print(f\"  Min confidence: {MIN_CONFIDENCE} ({MIN_CONFIDENCE*100:.1f}%)\")\n",
    "print(f\"  Parallel workers: {MAX_WORKERS}\")\n",
    "\n",
    "print(f\"\\nüíæ Output Location:\")\n",
    "print(f\"  S3 Base: {S3_OUTPUT_BASE}\")\n",
    "print(f\"  Structure: <item_type>/cohort_name=<name>/age_band=<band>/event_year=<year>/\")\n",
    "print(f\"  Item types:\")\n",
    "print(f\"    - drug_name/ (pharmacy events)\")\n",
    "print(f\"    - icd_code/ (diagnosis codes)\")\n",
    "print(f\"    - cpt_code/ (procedure codes)\")\n",
    "print(f\"  Files per cohort:\")\n",
    "print(f\"    - itemsets.json (frequent itemsets)\")\n",
    "print(f\"    - rules.json (association rules)\")\n",
    "print(f\"    - summary.json (metadata)\")\n",
    "\n",
    "print(f\"\\nüéØ Next Steps:\")\n",
    "print(f\"  1. Load cohort-specific itemsets for BupaR process mining\")\n",
    "print(f\"  2. Compare patterns between OPIOID_ED and ED_NON_OPIOID cohorts across item types\")\n",
    "print(f\"  3. Use association rules for pathway analysis\")\n",
    "print(f\"  4. Filter features for cohort-specific CatBoost models\")\n",
    "print(f\"  5. Create network visualizations for cohort-specific patterns\")\n",
    "\n",
    "print(f\"\\nüìù Example Usage:\")\n",
    "print(f\"  # Load drug patterns for a specific cohort\")\n",
    "print(f\"  from helpers_1997_13.s3_utils import load_from_s3_json\")\n",
    "print(f\"  itemsets = load_from_s3_json('{S3_OUTPUT_BASE}/drug_name/cohort_name=opioid_ed/age_band=65-74/event_year=2020/itemsets.json')\")\n",
    "print(f\"  # Load ICD patterns for same cohort\")\n",
    "print(f\"  icd_itemsets = load_from_s3_json('{S3_OUTPUT_BASE}/icd_code/cohort_name=opioid_ed/age_band=65-74/event_year=2020/itemsets.json')\")\n",
    "\n",
    "print(f\"\\n‚úì Analysis complete: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(\"=\"*80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Auto-Shutdown EC2 Instance (Optional)\n",
    "\n",
    "Set `SHUTDOWN_EC2 = True` to automatically stop the EC2 instance after analysis completes.\n",
    "\n",
    "**Note:** This is a **STOP** (not terminate), so you can restart the instance later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# EC2 AUTO-SHUTDOWN (OPTIONAL)\n",
    "# =============================================================================\n",
    "# Set SHUTDOWN_EC2 = True to enable, False to disable\n",
    "SHUTDOWN_EC2 = False  # Change to True to enable auto-shutdown\n",
    "\n",
    "if SHUTDOWN_EC2:\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"Shutting down EC2 instance...\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    import subprocess\n",
    "    import requests\n",
    "    import shutil\n",
    "    \n",
    "    # Get instance ID from EC2 metadata service\n",
    "    try:\n",
    "        response = requests.get(\n",
    "            \"http://169.254.169.254/latest/meta-data/instance-id\",\n",
    "            timeout=2\n",
    "        )\n",
    "        if response.status_code == 200:\n",
    "            instance_id = response.text.strip()\n",
    "            print(f\"Instance ID: {instance_id}\")\n",
    "            \n",
    "            # Find AWS CLI\n",
    "            aws_cmd = shutil.which(\"aws\")\n",
    "            if not aws_cmd:\n",
    "                # Try common paths\n",
    "                for path in [\"/usr/local/bin/aws\", \"/usr/bin/aws\", \n",
    "                           \"/home/ec2-user/.local/bin/aws\", \n",
    "                           \"/home/ubuntu/.local/bin/aws\",\n",
    "                           \"/home/pgx3874/.local/bin/aws\"]:\n",
    "                    if Path(path).exists():\n",
    "                        aws_cmd = path\n",
    "                        break\n",
    "            \n",
    "            if aws_cmd:\n",
    "                # Stop the instance (use terminate-instances for permanent deletion)\n",
    "                shutdown_cmd = [aws_cmd, \"ec2\", \"stop-instances\", \"--instance-ids\", instance_id]\n",
    "                \n",
    "                print(f\"Running: {' '.join(shutdown_cmd)}\")\n",
    "                result = subprocess.run(shutdown_cmd, capture_output=True, text=True)\n",
    "                \n",
    "                if result.returncode == 0:\n",
    "                    print(\"‚úì EC2 instance stop command sent successfully\")\n",
    "                    print(\"Instance will stop in a few moments.\")\n",
    "                    print(\"Note: This is a STOP (not terminate), so you can restart it later.\")\n",
    "                    if result.stdout:\n",
    "                        print(f\"\\nAWS Response:\\n{result.stdout}\")\n",
    "                else:\n",
    "                    print(f\"‚úó EC2 stop command failed with exit code {result.returncode}\")\n",
    "                    if result.stderr:\n",
    "                        print(f\"Error: {result.stderr}\")\n",
    "                    print(\"Check AWS credentials and IAM permissions.\")\n",
    "            else:\n",
    "                print(\"‚úó AWS CLI not found. Cannot shutdown instance.\")\n",
    "                print(\"Install AWS CLI or ensure it's in your PATH.\")\n",
    "                print(\"Manual shutdown: aws ec2 stop-instances --instance-ids \" + instance_id)\n",
    "        else:\n",
    "            print(f\"‚úó Metadata service returned status code {response.status_code}\")\n",
    "            print(\"Could not retrieve instance ID.\")\n",
    "    \n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(\"‚úó Could not retrieve instance ID from metadata service.\")\n",
    "        print(f\"Error: {e}\")\n",
    "        print(\"If running on EC2, check that metadata service is accessible.\")\n",
    "        print(\"\\nManual shutdown command:\")\n",
    "        print(\"  aws ec2 stop-instances --instance-ids <your-instance-id>\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"‚úó Unexpected error during shutdown: {e}\")\n",
    "\n",
    "else:\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"EC2 Auto-Shutdown: DISABLED\")\n",
    "    print(\"=\"*80)\n",
    "    print(\"To enable auto-shutdown, set SHUTDOWN_EC2 = True in this cell.\")\n",
    "    print(\"Instance will continue running.\")\n",
    "    print(\"\\nTo manually stop this instance later:\")\n",
    "    print(\"  aws ec2 stop-instances --instance-ids $(ec2-metadata --instance-id | cut -d ' ' -f 2)\")\n",
    "    print(\"Or use AWS Console: EC2 > Instances > Select instance > Instance State > Stop\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
